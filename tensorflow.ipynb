{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pandas import DataFrame\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reformat = pd.read_csv('WISDM_ar_latest/WISDM_ar_v1.1/reformat_data.csv')\n",
    "data_reformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# Define column name of the label vector\n",
    "LABEL = 'ActivityEncoded'\n",
    "# Transform the labels from String to Integer via LabelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Add a new column to the existing DataFrame with the encoded values\n",
    "data_reformat[LABEL] = le.fit_transform(data_reformat['class'].values.ravel())\n",
    "dummy=data_reformat.groupby(['class'])\n",
    "dummy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unique_id            int64\n",
       "user_id              int64\n",
       "x0                 float64\n",
       "x1                 float64\n",
       "x2                 float64\n",
       "x3                 float64\n",
       "x4                 float64\n",
       "x5                 float64\n",
       "x6                 float64\n",
       "x7                 float64\n",
       "x8                 float64\n",
       "x9                 float64\n",
       "y0                 float64\n",
       "y1                 float64\n",
       "y2                 float64\n",
       "y3                 float64\n",
       "y4                 float64\n",
       "y5                 float64\n",
       "y6                 float64\n",
       "y7                 float64\n",
       "y8                 float64\n",
       "y9                 float64\n",
       "z0                 float64\n",
       "z1                 float64\n",
       "z2                 float64\n",
       "z3                 float64\n",
       "z4                 float64\n",
       "z5                 float64\n",
       "z6                 float64\n",
       "z7                 float64\n",
       "z8                 float64\n",
       "z9                 float64\n",
       "xavg                 int64\n",
       "yavg               float64\n",
       "zavg               float64\n",
       "xpeak              float64\n",
       "ypeak              float64\n",
       "zpeak              float64\n",
       "xabsoldev          float64\n",
       "yabsoldev          float64\n",
       "zabsoldev          float64\n",
       "xstandardev        float64\n",
       "ystandarddev       float64\n",
       "zstandarddev       float64\n",
       "resultant          float64\n",
       "class               object\n",
       "ActivityEncoded      int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reformat.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    5\n",
       "4    5\n",
       "5    5\n",
       "6    4\n",
       "7    4\n",
       "8    4\n",
       "9    4\n",
       "Name: ActivityEncoded, dtype: int32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y =data_reformat[\"ActivityEncoded\"]\n",
    "y.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x0              float64\n",
       "x1              float64\n",
       "x2              float64\n",
       "x3              float64\n",
       "x4              float64\n",
       "x5              float64\n",
       "x6              float64\n",
       "x7              float64\n",
       "x8              float64\n",
       "x9              float64\n",
       "y0              float64\n",
       "y1              float64\n",
       "y2              float64\n",
       "y3              float64\n",
       "y4              float64\n",
       "y5              float64\n",
       "y6              float64\n",
       "y7              float64\n",
       "y8              float64\n",
       "y9              float64\n",
       "z0              float64\n",
       "z1              float64\n",
       "z2              float64\n",
       "z3              float64\n",
       "z4              float64\n",
       "z5              float64\n",
       "z6              float64\n",
       "z7              float64\n",
       "z8              float64\n",
       "z9              float64\n",
       "xavg              int64\n",
       "yavg            float64\n",
       "zavg            float64\n",
       "xpeak           float64\n",
       "ypeak           float64\n",
       "zpeak           float64\n",
       "xabsoldev       float64\n",
       "yabsoldev       float64\n",
       "zabsoldev       float64\n",
       "xstandardev     float64\n",
       "ystandarddev    float64\n",
       "zstandarddev    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data_reformat[['x0','x1','x2','x3','x4','x5','x6','x7','x8','x9','y0','y1','y2','y3','y4','y5','y6','y7','y8','y9','z0','z1','z2','z3','z4','z5','z6','z7','z8','z9','xavg','yavg','zavg','xpeak','ypeak','zpeak','xabsoldev','yabsoldev','zabsoldev','xstandardev','ystandarddev','zstandarddev']]\n",
    "#feature_names = x.columns\n",
    "x.head()\n",
    "x.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to create training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5034    1\n",
       "154     5\n",
       "58      5\n",
       "540     1\n",
       "3536    0\n",
       "       ..\n",
       "3603    1\n",
       "4722    1\n",
       "3340    4\n",
       "3064    4\n",
       "3398    5\n",
       "Name: ActivityEncoded, Length: 4063, dtype: int32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScater model and fit it to the training data\n",
    "X_scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training and testing data using the X_scaler\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encoding\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)\n",
    "y_test_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create a normal neural network with 2 inputs, 6 hidden nodes, and 2 outputs\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "#59.78%\n",
    "#model.add(Dense(units=6, activation='relu', input_dim=30))\n",
    "#model.add(Dense(units=6, activation='softmax'))\n",
    "#82.66%, with noise - (0.05- 83.76%, 83.1%, 0.2 - 84.2%, 81.9%),with dropout (0.5)-85 to 87%, with dropout (0.1,0.1) - 85.2%, (0.2,0.1) - 86-87%\n",
    "model.add(Dense(units=60, activation='relu', input_dim=42))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(units=6, activation='softmax'))\n",
    "#keras.layers.Dropout(0.05, noise_shape=None, seed=5)\n",
    "\n",
    "#82.7%\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.layers.normalization import BatchNormalization\n",
    "#model.add(Dense(units=60, use_bias=False, activation='relu', input_dim=42))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dense(units=100, activation='relu'))\n",
    "##model.add(BatchNormalization())\n",
    "#model.add(Dense(units=6, activation='softmax'))\n",
    "##model.add(BatchNormalization())\n",
    "#keras.layers.Dropout(0.05, noise_shape=None, seed=5)\n",
    "\n",
    "#63.83 -- old experiment; tried again, not worth the effort\n",
    "#model.add(Dense(units=120, activation='relu', input_dim=30))\n",
    "#model.add(Dense(units=120, activation='relu'))\n",
    "#model.add(Dense(units=120, activation='relu'))\n",
    "#model.add(Dense(units=120, activation='relu'))\n",
    "#model.add(Dense(units=120, activation='relu'))\n",
    "#model.add(Dense(units=120, activation='relu'))\n",
    "#model.add(Dense(units=120, activation='relu'))\n",
    "#model.add(Dense(units=6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 60)                2580      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               6100      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 9,286\n",
      "Trainable params: 9,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#from keras.optimizers import SGD\n",
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#model.compile(loss='categorical_crossentropy',\n",
    "#              optimizer=sgd,\n",
    "#              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      " - 1s - loss: 1.2059 - accuracy: 0.6096\n",
      "Epoch 2/300\n",
      " - 0s - loss: 0.7956 - accuracy: 0.7204\n",
      "Epoch 3/300\n",
      " - 0s - loss: 0.6815 - accuracy: 0.7453\n",
      "Epoch 4/300\n",
      " - 0s - loss: 0.6262 - accuracy: 0.7637\n",
      "Epoch 5/300\n",
      " - 0s - loss: 0.5905 - accuracy: 0.7704\n",
      "Epoch 6/300\n",
      " - 0s - loss: 0.5580 - accuracy: 0.7839\n",
      "Epoch 7/300\n",
      " - 0s - loss: 0.5418 - accuracy: 0.7891\n",
      "Epoch 8/300\n",
      " - 0s - loss: 0.5212 - accuracy: 0.7896\n",
      "Epoch 9/300\n",
      " - 0s - loss: 0.5135 - accuracy: 0.7928\n",
      "Epoch 10/300\n",
      " - 0s - loss: 0.4883 - accuracy: 0.8073\n",
      "Epoch 11/300\n",
      " - 0s - loss: 0.4805 - accuracy: 0.8070\n",
      "Epoch 12/300\n",
      " - 0s - loss: 0.4566 - accuracy: 0.8186\n",
      "Epoch 13/300\n",
      " - 0s - loss: 0.4483 - accuracy: 0.8206\n",
      "Epoch 14/300\n",
      " - 0s - loss: 0.4435 - accuracy: 0.8230\n",
      "Epoch 15/300\n",
      " - 0s - loss: 0.4268 - accuracy: 0.8344\n",
      "Epoch 16/300\n",
      " - 0s - loss: 0.4166 - accuracy: 0.8351\n",
      "Epoch 17/300\n",
      " - 0s - loss: 0.4120 - accuracy: 0.8415\n",
      "Epoch 18/300\n",
      " - 0s - loss: 0.4037 - accuracy: 0.8430\n",
      "Epoch 19/300\n",
      " - 0s - loss: 0.3973 - accuracy: 0.8422\n",
      "Epoch 20/300\n",
      " - 0s - loss: 0.3939 - accuracy: 0.8442\n",
      "Epoch 21/300\n",
      " - 0s - loss: 0.3816 - accuracy: 0.8533\n",
      "Epoch 22/300\n",
      " - 0s - loss: 0.3766 - accuracy: 0.8511\n",
      "Epoch 23/300\n",
      " - 0s - loss: 0.3716 - accuracy: 0.8568\n",
      "Epoch 24/300\n",
      " - 0s - loss: 0.3614 - accuracy: 0.8585\n",
      "Epoch 25/300\n",
      " - 0s - loss: 0.3553 - accuracy: 0.8607\n",
      "Epoch 26/300\n",
      " - 0s - loss: 0.3537 - accuracy: 0.8634\n",
      "Epoch 27/300\n",
      " - 0s - loss: 0.3540 - accuracy: 0.8673\n",
      "Epoch 28/300\n",
      " - 0s - loss: 0.3433 - accuracy: 0.8705\n",
      "Epoch 29/300\n",
      " - 0s - loss: 0.3390 - accuracy: 0.8654\n",
      "Epoch 30/300\n",
      " - 0s - loss: 0.3386 - accuracy: 0.8708\n",
      "Epoch 31/300\n",
      " - 0s - loss: 0.3278 - accuracy: 0.8698\n",
      "Epoch 32/300\n",
      " - 0s - loss: 0.3330 - accuracy: 0.8730\n",
      "Epoch 33/300\n",
      " - 0s - loss: 0.3257 - accuracy: 0.8767\n",
      "Epoch 34/300\n",
      " - 0s - loss: 0.3279 - accuracy: 0.8666\n",
      "Epoch 35/300\n",
      " - 0s - loss: 0.3146 - accuracy: 0.8774\n",
      "Epoch 36/300\n",
      " - 0s - loss: 0.3123 - accuracy: 0.8796\n",
      "Epoch 37/300\n",
      " - 0s - loss: 0.3130 - accuracy: 0.8806\n",
      "Epoch 38/300\n",
      " - 0s - loss: 0.3099 - accuracy: 0.8779\n",
      "Epoch 39/300\n",
      " - 0s - loss: 0.3017 - accuracy: 0.8801\n",
      "Epoch 40/300\n",
      " - 0s - loss: 0.3041 - accuracy: 0.8799\n",
      "Epoch 41/300\n",
      " - 0s - loss: 0.2920 - accuracy: 0.8905\n",
      "Epoch 42/300\n",
      " - 0s - loss: 0.2923 - accuracy: 0.8819\n",
      "Epoch 43/300\n",
      " - 0s - loss: 0.2998 - accuracy: 0.8846\n",
      "Epoch 44/300\n",
      " - 0s - loss: 0.2871 - accuracy: 0.8831\n",
      "Epoch 45/300\n",
      " - 0s - loss: 0.2854 - accuracy: 0.8858\n",
      "Epoch 46/300\n",
      " - 0s - loss: 0.2797 - accuracy: 0.8922\n",
      "Epoch 47/300\n",
      " - 0s - loss: 0.2807 - accuracy: 0.8873\n",
      "Epoch 48/300\n",
      " - 0s - loss: 0.2765 - accuracy: 0.8912\n",
      "Epoch 49/300\n",
      " - 0s - loss: 0.2619 - accuracy: 0.9016\n",
      "Epoch 50/300\n",
      " - 0s - loss: 0.2728 - accuracy: 0.8942\n",
      "Epoch 51/300\n",
      " - 0s - loss: 0.2689 - accuracy: 0.8917\n",
      "Epoch 52/300\n",
      " - 0s - loss: 0.2712 - accuracy: 0.8952\n",
      "Epoch 53/300\n",
      " - 0s - loss: 0.2612 - accuracy: 0.8986\n",
      "Epoch 54/300\n",
      " - 0s - loss: 0.2634 - accuracy: 0.8959\n",
      "Epoch 55/300\n",
      " - 0s - loss: 0.2602 - accuracy: 0.8971\n",
      "Epoch 56/300\n",
      " - 0s - loss: 0.2657 - accuracy: 0.8961\n",
      "Epoch 57/300\n",
      " - 0s - loss: 0.2518 - accuracy: 0.9008\n",
      "Epoch 58/300\n",
      " - 0s - loss: 0.2423 - accuracy: 0.9033\n",
      "Epoch 59/300\n",
      " - 0s - loss: 0.2579 - accuracy: 0.8974\n",
      "Epoch 60/300\n",
      " - 0s - loss: 0.2467 - accuracy: 0.8991\n",
      "Epoch 61/300\n",
      " - 0s - loss: 0.2503 - accuracy: 0.9057\n",
      "Epoch 62/300\n",
      " - 0s - loss: 0.2433 - accuracy: 0.9020\n",
      "Epoch 63/300\n",
      " - 0s - loss: 0.2424 - accuracy: 0.9055\n",
      "Epoch 64/300\n",
      " - 0s - loss: 0.2358 - accuracy: 0.9116\n",
      "Epoch 65/300\n",
      " - 0s - loss: 0.2438 - accuracy: 0.9023\n",
      "Epoch 66/300\n",
      " - 0s - loss: 0.2316 - accuracy: 0.9094\n",
      "Epoch 67/300\n",
      " - 0s - loss: 0.2265 - accuracy: 0.9143\n",
      "Epoch 68/300\n",
      " - 0s - loss: 0.2364 - accuracy: 0.9082\n",
      "Epoch 69/300\n",
      " - 0s - loss: 0.2336 - accuracy: 0.9089\n",
      "Epoch 70/300\n",
      " - 0s - loss: 0.2355 - accuracy: 0.9102\n",
      "Epoch 71/300\n",
      " - 0s - loss: 0.2331 - accuracy: 0.9079\n",
      "Epoch 72/300\n",
      " - 0s - loss: 0.2268 - accuracy: 0.9109\n",
      "Epoch 73/300\n",
      " - 0s - loss: 0.2223 - accuracy: 0.9146\n",
      "Epoch 74/300\n",
      " - 0s - loss: 0.2240 - accuracy: 0.9134\n",
      "Epoch 75/300\n",
      " - 0s - loss: 0.2160 - accuracy: 0.9139\n",
      "Epoch 76/300\n",
      " - 0s - loss: 0.2119 - accuracy: 0.9237\n",
      "Epoch 77/300\n",
      " - 0s - loss: 0.2070 - accuracy: 0.9180\n",
      "Epoch 78/300\n",
      " - 0s - loss: 0.2117 - accuracy: 0.9232\n",
      "Epoch 79/300\n",
      " - 0s - loss: 0.2173 - accuracy: 0.9161\n",
      "Epoch 80/300\n",
      " - 0s - loss: 0.2345 - accuracy: 0.9102\n",
      "Epoch 81/300\n",
      " - 0s - loss: 0.2247 - accuracy: 0.9173\n",
      "Epoch 82/300\n",
      " - 0s - loss: 0.2117 - accuracy: 0.9168\n",
      "Epoch 83/300\n",
      " - 0s - loss: 0.2133 - accuracy: 0.9193\n",
      "Epoch 84/300\n",
      " - 0s - loss: 0.2156 - accuracy: 0.9168\n",
      "Epoch 85/300\n",
      " - 0s - loss: 0.2155 - accuracy: 0.9207\n",
      "Epoch 86/300\n",
      " - 0s - loss: 0.2120 - accuracy: 0.9207\n",
      "Epoch 87/300\n",
      " - 0s - loss: 0.2015 - accuracy: 0.9198\n",
      "Epoch 88/300\n",
      " - 0s - loss: 0.2043 - accuracy: 0.9257\n",
      "Epoch 89/300\n",
      " - 0s - loss: 0.2005 - accuracy: 0.9264\n",
      "Epoch 90/300\n",
      " - 0s - loss: 0.1967 - accuracy: 0.9220\n",
      "Epoch 91/300\n",
      " - 0s - loss: 0.2053 - accuracy: 0.9180\n",
      "Epoch 92/300\n",
      " - 0s - loss: 0.1960 - accuracy: 0.9274\n",
      "Epoch 93/300\n",
      " - 0s - loss: 0.2015 - accuracy: 0.9227\n",
      "Epoch 94/300\n",
      " - 0s - loss: 0.2080 - accuracy: 0.9188\n",
      "Epoch 95/300\n",
      " - 0s - loss: 0.2020 - accuracy: 0.9200\n",
      "Epoch 96/300\n",
      " - 0s - loss: 0.1964 - accuracy: 0.9274\n",
      "Epoch 97/300\n",
      " - 0s - loss: 0.1980 - accuracy: 0.9244\n",
      "Epoch 98/300\n",
      " - 0s - loss: 0.1945 - accuracy: 0.9239\n",
      "Epoch 99/300\n",
      " - 0s - loss: 0.1997 - accuracy: 0.9237\n",
      "Epoch 100/300\n",
      " - 0s - loss: 0.1860 - accuracy: 0.9289\n",
      "Epoch 101/300\n",
      " - 0s - loss: 0.2009 - accuracy: 0.9249\n",
      "Epoch 102/300\n",
      " - 0s - loss: 0.1856 - accuracy: 0.9269\n",
      "Epoch 103/300\n",
      " - 0s - loss: 0.1878 - accuracy: 0.9286\n",
      "Epoch 104/300\n",
      " - 0s - loss: 0.1907 - accuracy: 0.9276\n",
      "Epoch 105/300\n",
      " - 0s - loss: 0.1858 - accuracy: 0.9269\n",
      "Epoch 106/300\n",
      " - 0s - loss: 0.1921 - accuracy: 0.9252\n",
      "Epoch 107/300\n",
      " - 0s - loss: 0.1879 - accuracy: 0.9289\n",
      "Epoch 108/300\n",
      " - 0s - loss: 0.1852 - accuracy: 0.9321\n",
      "Epoch 109/300\n",
      " - 0s - loss: 0.1819 - accuracy: 0.9308\n",
      "Epoch 110/300\n",
      " - 0s - loss: 0.1802 - accuracy: 0.9323\n",
      "Epoch 111/300\n",
      " - 0s - loss: 0.1797 - accuracy: 0.9323\n",
      "Epoch 112/300\n",
      " - 0s - loss: 0.1779 - accuracy: 0.9313\n",
      "Epoch 113/300\n",
      " - 0s - loss: 0.1755 - accuracy: 0.9303\n",
      "Epoch 114/300\n",
      " - 0s - loss: 0.1736 - accuracy: 0.9333\n",
      "Epoch 115/300\n",
      " - 0s - loss: 0.1767 - accuracy: 0.9331\n",
      "Epoch 116/300\n",
      " - 0s - loss: 0.1828 - accuracy: 0.9335\n",
      "Epoch 117/300\n",
      " - 0s - loss: 0.1750 - accuracy: 0.9333\n",
      "Epoch 118/300\n",
      " - 0s - loss: 0.1707 - accuracy: 0.9348\n",
      "Epoch 119/300\n",
      " - 0s - loss: 0.1653 - accuracy: 0.9397\n",
      "Epoch 120/300\n",
      " - 0s - loss: 0.1805 - accuracy: 0.9289\n",
      "Epoch 121/300\n",
      " - 0s - loss: 0.1693 - accuracy: 0.9358\n",
      "Epoch 122/300\n",
      " - 0s - loss: 0.1719 - accuracy: 0.9296\n",
      "Epoch 123/300\n",
      " - 0s - loss: 0.1699 - accuracy: 0.9350\n",
      "Epoch 124/300\n",
      " - 0s - loss: 0.1791 - accuracy: 0.9299\n",
      "Epoch 125/300\n",
      " - 0s - loss: 0.1687 - accuracy: 0.9358\n",
      "Epoch 126/300\n",
      " - 0s - loss: 0.1612 - accuracy: 0.9382\n",
      "Epoch 127/300\n",
      " - 0s - loss: 0.1609 - accuracy: 0.9392\n",
      "Epoch 128/300\n",
      " - 0s - loss: 0.1676 - accuracy: 0.9367\n",
      "Epoch 129/300\n",
      " - 0s - loss: 0.1757 - accuracy: 0.9316\n",
      "Epoch 130/300\n",
      " - 0s - loss: 0.1691 - accuracy: 0.9377\n",
      "Epoch 131/300\n",
      " - 0s - loss: 0.1554 - accuracy: 0.9417\n",
      "Epoch 132/300\n",
      " - 0s - loss: 0.1607 - accuracy: 0.9372\n",
      "Epoch 133/300\n",
      " - 0s - loss: 0.1608 - accuracy: 0.9392\n",
      "Epoch 134/300\n",
      " - 0s - loss: 0.1551 - accuracy: 0.9404\n",
      "Epoch 135/300\n",
      " - 0s - loss: 0.1691 - accuracy: 0.9338\n",
      "Epoch 136/300\n",
      " - 0s - loss: 0.1674 - accuracy: 0.9333\n",
      "Epoch 137/300\n",
      " - 0s - loss: 0.1625 - accuracy: 0.9395\n",
      "Epoch 138/300\n",
      " - 0s - loss: 0.1631 - accuracy: 0.9390\n",
      "Epoch 139/300\n",
      " - 0s - loss: 0.1618 - accuracy: 0.9377\n",
      "Epoch 140/300\n",
      " - 0s - loss: 0.1582 - accuracy: 0.9397\n",
      "Epoch 141/300\n",
      " - 0s - loss: 0.1608 - accuracy: 0.9422\n",
      "Epoch 142/300\n",
      " - 0s - loss: 0.1585 - accuracy: 0.9382\n",
      "Epoch 143/300\n",
      " - 0s - loss: 0.1593 - accuracy: 0.9377\n",
      "Epoch 144/300\n",
      " - 0s - loss: 0.1574 - accuracy: 0.9395\n",
      "Epoch 145/300\n",
      " - 0s - loss: 0.1607 - accuracy: 0.9377\n",
      "Epoch 146/300\n",
      " - 0s - loss: 0.1553 - accuracy: 0.9441\n",
      "Epoch 147/300\n",
      " - 0s - loss: 0.1551 - accuracy: 0.9431\n",
      "Epoch 148/300\n",
      " - 0s - loss: 0.1550 - accuracy: 0.9429\n",
      "Epoch 149/300\n",
      " - 0s - loss: 0.1429 - accuracy: 0.9476\n",
      "Epoch 150/300\n",
      " - 0s - loss: 0.1493 - accuracy: 0.9449\n",
      "Epoch 151/300\n",
      " - 0s - loss: 0.1520 - accuracy: 0.9439\n",
      "Epoch 152/300\n",
      " - 0s - loss: 0.1582 - accuracy: 0.9348\n",
      "Epoch 153/300\n",
      " - 0s - loss: 0.1571 - accuracy: 0.9417\n",
      "Epoch 154/300\n",
      " - 0s - loss: 0.1489 - accuracy: 0.9454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/300\n",
      " - 0s - loss: 0.1535 - accuracy: 0.9439\n",
      "Epoch 156/300\n",
      " - 0s - loss: 0.1467 - accuracy: 0.9459\n",
      "Epoch 157/300\n",
      " - 0s - loss: 0.1447 - accuracy: 0.9456\n",
      "Epoch 158/300\n",
      " - 0s - loss: 0.1387 - accuracy: 0.9481\n",
      "Epoch 159/300\n",
      " - 0s - loss: 0.1630 - accuracy: 0.9380\n",
      "Epoch 160/300\n",
      " - 0s - loss: 0.1542 - accuracy: 0.9407\n",
      "Epoch 161/300\n",
      " - 0s - loss: 0.1423 - accuracy: 0.9495\n",
      "Epoch 162/300\n",
      " - 0s - loss: 0.1401 - accuracy: 0.9486\n",
      "Epoch 163/300\n",
      " - 0s - loss: 0.1428 - accuracy: 0.9451\n",
      "Epoch 164/300\n",
      " - 0s - loss: 0.1480 - accuracy: 0.9436\n",
      "Epoch 165/300\n",
      " - 0s - loss: 0.1452 - accuracy: 0.9466\n",
      "Epoch 166/300\n",
      " - 0s - loss: 0.1514 - accuracy: 0.9436\n",
      "Epoch 167/300\n",
      " - 0s - loss: 0.1348 - accuracy: 0.9488\n",
      "Epoch 168/300\n",
      " - 0s - loss: 0.1324 - accuracy: 0.9552\n",
      "Epoch 169/300\n",
      " - 0s - loss: 0.1307 - accuracy: 0.9523\n",
      "Epoch 170/300\n",
      " - 0s - loss: 0.1316 - accuracy: 0.9500\n",
      "Epoch 171/300\n",
      " - 0s - loss: 0.1399 - accuracy: 0.9456\n",
      "Epoch 172/300\n",
      " - 0s - loss: 0.1430 - accuracy: 0.9422\n",
      "Epoch 173/300\n",
      " - 0s - loss: 0.1364 - accuracy: 0.9463\n",
      "Epoch 174/300\n",
      " - 0s - loss: 0.1431 - accuracy: 0.9436\n",
      "Epoch 175/300\n",
      " - 0s - loss: 0.1336 - accuracy: 0.9532\n",
      "Epoch 176/300\n",
      " - 0s - loss: 0.1343 - accuracy: 0.9552\n",
      "Epoch 177/300\n",
      " - 0s - loss: 0.1326 - accuracy: 0.9513\n",
      "Epoch 178/300\n",
      " - 0s - loss: 0.1330 - accuracy: 0.9518\n",
      "Epoch 179/300\n",
      " - 0s - loss: 0.1394 - accuracy: 0.9488\n",
      "Epoch 180/300\n",
      " - 0s - loss: 0.1349 - accuracy: 0.9527\n",
      "Epoch 181/300\n",
      " - 0s - loss: 0.1388 - accuracy: 0.9491\n",
      "Epoch 182/300\n",
      " - 0s - loss: 0.1251 - accuracy: 0.9547\n",
      "Epoch 183/300\n",
      " - 0s - loss: 0.1500 - accuracy: 0.9436\n",
      "Epoch 184/300\n",
      " - 0s - loss: 0.1374 - accuracy: 0.9481\n",
      "Epoch 185/300\n",
      " - 0s - loss: 0.1254 - accuracy: 0.9518\n",
      "Epoch 186/300\n",
      " - 0s - loss: 0.1280 - accuracy: 0.9523\n",
      "Epoch 187/300\n",
      " - 0s - loss: 0.1285 - accuracy: 0.9510\n",
      "Epoch 188/300\n",
      " - 0s - loss: 0.1364 - accuracy: 0.9483\n",
      "Epoch 189/300\n",
      " - 0s - loss: 0.1303 - accuracy: 0.9518\n",
      "Epoch 190/300\n",
      " - 0s - loss: 0.1335 - accuracy: 0.9508\n",
      "Epoch 191/300\n",
      " - 0s - loss: 0.1145 - accuracy: 0.9587\n",
      "Epoch 192/300\n",
      " - 0s - loss: 0.1316 - accuracy: 0.9515\n",
      "Epoch 193/300\n",
      " - 0s - loss: 0.1281 - accuracy: 0.9532\n",
      "Epoch 194/300\n",
      " - 0s - loss: 0.1250 - accuracy: 0.9527\n",
      "Epoch 195/300\n",
      " - 0s - loss: 0.1278 - accuracy: 0.9478\n",
      "Epoch 196/300\n",
      " - 0s - loss: 0.1300 - accuracy: 0.9515\n",
      "Epoch 197/300\n",
      " - 0s - loss: 0.1184 - accuracy: 0.9569\n",
      "Epoch 198/300\n",
      " - 0s - loss: 0.1376 - accuracy: 0.9520\n",
      "Epoch 199/300\n",
      " - 0s - loss: 0.1244 - accuracy: 0.9527\n",
      "Epoch 200/300\n",
      " - 0s - loss: 0.1455 - accuracy: 0.9476\n",
      "Epoch 201/300\n",
      " - 0s - loss: 0.1234 - accuracy: 0.9550\n",
      "Epoch 202/300\n",
      " - 0s - loss: 0.1336 - accuracy: 0.9523\n",
      "Epoch 203/300\n",
      " - 0s - loss: 0.1289 - accuracy: 0.9498\n",
      "Epoch 204/300\n",
      " - 0s - loss: 0.1421 - accuracy: 0.9454\n",
      "Epoch 205/300\n",
      " - 0s - loss: 0.1204 - accuracy: 0.9589\n",
      "Epoch 206/300\n",
      " - 0s - loss: 0.1198 - accuracy: 0.9552\n",
      "Epoch 207/300\n",
      " - 0s - loss: 0.1260 - accuracy: 0.9518\n",
      "Epoch 208/300\n",
      " - 0s - loss: 0.1201 - accuracy: 0.9572\n",
      "Epoch 209/300\n",
      " - 0s - loss: 0.1264 - accuracy: 0.9557\n",
      "Epoch 210/300\n",
      " - 0s - loss: 0.1216 - accuracy: 0.9547\n",
      "Epoch 211/300\n",
      " - 0s - loss: 0.1219 - accuracy: 0.9532\n",
      "Epoch 212/300\n",
      " - 1s - loss: 0.1199 - accuracy: 0.9567\n",
      "Epoch 213/300\n",
      " - 0s - loss: 0.1185 - accuracy: 0.9587\n",
      "Epoch 214/300\n",
      " - 0s - loss: 0.1204 - accuracy: 0.9564\n",
      "Epoch 215/300\n",
      " - 0s - loss: 0.1295 - accuracy: 0.9525\n",
      "Epoch 216/300\n",
      " - 0s - loss: 0.1114 - accuracy: 0.9619\n",
      "Epoch 217/300\n",
      " - 0s - loss: 0.1205 - accuracy: 0.9550\n",
      "Epoch 218/300\n",
      " - 0s - loss: 0.1123 - accuracy: 0.9564\n",
      "Epoch 219/300\n",
      " - 0s - loss: 0.1247 - accuracy: 0.9555\n",
      "Epoch 220/300\n",
      " - 0s - loss: 0.1225 - accuracy: 0.9567\n",
      "Epoch 221/300\n",
      " - 0s - loss: 0.1199 - accuracy: 0.9577\n",
      "Epoch 222/300\n",
      " - 0s - loss: 0.1069 - accuracy: 0.9658\n",
      "Epoch 223/300\n",
      " - 0s - loss: 0.1241 - accuracy: 0.9523\n",
      "Epoch 224/300\n",
      " - 0s - loss: 0.1305 - accuracy: 0.9508\n",
      "Epoch 225/300\n",
      " - 0s - loss: 0.1175 - accuracy: 0.9540\n",
      "Epoch 226/300\n",
      " - 0s - loss: 0.1156 - accuracy: 0.9574\n",
      "Epoch 227/300\n",
      " - 0s - loss: 0.1330 - accuracy: 0.9545\n",
      "Epoch 228/300\n",
      " - 0s - loss: 0.1174 - accuracy: 0.9555\n",
      "Epoch 229/300\n",
      " - 0s - loss: 0.1168 - accuracy: 0.9552\n",
      "Epoch 230/300\n",
      " - 0s - loss: 0.1148 - accuracy: 0.9550\n",
      "Epoch 231/300\n",
      " - 0s - loss: 0.1180 - accuracy: 0.9557\n",
      "Epoch 232/300\n",
      " - 0s - loss: 0.1196 - accuracy: 0.9550\n",
      "Epoch 233/300\n",
      " - 0s - loss: 0.1176 - accuracy: 0.9564\n",
      "Epoch 234/300\n",
      " - 0s - loss: 0.1132 - accuracy: 0.9601\n",
      "Epoch 235/300\n",
      " - 0s - loss: 0.1140 - accuracy: 0.9606\n",
      "Epoch 236/300\n",
      " - 0s - loss: 0.1322 - accuracy: 0.9520\n",
      "Epoch 237/300\n",
      " - 0s - loss: 0.1126 - accuracy: 0.9599\n",
      "Epoch 238/300\n",
      " - 0s - loss: 0.1077 - accuracy: 0.9636\n",
      "Epoch 239/300\n",
      " - 0s - loss: 0.1141 - accuracy: 0.9584\n",
      "Epoch 240/300\n",
      " - 0s - loss: 0.1052 - accuracy: 0.9623\n",
      "Epoch 241/300\n",
      " - 0s - loss: 0.1178 - accuracy: 0.9567\n",
      "Epoch 242/300\n",
      " - 0s - loss: 0.1121 - accuracy: 0.9591\n",
      "Epoch 243/300\n",
      " - 0s - loss: 0.1142 - accuracy: 0.9587\n",
      "Epoch 244/300\n",
      " - 0s - loss: 0.1120 - accuracy: 0.9594\n",
      "Epoch 245/300\n",
      " - 0s - loss: 0.1109 - accuracy: 0.9614\n",
      "Epoch 246/300\n",
      " - 0s - loss: 0.1214 - accuracy: 0.9574\n",
      "Epoch 247/300\n",
      " - 0s - loss: 0.1207 - accuracy: 0.9594\n",
      "Epoch 248/300\n",
      " - 0s - loss: 0.1070 - accuracy: 0.9623\n",
      "Epoch 249/300\n",
      " - 0s - loss: 0.1079 - accuracy: 0.9636\n",
      "Epoch 250/300\n",
      " - 0s - loss: 0.1062 - accuracy: 0.9616\n",
      "Epoch 251/300\n",
      " - 0s - loss: 0.0973 - accuracy: 0.9646\n",
      "Epoch 252/300\n",
      " - 0s - loss: 0.1126 - accuracy: 0.9587\n",
      "Epoch 253/300\n",
      " - 0s - loss: 0.1072 - accuracy: 0.9636\n",
      "Epoch 254/300\n",
      " - 0s - loss: 0.0977 - accuracy: 0.9648\n",
      "Epoch 255/300\n",
      " - 0s - loss: 0.1047 - accuracy: 0.9623\n",
      "Epoch 256/300\n",
      " - 0s - loss: 0.1056 - accuracy: 0.9574\n",
      "Epoch 257/300\n",
      " - 0s - loss: 0.1029 - accuracy: 0.9653\n",
      "Epoch 258/300\n",
      " - 0s - loss: 0.1050 - accuracy: 0.9621\n",
      "Epoch 259/300\n",
      " - 0s - loss: 0.1146 - accuracy: 0.9584\n",
      "Epoch 260/300\n",
      " - 0s - loss: 0.1096 - accuracy: 0.9596\n",
      "Epoch 261/300\n",
      " - 0s - loss: 0.1050 - accuracy: 0.9606\n",
      "Epoch 262/300\n",
      " - 0s - loss: 0.1058 - accuracy: 0.9606\n",
      "Epoch 263/300\n",
      " - 0s - loss: 0.1122 - accuracy: 0.9626\n",
      "Epoch 264/300\n",
      " - 0s - loss: 0.0972 - accuracy: 0.9673\n",
      "Epoch 265/300\n",
      " - 0s - loss: 0.1032 - accuracy: 0.9633\n",
      "Epoch 266/300\n",
      " - 0s - loss: 0.1088 - accuracy: 0.9562\n",
      "Epoch 267/300\n",
      " - 0s - loss: 0.1070 - accuracy: 0.9582\n",
      "Epoch 268/300\n",
      " - 0s - loss: 0.1106 - accuracy: 0.9601\n",
      "Epoch 269/300\n",
      " - 0s - loss: 0.1172 - accuracy: 0.9557\n",
      "Epoch 270/300\n",
      " - 0s - loss: 0.1033 - accuracy: 0.9643\n",
      "Epoch 271/300\n",
      " - 0s - loss: 0.1018 - accuracy: 0.9636\n",
      "Epoch 272/300\n",
      " - 0s - loss: 0.1106 - accuracy: 0.9626\n",
      "Epoch 273/300\n",
      " - 0s - loss: 0.0873 - accuracy: 0.9697\n",
      "Epoch 274/300\n",
      " - 0s - loss: 0.0935 - accuracy: 0.9658\n",
      "Epoch 275/300\n",
      " - 0s - loss: 0.1052 - accuracy: 0.9614\n",
      "Epoch 276/300\n",
      " - 0s - loss: 0.1106 - accuracy: 0.9596\n",
      "Epoch 277/300\n",
      " - 0s - loss: 0.0908 - accuracy: 0.9685\n",
      "Epoch 278/300\n",
      " - 0s - loss: 0.1072 - accuracy: 0.9631\n",
      "Epoch 279/300\n",
      " - 0s - loss: 0.1100 - accuracy: 0.9623\n",
      "Epoch 280/300\n",
      " - 0s - loss: 0.1035 - accuracy: 0.9636\n",
      "Epoch 281/300\n",
      " - 0s - loss: 0.0885 - accuracy: 0.9685\n",
      "Epoch 282/300\n",
      " - 0s - loss: 0.0881 - accuracy: 0.9685\n",
      "Epoch 283/300\n",
      " - 0s - loss: 0.1021 - accuracy: 0.9623\n",
      "Epoch 284/300\n",
      " - 0s - loss: 0.1084 - accuracy: 0.9626\n",
      "Epoch 285/300\n",
      " - 0s - loss: 0.0958 - accuracy: 0.9648\n",
      "Epoch 286/300\n",
      " - 0s - loss: 0.0996 - accuracy: 0.9628\n",
      "Epoch 287/300\n",
      " - 0s - loss: 0.1006 - accuracy: 0.9663\n",
      "Epoch 288/300\n",
      " - 0s - loss: 0.1096 - accuracy: 0.9584\n",
      "Epoch 289/300\n",
      " - 0s - loss: 0.1004 - accuracy: 0.9636\n",
      "Epoch 290/300\n",
      " - 0s - loss: 0.1004 - accuracy: 0.9653\n",
      "Epoch 291/300\n",
      " - 0s - loss: 0.1009 - accuracy: 0.9653\n",
      "Epoch 292/300\n",
      " - 0s - loss: 0.1087 - accuracy: 0.9619\n",
      "Epoch 293/300\n",
      " - 0s - loss: 0.1018 - accuracy: 0.9641\n",
      "Epoch 294/300\n",
      " - 0s - loss: 0.0968 - accuracy: 0.9665\n",
      "Epoch 295/300\n",
      " - 0s - loss: 0.0984 - accuracy: 0.9631\n",
      "Epoch 296/300\n",
      " - 0s - loss: 0.1022 - accuracy: 0.9641\n",
      "Epoch 297/300\n",
      " - 0s - loss: 0.0989 - accuracy: 0.9626\n",
      "Epoch 298/300\n",
      " - 0s - loss: 0.1106 - accuracy: 0.9614\n",
      "Epoch 299/300\n",
      " - 0s - loss: 0.1046 - accuracy: 0.9653\n",
      "Epoch 300/300\n",
      " - 0s - loss: 0.1021 - accuracy: 0.9646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x239167b8fd0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "model.fit(X_train_scaled, y_train_categorical, epochs=300, class_weight=class_weights, verbose=2)\n",
    "\n",
    "#model.fit(\n",
    "#    X_train_scaled,\n",
    "#    y_train_categorical,\n",
    "#    epochs=200,\n",
    "#    shuffle=True,\n",
    "#    verbose=2\n",
    "#)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Neural Network - Loss: 0.5630168875645007, Accuracy: 0.8612546324729919\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "#other code to use label encoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(data_reformat['class'].values)\n",
    "print(integer_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Neural Network - Loss: 0.022639295654272817, Accuracy: 0.9972926378250122\n"
     ]
    }
   ],
   "source": [
    "model_loss_training, model_accuracy_training = model.evaluate(\n",
    "    X_train_scaled, y_train_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss_training}, Accuracy: {model_accuracy_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1008 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.36      0.45       121\n",
      "           1       0.98      0.99      0.98       412\n",
      "           2       0.98      0.95      0.97        60\n",
      "           3       0.96      0.96      0.96        73\n",
      "           4       0.61      0.52      0.56       153\n",
      "           5       0.85      0.95      0.90       536\n",
      "\n",
      "    accuracy                           0.86      1355\n",
      "   macro avg       0.83      0.79      0.80      1355\n",
      "weighted avg       0.85      0.86      0.85      1355\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_confusion_matrix(validations, predictions):\n",
    "\n",
    "    matrix = metrics.confusion_matrix(validations, predictions)\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    plt.tight_layout()\n",
    "    sns.heatmap(matrix,\n",
    "                cmap='coolwarm',\n",
    "                linecolor='white',\n",
    "                linewidths=1,\n",
    "                xticklabels= [1,2,3,4,5,6],\n",
    "                yticklabels= [1,2,3,4,5,6],\n",
    "                annot=True,\n",
    "                square=True,\n",
    "                fmt='d')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "y_pred_test = model.predict(X_test_scaled)\n",
    "# Take the class with the highest probability from the test predictions\n",
    "max_y_pred_test = np.argmax(y_pred_test, axis=1)\n",
    "max_y_test = np.argmax(y_test_categorical, axis=1)\n",
    "\n",
    "show_confusion_matrix(max_y_test, max_y_pred_test)\n",
    "\n",
    "print(classification_report(max_y_test, max_y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.59730833, -0.51111261, -0.49752183, ...,  1.16763333,\n",
       "         1.3729963 ,  1.16763333],\n",
       "       [ 0.48165743,  0.74713666,  0.91310705, ...,  1.60118031,\n",
       "         0.64907354,  1.60118031],\n",
       "       [ 0.48165743, -0.33136272, -0.67385044, ...,  2.06614375,\n",
       "         1.92165354,  2.06614375],\n",
       "       ...,\n",
       "       [-1.27166193, -1.76936188, -1.7318221 , ..., -0.44717212,\n",
       "        -1.39315064, -0.44717212],\n",
       "       [-0.59730833, -0.15161282, -0.49752183, ..., -0.79275306,\n",
       "         1.52540109, -0.79275306],\n",
       "       [-0.46243761, -0.51111261,  0.56044983, ..., -0.5508464 ,\n",
       "        -0.38727903, -0.5508464 ]])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab just one data point to test with\n",
    "test = np.expand_dims(X_test_scaled[100], axis=0)\n",
    "result = np.expand_dims(y_test_categorical[100], axis=0)\n",
    "test.shape\n",
    "test\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: [3]\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction. The result should be 5 - STANDING\n",
    "print(f\"Predicted class: {model.predict_classes(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout , BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "#adding more layers to the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=60, activation='relu', input_dim=42))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Dense(196, activation='relu'))\n",
    "model.add(Dense(196, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(units=6, activation='softmax'))\n",
    "#keras.layers.Dropout(0.05, noise_shape=None, seed=5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_44 (Dense)             (None, 60)                2580      \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 100)               6100      \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 64)                6464      \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 196)               12740     \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 196)               38612     \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 32)                6304      \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 72,998\n",
      "Trainable params: 72,998\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4063 samples\n",
      "Epoch 1/100\n",
      "4063/4063 - 1s - loss: 0.9934 - accuracy: 0.6650\n",
      "Epoch 2/100\n",
      "4063/4063 - 0s - loss: 0.6076 - accuracy: 0.7684\n",
      "Epoch 3/100\n",
      "4063/4063 - 0s - loss: 0.5197 - accuracy: 0.7871\n",
      "Epoch 4/100\n",
      "4063/4063 - 0s - loss: 0.4702 - accuracy: 0.8149\n",
      "Epoch 5/100\n",
      "4063/4063 - 0s - loss: 0.4187 - accuracy: 0.8324\n",
      "Epoch 6/100\n",
      "4063/4063 - 0s - loss: 0.3894 - accuracy: 0.8437\n",
      "Epoch 7/100\n",
      "4063/4063 - 0s - loss: 0.3574 - accuracy: 0.8587\n",
      "Epoch 8/100\n",
      "4063/4063 - 0s - loss: 0.3213 - accuracy: 0.8688\n",
      "Epoch 9/100\n",
      "4063/4063 - 0s - loss: 0.3151 - accuracy: 0.8745\n",
      "Epoch 10/100\n",
      "4063/4063 - 0s - loss: 0.2771 - accuracy: 0.8878\n",
      "Epoch 11/100\n",
      "4063/4063 - 0s - loss: 0.2548 - accuracy: 0.8961\n",
      "Epoch 12/100\n",
      "4063/4063 - 0s - loss: 0.2366 - accuracy: 0.9038\n",
      "Epoch 13/100\n",
      "4063/4063 - 0s - loss: 0.2348 - accuracy: 0.9018\n",
      "Epoch 14/100\n",
      "4063/4063 - 0s - loss: 0.2096 - accuracy: 0.9146\n",
      "Epoch 15/100\n",
      "4063/4063 - 1s - loss: 0.1775 - accuracy: 0.9259\n",
      "Epoch 16/100\n",
      "4063/4063 - 0s - loss: 0.1711 - accuracy: 0.9274\n",
      "Epoch 17/100\n",
      "4063/4063 - 0s - loss: 0.1622 - accuracy: 0.9338\n",
      "Epoch 18/100\n",
      "4063/4063 - 0s - loss: 0.1511 - accuracy: 0.9407\n",
      "Epoch 19/100\n",
      "4063/4063 - 0s - loss: 0.1380 - accuracy: 0.9444\n",
      "Epoch 20/100\n",
      "4063/4063 - 0s - loss: 0.1478 - accuracy: 0.9427\n",
      "Epoch 21/100\n",
      "4063/4063 - 0s - loss: 0.1429 - accuracy: 0.9478\n",
      "Epoch 22/100\n",
      "4063/4063 - 0s - loss: 0.0920 - accuracy: 0.9633\n",
      "Epoch 23/100\n",
      "4063/4063 - 0s - loss: 0.0914 - accuracy: 0.9646\n",
      "Epoch 24/100\n",
      "4063/4063 - 0s - loss: 0.0784 - accuracy: 0.9727\n",
      "Epoch 25/100\n",
      "4063/4063 - 0s - loss: 0.1007 - accuracy: 0.9660\n",
      "Epoch 26/100\n",
      "4063/4063 - 0s - loss: 0.0679 - accuracy: 0.9771\n",
      "Epoch 27/100\n",
      "4063/4063 - 0s - loss: 0.0614 - accuracy: 0.9786\n",
      "Epoch 28/100\n",
      "4063/4063 - 0s - loss: 0.1386 - accuracy: 0.9518\n",
      "Epoch 29/100\n",
      "4063/4063 - 0s - loss: 0.1052 - accuracy: 0.9663\n",
      "Epoch 30/100\n",
      "4063/4063 - 0s - loss: 0.0493 - accuracy: 0.9825\n",
      "Epoch 31/100\n",
      "4063/4063 - 0s - loss: 0.0379 - accuracy: 0.9865\n",
      "Epoch 32/100\n",
      "4063/4063 - 0s - loss: 0.0278 - accuracy: 0.9916\n",
      "Epoch 33/100\n",
      "4063/4063 - 0s - loss: 0.0359 - accuracy: 0.9882\n",
      "Epoch 34/100\n",
      "4063/4063 - 0s - loss: 0.0561 - accuracy: 0.9798\n",
      "Epoch 35/100\n",
      "4063/4063 - 0s - loss: 0.0573 - accuracy: 0.9801\n",
      "Epoch 36/100\n",
      "4063/4063 - 0s - loss: 0.0458 - accuracy: 0.9852\n",
      "Epoch 37/100\n",
      "4063/4063 - 0s - loss: 0.0344 - accuracy: 0.9884\n",
      "Epoch 38/100\n",
      "4063/4063 - 0s - loss: 0.0227 - accuracy: 0.9914\n",
      "Epoch 39/100\n",
      "4063/4063 - 0s - loss: 0.0319 - accuracy: 0.9897\n",
      "Epoch 40/100\n",
      "4063/4063 - 0s - loss: 0.0862 - accuracy: 0.9727\n",
      "Epoch 41/100\n",
      "4063/4063 - 0s - loss: 0.0418 - accuracy: 0.9867\n",
      "Epoch 42/100\n",
      "4063/4063 - 0s - loss: 0.0434 - accuracy: 0.9862\n",
      "Epoch 43/100\n",
      "4063/4063 - 0s - loss: 0.0578 - accuracy: 0.9806\n",
      "Epoch 44/100\n",
      "4063/4063 - 0s - loss: 0.0528 - accuracy: 0.9803\n",
      "Epoch 45/100\n",
      "4063/4063 - 0s - loss: 0.0482 - accuracy: 0.9840\n",
      "Epoch 46/100\n",
      "4063/4063 - 0s - loss: 0.0240 - accuracy: 0.9931\n",
      "Epoch 47/100\n",
      "4063/4063 - 0s - loss: 0.0133 - accuracy: 0.9956\n",
      "Epoch 48/100\n",
      "4063/4063 - 0s - loss: 0.0127 - accuracy: 0.9956\n",
      "Epoch 49/100\n",
      "4063/4063 - 0s - loss: 0.0128 - accuracy: 0.9958\n",
      "Epoch 50/100\n",
      "4063/4063 - 0s - loss: 0.0157 - accuracy: 0.9948\n",
      "Epoch 51/100\n",
      "4063/4063 - 0s - loss: 0.0256 - accuracy: 0.9916\n",
      "Epoch 52/100\n",
      "4063/4063 - 0s - loss: 0.0996 - accuracy: 0.9687\n",
      "Epoch 53/100\n",
      "4063/4063 - 0s - loss: 0.0448 - accuracy: 0.9850\n",
      "Epoch 54/100\n",
      "4063/4063 - 0s - loss: 0.0487 - accuracy: 0.9833\n",
      "Epoch 55/100\n",
      "4063/4063 - 0s - loss: 0.0363 - accuracy: 0.9882\n",
      "Epoch 56/100\n",
      "4063/4063 - 0s - loss: 0.0257 - accuracy: 0.9934\n",
      "Epoch 57/100\n",
      "4063/4063 - 0s - loss: 0.0371 - accuracy: 0.9892\n",
      "Epoch 58/100\n",
      "4063/4063 - 0s - loss: 0.0354 - accuracy: 0.9884\n",
      "Epoch 59/100\n",
      "4063/4063 - 0s - loss: 0.0416 - accuracy: 0.9874\n",
      "Epoch 60/100\n",
      "4063/4063 - 0s - loss: 0.0189 - accuracy: 0.9929\n",
      "Epoch 61/100\n",
      "4063/4063 - 0s - loss: 0.0170 - accuracy: 0.9934\n",
      "Epoch 62/100\n",
      "4063/4063 - 0s - loss: 0.0128 - accuracy: 0.9953\n",
      "Epoch 63/100\n",
      "4063/4063 - 0s - loss: 0.0534 - accuracy: 0.9828\n",
      "Epoch 64/100\n",
      "4063/4063 - 0s - loss: 0.0272 - accuracy: 0.9906\n",
      "Epoch 65/100\n",
      "4063/4063 - 0s - loss: 0.0272 - accuracy: 0.9904\n",
      "Epoch 66/100\n",
      "4063/4063 - 0s - loss: 0.0511 - accuracy: 0.9847\n",
      "Epoch 67/100\n",
      "4063/4063 - 0s - loss: 0.0303 - accuracy: 0.9897\n",
      "Epoch 68/100\n",
      "4063/4063 - 0s - loss: 0.0402 - accuracy: 0.9872\n",
      "Epoch 69/100\n",
      "4063/4063 - 0s - loss: 0.0381 - accuracy: 0.9870\n",
      "Epoch 70/100\n",
      "4063/4063 - 0s - loss: 0.0323 - accuracy: 0.9897\n",
      "Epoch 71/100\n",
      "4063/4063 - 0s - loss: 0.0221 - accuracy: 0.9924\n",
      "Epoch 72/100\n",
      "4063/4063 - 0s - loss: 0.0220 - accuracy: 0.9921\n",
      "Epoch 73/100\n",
      "4063/4063 - 0s - loss: 0.0364 - accuracy: 0.9870\n",
      "Epoch 74/100\n",
      "4063/4063 - 0s - loss: 0.0180 - accuracy: 0.9941\n",
      "Epoch 75/100\n",
      "4063/4063 - 0s - loss: 0.0236 - accuracy: 0.9941\n",
      "Epoch 76/100\n",
      "4063/4063 - 0s - loss: 0.0233 - accuracy: 0.9911\n",
      "Epoch 77/100\n",
      "4063/4063 - 0s - loss: 0.0359 - accuracy: 0.9894\n",
      "Epoch 78/100\n",
      "4063/4063 - 0s - loss: 0.0276 - accuracy: 0.9909\n",
      "Epoch 79/100\n",
      "4063/4063 - 0s - loss: 0.0206 - accuracy: 0.9941\n",
      "Epoch 80/100\n",
      "4063/4063 - 0s - loss: 0.0173 - accuracy: 0.9938\n",
      "Epoch 81/100\n",
      "4063/4063 - 0s - loss: 0.0234 - accuracy: 0.9909\n",
      "Epoch 82/100\n",
      "4063/4063 - 0s - loss: 0.0243 - accuracy: 0.9916\n",
      "Epoch 83/100\n",
      "4063/4063 - 0s - loss: 0.0126 - accuracy: 0.9958\n",
      "Epoch 84/100\n",
      "4063/4063 - 0s - loss: 0.0420 - accuracy: 0.9847\n",
      "Epoch 85/100\n",
      "4063/4063 - 0s - loss: 0.0565 - accuracy: 0.9830\n",
      "Epoch 86/100\n",
      "4063/4063 - 0s - loss: 0.0202 - accuracy: 0.9929\n",
      "Epoch 87/100\n",
      "4063/4063 - 0s - loss: 0.0191 - accuracy: 0.9953\n",
      "Epoch 88/100\n",
      "4063/4063 - 0s - loss: 0.0216 - accuracy: 0.9924\n",
      "Epoch 89/100\n",
      "4063/4063 - 0s - loss: 0.0335 - accuracy: 0.9892\n",
      "Epoch 90/100\n",
      "4063/4063 - 0s - loss: 0.0314 - accuracy: 0.9904\n",
      "Epoch 91/100\n",
      "4063/4063 - 0s - loss: 0.0220 - accuracy: 0.9931\n",
      "Epoch 92/100\n",
      "4063/4063 - 0s - loss: 0.0175 - accuracy: 0.9943\n",
      "Epoch 93/100\n",
      "4063/4063 - 0s - loss: 0.0066 - accuracy: 0.9970\n",
      "Epoch 94/100\n",
      "4063/4063 - 0s - loss: 0.0189 - accuracy: 0.9936\n",
      "Epoch 95/100\n",
      "4063/4063 - 0s - loss: 0.0290 - accuracy: 0.9906\n",
      "Epoch 96/100\n",
      "4063/4063 - 0s - loss: 0.0399 - accuracy: 0.9877\n",
      "Epoch 97/100\n",
      "4063/4063 - 0s - loss: 0.0406 - accuracy: 0.9877\n",
      "Epoch 98/100\n",
      "4063/4063 - 0s - loss: 0.0196 - accuracy: 0.9943\n",
      "Epoch 99/100\n",
      "4063/4063 - 0s - loss: 0.0052 - accuracy: 0.9988\n",
      "Epoch 100/100\n",
      "4063/4063 - 0s - loss: 0.0045 - accuracy: 0.9983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22503e802b0>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=100,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1355/1 - 0s - loss: 1.1899 - accuracy: 0.8332\n",
      "Normal Neural Network - Loss: 1.3076625861804863, Accuracy: 0.8332103490829468\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agarw\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "C:\\Users\\agarw\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(24, return_sequences=True, input_shape=(None, 42))`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The added layer must be an instance of class Layer. Found: <keras.layers.recurrent.LSTM object at 0x000001A6A75C9BE0>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-134-d892dc121b60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    156\u001b[0m       raise TypeError('The added layer must be '\n\u001b[0;32m    157\u001b[0m                       \u001b[1;34m'an instance of class Layer. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m                       'Found: ' + str(layer))\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_no_legacy_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: The added layer must be an instance of class Layer. Found: <keras.layers.recurrent.LSTM object at 0x000001A6A75C9BE0>"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense,LSTM,Dropout\n",
    "\n",
    "np.random.seed(7)\n",
    "model = Sequential()\n",
    "model.add(LSTM(24, input_dim =42,return_sequences=True))\n",
    "model.add(LSTM(12))\n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('WISDM_ar_latest/WISDM_ar_v1.1/WISDM_ar_v1.1.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import LSTM, Dense\n",
    "# import numpy as np\n",
    "\n",
    "# data_dim = 6\n",
    "# timesteps = 100\n",
    "# num_classes = 10\n",
    "# batch_size = 32\n",
    "\n",
    "# # Expected input batch shape: (batch_size, timesteps, data_dim)\n",
    "# # Note that we have to provide the full batch_input_shape since the network is stateful.\n",
    "# # the sample of index i in batch k is the follow-up for the sample i in batch k-1.\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(32, return_sequences=True, stateful=True,\n",
    "#                batch_input_shape=(batch_size, timesteps, data_dim)))\n",
    "# model.add(LSTM(32, return_sequences=True, stateful=True))\n",
    "# model.add(LSTM(32, stateful=True))\n",
    "# model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Generate dummy training data\n",
    "# x_train = np.random.random((batch_size * 10, timesteps, data_dim))\n",
    "# y_train = np.random.random((batch_size * 10, num_classes))\n",
    "\n",
    "# # Generate dummy validation data\n",
    "# x_val = np.random.random((batch_size * 3, timesteps, data_dim))\n",
    "# y_val = np.random.random((batch_size * 3, num_classes))\n",
    "\n",
    "# model.fit(x_train, y_train,\n",
    "#           batch_size=batch_size, epochs=5, shuffle=False,\n",
    "#           validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>activity</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49105962326000</td>\n",
       "      <td>-0.694638</td>\n",
       "      <td>12.680544</td>\n",
       "      <td>0.503953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106062271000</td>\n",
       "      <td>5.012288</td>\n",
       "      <td>11.264028</td>\n",
       "      <td>0.953424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106112167000</td>\n",
       "      <td>4.903325</td>\n",
       "      <td>10.882658</td>\n",
       "      <td>-0.081722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106222305000</td>\n",
       "      <td>-0.612916</td>\n",
       "      <td>18.496431</td>\n",
       "      <td>3.023717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106332290000</td>\n",
       "      <td>-1.184970</td>\n",
       "      <td>12.108489</td>\n",
       "      <td>7.205164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098204</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623331483000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>-1.570000</td>\n",
       "      <td>1.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098205</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623371431000</td>\n",
       "      <td>9.040000</td>\n",
       "      <td>-1.460000</td>\n",
       "      <td>1.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098206</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623411592000</td>\n",
       "      <td>9.080000</td>\n",
       "      <td>-1.380000</td>\n",
       "      <td>1.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098207</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623491487000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>-1.460000</td>\n",
       "      <td>1.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098208</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623531465000</td>\n",
       "      <td>8.880000</td>\n",
       "      <td>-1.330000</td>\n",
       "      <td>1.610000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1098209 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id activity        timestamp     acc_x      acc_y     acc_z\n",
       "0             33  Jogging   49105962326000 -0.694638  12.680544  0.503953\n",
       "1             33  Jogging   49106062271000  5.012288  11.264028  0.953424\n",
       "2             33  Jogging   49106112167000  4.903325  10.882658 -0.081722\n",
       "3             33  Jogging   49106222305000 -0.612916  18.496431  3.023717\n",
       "4             33  Jogging   49106332290000 -1.184970  12.108489  7.205164\n",
       "...          ...      ...              ...       ...        ...       ...\n",
       "1098204       19  Sitting  131623331483000  9.000000  -1.570000  1.690000\n",
       "1098205       19  Sitting  131623371431000  9.040000  -1.460000  1.730000\n",
       "1098206       19  Sitting  131623411592000  9.080000  -1.380000  1.690000\n",
       "1098207       19  Sitting  131623491487000  9.000000  -1.460000  1.730000\n",
       "1098208       19  Sitting  131623531465000  8.880000  -1.330000  1.610000\n",
       "\n",
       "[1098209 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('WISDM_ar_latest/WISDM_ar_v1.1/WISDM_ar_v1.1.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>activity</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>ActivityEncoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49105962326000</td>\n",
       "      <td>-0.694638</td>\n",
       "      <td>12.680544</td>\n",
       "      <td>0.503953</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106062271000</td>\n",
       "      <td>5.012288</td>\n",
       "      <td>11.264028</td>\n",
       "      <td>0.953424</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106112167000</td>\n",
       "      <td>4.903325</td>\n",
       "      <td>10.882658</td>\n",
       "      <td>-0.081722</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106222305000</td>\n",
       "      <td>-0.612916</td>\n",
       "      <td>18.496431</td>\n",
       "      <td>3.023717</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106332290000</td>\n",
       "      <td>-1.184970</td>\n",
       "      <td>12.108489</td>\n",
       "      <td>7.205164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106442306000</td>\n",
       "      <td>1.375655</td>\n",
       "      <td>-2.492524</td>\n",
       "      <td>-6.510526</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106542312000</td>\n",
       "      <td>-0.612916</td>\n",
       "      <td>10.569390</td>\n",
       "      <td>5.706926</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106652389000</td>\n",
       "      <td>-0.503953</td>\n",
       "      <td>13.947236</td>\n",
       "      <td>7.055340</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106762313000</td>\n",
       "      <td>-8.430995</td>\n",
       "      <td>11.413852</td>\n",
       "      <td>5.134871</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106872299000</td>\n",
       "      <td>0.953424</td>\n",
       "      <td>1.375655</td>\n",
       "      <td>1.648062</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>33</td>\n",
       "      <td>Walking</td>\n",
       "      <td>49394992294000</td>\n",
       "      <td>0.844462</td>\n",
       "      <td>8.008764</td>\n",
       "      <td>2.792171</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>33</td>\n",
       "      <td>Walking</td>\n",
       "      <td>49395102310000</td>\n",
       "      <td>1.116869</td>\n",
       "      <td>8.621680</td>\n",
       "      <td>3.786457</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>33</td>\n",
       "      <td>Walking</td>\n",
       "      <td>49395202316000</td>\n",
       "      <td>-0.503953</td>\n",
       "      <td>16.657684</td>\n",
       "      <td>1.307553</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>33</td>\n",
       "      <td>Walking</td>\n",
       "      <td>49395302292000</td>\n",
       "      <td>4.794363</td>\n",
       "      <td>10.760075</td>\n",
       "      <td>-1.184970</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>33</td>\n",
       "      <td>Walking</td>\n",
       "      <td>49395412338000</td>\n",
       "      <td>-0.040861</td>\n",
       "      <td>9.234595</td>\n",
       "      <td>-0.694638</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>33</td>\n",
       "      <td>Walking</td>\n",
       "      <td>49395522293000</td>\n",
       "      <td>2.492524</td>\n",
       "      <td>8.730643</td>\n",
       "      <td>-1.457377</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>33</td>\n",
       "      <td>Walking</td>\n",
       "      <td>49395632309000</td>\n",
       "      <td>0.531194</td>\n",
       "      <td>9.888372</td>\n",
       "      <td>-1.225831</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>33</td>\n",
       "      <td>Walking</td>\n",
       "      <td>49395742294000</td>\n",
       "      <td>1.757025</td>\n",
       "      <td>11.032481</td>\n",
       "      <td>-0.653777</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>33</td>\n",
       "      <td>Walking</td>\n",
       "      <td>49395852340000</td>\n",
       "      <td>2.982856</td>\n",
       "      <td>12.149350</td>\n",
       "      <td>-1.307553</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>33</td>\n",
       "      <td>Walking</td>\n",
       "      <td>49395962295000</td>\n",
       "      <td>-0.803601</td>\n",
       "      <td>12.721405</td>\n",
       "      <td>-1.266692</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>33</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>49560572311000</td>\n",
       "      <td>10.119919</td>\n",
       "      <td>4.331271</td>\n",
       "      <td>-3.786457</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>33</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>49560682449000</td>\n",
       "      <td>4.862464</td>\n",
       "      <td>3.909040</td>\n",
       "      <td>-2.792171</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>33</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>49560782303000</td>\n",
       "      <td>9.466142</td>\n",
       "      <td>13.402422</td>\n",
       "      <td>-3.827318</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>33</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>49560842209000</td>\n",
       "      <td>5.747787</td>\n",
       "      <td>7.627395</td>\n",
       "      <td>-3.146300</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>33</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>49560942184000</td>\n",
       "      <td>2.301839</td>\n",
       "      <td>8.921328</td>\n",
       "      <td>-3.023717</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>33</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>49561002456000</td>\n",
       "      <td>7.818079</td>\n",
       "      <td>7.055340</td>\n",
       "      <td>-4.331271</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>33</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>49561062118000</td>\n",
       "      <td>2.833032</td>\n",
       "      <td>8.539958</td>\n",
       "      <td>-3.336985</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>33</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>49561162613000</td>\n",
       "      <td>1.797886</td>\n",
       "      <td>10.950760</td>\n",
       "      <td>-3.105439</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>33</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>49561262283000</td>\n",
       "      <td>0.994285</td>\n",
       "      <td>8.771504</td>\n",
       "      <td>-3.568531</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>33</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>49561332321000</td>\n",
       "      <td>1.225831</td>\n",
       "      <td>9.466142</td>\n",
       "      <td>-3.527670</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>33</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>49646322311000</td>\n",
       "      <td>-0.040861</td>\n",
       "      <td>4.985047</td>\n",
       "      <td>6.510526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>33</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>49646422317000</td>\n",
       "      <td>-0.463092</td>\n",
       "      <td>4.372132</td>\n",
       "      <td>7.436710</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>33</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>49646522323000</td>\n",
       "      <td>-0.299648</td>\n",
       "      <td>4.603678</td>\n",
       "      <td>6.510526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>33</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>49646572281000</td>\n",
       "      <td>-0.272407</td>\n",
       "      <td>4.481094</td>\n",
       "      <td>6.360703</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>33</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>49646672317000</td>\n",
       "      <td>-1.525479</td>\n",
       "      <td>5.175732</td>\n",
       "      <td>7.164303</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>33</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>49646782303000</td>\n",
       "      <td>1.266692</td>\n",
       "      <td>6.782933</td>\n",
       "      <td>8.853226</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>33</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>49646882461000</td>\n",
       "      <td>-1.116869</td>\n",
       "      <td>1.457377</td>\n",
       "      <td>7.082581</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>33</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>49646982315000</td>\n",
       "      <td>-0.381370</td>\n",
       "      <td>0.762740</td>\n",
       "      <td>7.545672</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>33</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>49647082260000</td>\n",
       "      <td>-0.653777</td>\n",
       "      <td>2.369940</td>\n",
       "      <td>7.273266</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>33</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>49647192306000</td>\n",
       "      <td>0.272407</td>\n",
       "      <td>1.648062</td>\n",
       "      <td>8.117727</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221335</th>\n",
       "      <td>27</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>12363992261000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>9.660000</td>\n",
       "      <td>1.035146</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221336</th>\n",
       "      <td>27</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>12364042279000</td>\n",
       "      <td>2.680000</td>\n",
       "      <td>9.530000</td>\n",
       "      <td>0.503953</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221337</th>\n",
       "      <td>27</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>12364092267000</td>\n",
       "      <td>3.490000</td>\n",
       "      <td>8.890000</td>\n",
       "      <td>0.762740</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221338</th>\n",
       "      <td>27</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>12364142316000</td>\n",
       "      <td>3.640000</td>\n",
       "      <td>9.380000</td>\n",
       "      <td>0.926184</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221339</th>\n",
       "      <td>27</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>12364192273000</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>9.340000</td>\n",
       "      <td>1.035146</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221340</th>\n",
       "      <td>27</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>12364242292000</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>9.430000</td>\n",
       "      <td>0.885323</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221341</th>\n",
       "      <td>27</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>12364292249000</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>9.530000</td>\n",
       "      <td>0.844462</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221342</th>\n",
       "      <td>27</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>12364342267000</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>9.720000</td>\n",
       "      <td>0.844462</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221343</th>\n",
       "      <td>27</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>12364392285000</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>9.380000</td>\n",
       "      <td>0.926184</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221344</th>\n",
       "      <td>27</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>12364442273000</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>9.430000</td>\n",
       "      <td>0.762740</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223434</th>\n",
       "      <td>27</td>\n",
       "      <td>Standing</td>\n",
       "      <td>12535892255000</td>\n",
       "      <td>-1.880000</td>\n",
       "      <td>9.850000</td>\n",
       "      <td>-0.231546</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223435</th>\n",
       "      <td>27</td>\n",
       "      <td>Standing</td>\n",
       "      <td>12535942273000</td>\n",
       "      <td>-0.190000</td>\n",
       "      <td>9.920000</td>\n",
       "      <td>-0.572055</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223436</th>\n",
       "      <td>27</td>\n",
       "      <td>Standing</td>\n",
       "      <td>12535992231000</td>\n",
       "      <td>-0.610000</td>\n",
       "      <td>10.270000</td>\n",
       "      <td>-0.885323</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223437</th>\n",
       "      <td>27</td>\n",
       "      <td>Standing</td>\n",
       "      <td>12536042310000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>10.570000</td>\n",
       "      <td>-1.757025</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223438</th>\n",
       "      <td>27</td>\n",
       "      <td>Standing</td>\n",
       "      <td>12536092206000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>9.470000</td>\n",
       "      <td>-1.116869</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223439</th>\n",
       "      <td>27</td>\n",
       "      <td>Standing</td>\n",
       "      <td>12536142285000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>9.470000</td>\n",
       "      <td>-1.947710</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223440</th>\n",
       "      <td>27</td>\n",
       "      <td>Standing</td>\n",
       "      <td>12536192243000</td>\n",
       "      <td>-1.040000</td>\n",
       "      <td>10.650000</td>\n",
       "      <td>-1.525479</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223441</th>\n",
       "      <td>27</td>\n",
       "      <td>Standing</td>\n",
       "      <td>12536242261000</td>\n",
       "      <td>-1.920000</td>\n",
       "      <td>9.510000</td>\n",
       "      <td>-0.572055</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223442</th>\n",
       "      <td>27</td>\n",
       "      <td>Standing</td>\n",
       "      <td>12536292249000</td>\n",
       "      <td>-1.310000</td>\n",
       "      <td>9.850000</td>\n",
       "      <td>-0.531194</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223443</th>\n",
       "      <td>27</td>\n",
       "      <td>Standing</td>\n",
       "      <td>12536342298000</td>\n",
       "      <td>-0.080000</td>\n",
       "      <td>9.920000</td>\n",
       "      <td>-1.757025</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id    activity       timestamp      acc_x      acc_y     acc_z  \\\n",
       "0            33     Jogging  49105962326000  -0.694638  12.680544  0.503953   \n",
       "1            33     Jogging  49106062271000   5.012288  11.264028  0.953424   \n",
       "2            33     Jogging  49106112167000   4.903325  10.882658 -0.081722   \n",
       "3            33     Jogging  49106222305000  -0.612916  18.496431  3.023717   \n",
       "4            33     Jogging  49106332290000  -1.184970  12.108489  7.205164   \n",
       "5            33     Jogging  49106442306000   1.375655  -2.492524 -6.510526   \n",
       "6            33     Jogging  49106542312000  -0.612916  10.569390  5.706926   \n",
       "7            33     Jogging  49106652389000  -0.503953  13.947236  7.055340   \n",
       "8            33     Jogging  49106762313000  -8.430995  11.413852  5.134871   \n",
       "9            33     Jogging  49106872299000   0.953424   1.375655  1.648062   \n",
       "597          33     Walking  49394992294000   0.844462   8.008764  2.792171   \n",
       "598          33     Walking  49395102310000   1.116869   8.621680  3.786457   \n",
       "599          33     Walking  49395202316000  -0.503953  16.657684  1.307553   \n",
       "600          33     Walking  49395302292000   4.794363  10.760075 -1.184970   \n",
       "601          33     Walking  49395412338000  -0.040861   9.234595 -0.694638   \n",
       "602          33     Walking  49395522293000   2.492524   8.730643 -1.457377   \n",
       "603          33     Walking  49395632309000   0.531194   9.888372 -1.225831   \n",
       "604          33     Walking  49395742294000   1.757025  11.032481 -0.653777   \n",
       "605          33     Walking  49395852340000   2.982856  12.149350 -1.307553   \n",
       "606          33     Walking  49395962295000  -0.803601  12.721405 -1.266692   \n",
       "1161         33    Upstairs  49560572311000  10.119919   4.331271 -3.786457   \n",
       "1162         33    Upstairs  49560682449000   4.862464   3.909040 -2.792171   \n",
       "1163         33    Upstairs  49560782303000   9.466142  13.402422 -3.827318   \n",
       "1164         33    Upstairs  49560842209000   5.747787   7.627395 -3.146300   \n",
       "1165         33    Upstairs  49560942184000   2.301839   8.921328 -3.023717   \n",
       "1166         33    Upstairs  49561002456000   7.818079   7.055340 -4.331271   \n",
       "1167         33    Upstairs  49561062118000   2.833032   8.539958 -3.336985   \n",
       "1168         33    Upstairs  49561162613000   1.797886  10.950760 -3.105439   \n",
       "1169         33    Upstairs  49561262283000   0.994285   8.771504 -3.568531   \n",
       "1170         33    Upstairs  49561332321000   1.225831   9.466142 -3.527670   \n",
       "1769         33  Downstairs  49646322311000  -0.040861   4.985047  6.510526   \n",
       "1770         33  Downstairs  49646422317000  -0.463092   4.372132  7.436710   \n",
       "1771         33  Downstairs  49646522323000  -0.299648   4.603678  6.510526   \n",
       "1772         33  Downstairs  49646572281000  -0.272407   4.481094  6.360703   \n",
       "1773         33  Downstairs  49646672317000  -1.525479   5.175732  7.164303   \n",
       "1774         33  Downstairs  49646782303000   1.266692   6.782933  8.853226   \n",
       "1775         33  Downstairs  49646882461000  -1.116869   1.457377  7.082581   \n",
       "1776         33  Downstairs  49646982315000  -0.381370   0.762740  7.545672   \n",
       "1777         33  Downstairs  49647082260000  -0.653777   2.369940  7.273266   \n",
       "1778         33  Downstairs  49647192306000   0.272407   1.648062  8.117727   \n",
       "221335       27     Sitting  12363992261000   2.600000   9.660000  1.035146   \n",
       "221336       27     Sitting  12364042279000   2.680000   9.530000  0.503953   \n",
       "221337       27     Sitting  12364092267000   3.490000   8.890000  0.762740   \n",
       "221338       27     Sitting  12364142316000   3.640000   9.380000  0.926184   \n",
       "221339       27     Sitting  12364192273000   2.910000   9.340000  1.035146   \n",
       "221340       27     Sitting  12364242292000   3.150000   9.430000  0.885323   \n",
       "221341       27     Sitting  12364292249000   3.150000   9.530000  0.844462   \n",
       "221342       27     Sitting  12364342267000   2.910000   9.720000  0.844462   \n",
       "221343       27     Sitting  12364392285000   3.210000   9.380000  0.926184   \n",
       "221344       27     Sitting  12364442273000   3.150000   9.430000  0.762740   \n",
       "223434       27    Standing  12535892255000  -1.880000   9.850000 -0.231546   \n",
       "223435       27    Standing  12535942273000  -0.190000   9.920000 -0.572055   \n",
       "223436       27    Standing  12535992231000  -0.610000  10.270000 -0.885323   \n",
       "223437       27    Standing  12536042310000   0.760000  10.570000 -1.757025   \n",
       "223438       27    Standing  12536092206000   0.420000   9.470000 -1.116869   \n",
       "223439       27    Standing  12536142285000   0.380000   9.470000 -1.947710   \n",
       "223440       27    Standing  12536192243000  -1.040000  10.650000 -1.525479   \n",
       "223441       27    Standing  12536242261000  -1.920000   9.510000 -0.572055   \n",
       "223442       27    Standing  12536292249000  -1.310000   9.850000 -0.531194   \n",
       "223443       27    Standing  12536342298000  -0.080000   9.920000 -1.757025   \n",
       "\n",
       "        ActivityEncoded  \n",
       "0                     1  \n",
       "1                     1  \n",
       "2                     1  \n",
       "3                     1  \n",
       "4                     1  \n",
       "5                     1  \n",
       "6                     1  \n",
       "7                     1  \n",
       "8                     1  \n",
       "9                     1  \n",
       "597                   5  \n",
       "598                   5  \n",
       "599                   5  \n",
       "600                   5  \n",
       "601                   5  \n",
       "602                   5  \n",
       "603                   5  \n",
       "604                   5  \n",
       "605                   5  \n",
       "606                   5  \n",
       "1161                  4  \n",
       "1162                  4  \n",
       "1163                  4  \n",
       "1164                  4  \n",
       "1165                  4  \n",
       "1166                  4  \n",
       "1167                  4  \n",
       "1168                  4  \n",
       "1169                  4  \n",
       "1170                  4  \n",
       "1769                  0  \n",
       "1770                  0  \n",
       "1771                  0  \n",
       "1772                  0  \n",
       "1773                  0  \n",
       "1774                  0  \n",
       "1775                  0  \n",
       "1776                  0  \n",
       "1777                  0  \n",
       "1778                  0  \n",
       "221335                2  \n",
       "221336                2  \n",
       "221337                2  \n",
       "221338                2  \n",
       "221339                2  \n",
       "221340                2  \n",
       "221341                2  \n",
       "221342                2  \n",
       "221343                2  \n",
       "221344                2  \n",
       "223434                3  \n",
       "223435                3  \n",
       "223436                3  \n",
       "223437                3  \n",
       "223438                3  \n",
       "223439                3  \n",
       "223440                3  \n",
       "223441                3  \n",
       "223442                3  \n",
       "223443                3  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "# Define column name of the label vector\n",
    "LABEL = 'ActivityEncoded'\n",
    "# Transform the labels from String to Integer via LabelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Add a new column to the existing DataFrame with the encoded values\n",
    "data[LABEL] = le.fit_transform(data['activity'].values.ravel())\n",
    "# dummy=data_reformat.groupby(['class'])\n",
    "# dummy.head(10)\n",
    "\n",
    "data\n",
    "\n",
    "dummy2=data.groupby(['activity'])\n",
    "dummy2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data[data['user_id'] >20]\n",
    "data_test =  data[data['user_id'] <=20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>ActivityEncoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>-0.694638</td>\n",
       "      <td>12.680544</td>\n",
       "      <td>0.503953</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>5.012288</td>\n",
       "      <td>11.264028</td>\n",
       "      <td>0.953424</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>4.903325</td>\n",
       "      <td>10.882658</td>\n",
       "      <td>-0.081722</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>-0.612916</td>\n",
       "      <td>18.496431</td>\n",
       "      <td>3.023717</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>-1.184970</td>\n",
       "      <td>12.108489</td>\n",
       "      <td>7.205164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997750</th>\n",
       "      <td>25</td>\n",
       "      <td>10.530000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997751</th>\n",
       "      <td>25</td>\n",
       "      <td>3.170000</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>4.630000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997752</th>\n",
       "      <td>25</td>\n",
       "      <td>-1.040000</td>\n",
       "      <td>15.790000</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997753</th>\n",
       "      <td>25</td>\n",
       "      <td>6.130000</td>\n",
       "      <td>10.230000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997754</th>\n",
       "      <td>25</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>9.230000</td>\n",
       "      <td>-1.310000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>474942 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id      acc_x      acc_y     acc_z  ActivityEncoded\n",
       "0            33  -0.694638  12.680544  0.503953                1\n",
       "1            33   5.012288  11.264028  0.953424                1\n",
       "2            33   4.903325  10.882658 -0.081722                1\n",
       "3            33  -0.612916  18.496431  3.023717                1\n",
       "4            33  -1.184970  12.108489  7.205164                1\n",
       "...         ...        ...        ...       ...              ...\n",
       "997750       25  10.530000  14.900000  7.400000                1\n",
       "997751       25   3.170000  15.700000  4.630000                1\n",
       "997752       25  -1.040000  15.790000 -4.250000                1\n",
       "997753       25   6.130000  10.230000  0.500000                1\n",
       "997754       25   0.080000   9.230000 -1.310000                1\n",
       "\n",
       "[474942 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train\n",
    "data_train_filter = data_train[['user_id','acc_x','acc_y','acc_z','ActivityEncoded']]\n",
    "data_train_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8       , 0.47814364, 0.81438951, 0.51284938, 0.2       ],\n",
       "       [0.8       , 0.62240364, 0.77866401, 0.52430965, 0.2       ],\n",
       "       [0.8       , 0.61964927, 0.7690456 , 0.49791632, 0.2       ],\n",
       "       ...,\n",
       "       [0.26666667, 0.46941355, 0.89281211, 0.39163692, 0.2       ],\n",
       "       [0.26666667, 0.65065723, 0.75258512, 0.5127486 , 0.2       ],\n",
       "       [0.26666667, 0.49772497, 0.72736444, 0.46659867, 0.2       ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "data_training_scaled = scaler.fit_transform(data_train_filter)\n",
    "data_training_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-751adf6427fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_training_scaled\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mN_TIME_STEPS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_training_scaled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc_x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mN_TIME_STEPS\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_training_scaled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc_y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mN_TIME_STEPS\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mzs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_training_scaled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc_z'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mN_TIME_STEPS\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# N_TIME_STEPS = 200\n",
    "# N_FEATURES = 3\n",
    "# step = 20\n",
    "# segments = []\n",
    "# labels = []\n",
    "# for i in range(0, len(data_training_scaled) - N_TIME_STEPS, step):\n",
    "#     xs = data_training_scaled['acc_x'].values[i: i + N_TIME_STEPS]\n",
    "#     ys = data_training_scaled['acc_y'].values[i: i + N_TIME_STEPS]\n",
    "#     zs = data_training_scaled['acc_z'].values[i: i + N_TIME_STEPS]\n",
    "#     label = stats.mode(data_training_scaled['ActivityEncoded'][i: i + N_TIME_STEPS])[0][0]\n",
    "#     segments.append([xs, ys, zs])\n",
    "#     labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_set = []\n",
    "labels = []\n",
    "for i in range(200, 474942):\n",
    "    features_set.append(data_training_scaled[i-200:i, 0])\n",
    "    labels.append(data_training_scaled[i, 0])\n",
    "    \n",
    "labels\n",
    "# features_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_set, labels = np.array(features_set), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_set = np.reshape(features_set, (features_set.shape[0], features_set.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.8       ],\n",
       "        [0.8       ],\n",
       "        [0.8       ],\n",
       "        ...,\n",
       "        [0.8       ],\n",
       "        [0.8       ],\n",
       "        [0.8       ]],\n",
       "\n",
       "       [[0.8       ],\n",
       "        [0.8       ],\n",
       "        [0.8       ],\n",
       "        ...,\n",
       "        [0.8       ],\n",
       "        [0.8       ],\n",
       "        [0.8       ]],\n",
       "\n",
       "       [[0.8       ],\n",
       "        [0.8       ],\n",
       "        [0.8       ],\n",
       "        ...,\n",
       "        [0.8       ],\n",
       "        [0.8       ],\n",
       "        [0.8       ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.26666667],\n",
       "        [0.26666667],\n",
       "        [0.26666667],\n",
       "        ...,\n",
       "        [0.26666667],\n",
       "        [0.26666667],\n",
       "        [0.26666667]],\n",
       "\n",
       "       [[0.26666667],\n",
       "        [0.26666667],\n",
       "        [0.26666667],\n",
       "        ...,\n",
       "        [0.26666667],\n",
       "        [0.26666667],\n",
       "        [0.26666667]],\n",
       "\n",
       "       [[0.26666667],\n",
       "        [0.26666667],\n",
       "        [0.26666667],\n",
       "        ...,\n",
       "        [0.26666667],\n",
       "        [0.26666667],\n",
       "        [0.26666667]]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(features_set.shape[1], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 33024/474742 [=>............................] - ETA: 2:19:09 - loss: 0.0045"
     ]
    }
   ],
   "source": [
    "model.fit(features_set, labels, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
