{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pandas import DataFrame\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reformat = pd.read_csv('WISDM_ar_latest/WISDM_ar_v1.1/reformat_data.csv')\n",
    "data_reformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>...</th>\n",
       "      <th>zpeak</th>\n",
       "      <th>xabsoldev</th>\n",
       "      <th>yabsoldev</th>\n",
       "      <th>zabsoldev</th>\n",
       "      <th>xstandardev</th>\n",
       "      <th>ystandarddev</th>\n",
       "      <th>zstandarddev</th>\n",
       "      <th>resultant</th>\n",
       "      <th>class</th>\n",
       "      <th>ActivityEncoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>1550.00</td>\n",
       "      <td>3.29</td>\n",
       "      <td>7.21</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.05</td>\n",
       "      <td>8.17</td>\n",
       "      <td>4.05</td>\n",
       "      <td>11.96</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>1233.33</td>\n",
       "      <td>4.23</td>\n",
       "      <td>6.88</td>\n",
       "      <td>4.05</td>\n",
       "      <td>5.43</td>\n",
       "      <td>8.19</td>\n",
       "      <td>5.43</td>\n",
       "      <td>12.05</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>1780.00</td>\n",
       "      <td>4.18</td>\n",
       "      <td>6.89</td>\n",
       "      <td>4.07</td>\n",
       "      <td>5.55</td>\n",
       "      <td>8.19</td>\n",
       "      <td>5.55</td>\n",
       "      <td>11.99</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>1380.00</td>\n",
       "      <td>2.26</td>\n",
       "      <td>4.13</td>\n",
       "      <td>2.49</td>\n",
       "      <td>2.87</td>\n",
       "      <td>4.95</td>\n",
       "      <td>2.87</td>\n",
       "      <td>10.69</td>\n",
       "      <td>Walking</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>...</td>\n",
       "      <td>1775.00</td>\n",
       "      <td>2.29</td>\n",
       "      <td>3.94</td>\n",
       "      <td>2.41</td>\n",
       "      <td>3.08</td>\n",
       "      <td>4.64</td>\n",
       "      <td>3.08</td>\n",
       "      <td>10.80</td>\n",
       "      <td>Walking</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>2675.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.39</td>\n",
       "      <td>1.94</td>\n",
       "      <td>3.13</td>\n",
       "      <td>4.95</td>\n",
       "      <td>3.13</td>\n",
       "      <td>8.63</td>\n",
       "      <td>Walking</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>1825.00</td>\n",
       "      <td>1.56</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.72</td>\n",
       "      <td>2.09</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2.09</td>\n",
       "      <td>9.87</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>2983.33</td>\n",
       "      <td>1.84</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1.41</td>\n",
       "      <td>2.22</td>\n",
       "      <td>3.35</td>\n",
       "      <td>2.22</td>\n",
       "      <td>9.91</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>33</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>1533.33</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2.68</td>\n",
       "      <td>9.78</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>33</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.85</td>\n",
       "      <td>9.34</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.39</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>33</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>175.00</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.77</td>\n",
       "      <td>2.39</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.60</td>\n",
       "      <td>2.60</td>\n",
       "      <td>9.52</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>33</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>1460.00</td>\n",
       "      <td>1.62</td>\n",
       "      <td>3.03</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.98</td>\n",
       "      <td>3.74</td>\n",
       "      <td>1.98</td>\n",
       "      <td>9.62</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>2700.00</td>\n",
       "      <td>1.79</td>\n",
       "      <td>2.98</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.24</td>\n",
       "      <td>3.70</td>\n",
       "      <td>2.24</td>\n",
       "      <td>9.89</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.97</td>\n",
       "      <td>9.96</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.40</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>33</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>3900.00</td>\n",
       "      <td>1.59</td>\n",
       "      <td>2.71</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2.05</td>\n",
       "      <td>9.69</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>3525.00</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.93</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.78</td>\n",
       "      <td>3.62</td>\n",
       "      <td>2.78</td>\n",
       "      <td>9.75</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>33</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>1833.33</td>\n",
       "      <td>1.44</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2.75</td>\n",
       "      <td>4.86</td>\n",
       "      <td>2.75</td>\n",
       "      <td>6.42</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>33</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>1562.50</td>\n",
       "      <td>1.45</td>\n",
       "      <td>3.03</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1.82</td>\n",
       "      <td>3.82</td>\n",
       "      <td>1.82</td>\n",
       "      <td>9.84</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>33</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>4075.00</td>\n",
       "      <td>1.69</td>\n",
       "      <td>3.14</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.11</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.11</td>\n",
       "      <td>9.90</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>33</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>2175.00</td>\n",
       "      <td>1.14</td>\n",
       "      <td>2.39</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.33</td>\n",
       "      <td>4.96</td>\n",
       "      <td>2.33</td>\n",
       "      <td>5.78</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>33</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.18</td>\n",
       "      <td>...</td>\n",
       "      <td>1640.00</td>\n",
       "      <td>1.21</td>\n",
       "      <td>2.77</td>\n",
       "      <td>1.39</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.36</td>\n",
       "      <td>1.55</td>\n",
       "      <td>9.67</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>33</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>3450.00</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.96</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.03</td>\n",
       "      <td>3.73</td>\n",
       "      <td>2.03</td>\n",
       "      <td>9.84</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>33</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>1916.67</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.51</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.42</td>\n",
       "      <td>4.28</td>\n",
       "      <td>3.42</td>\n",
       "      <td>7.23</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>2075.00</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.27</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.13</td>\n",
       "      <td>3.95</td>\n",
       "      <td>2.13</td>\n",
       "      <td>9.79</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>33</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>1660.00</td>\n",
       "      <td>1.86</td>\n",
       "      <td>3.28</td>\n",
       "      <td>2.03</td>\n",
       "      <td>2.41</td>\n",
       "      <td>4.09</td>\n",
       "      <td>2.41</td>\n",
       "      <td>10.05</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>33</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>1500.00</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.58</td>\n",
       "      <td>1.51</td>\n",
       "      <td>2.21</td>\n",
       "      <td>4.59</td>\n",
       "      <td>2.21</td>\n",
       "      <td>7.37</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>3075.00</td>\n",
       "      <td>4.57</td>\n",
       "      <td>6.50</td>\n",
       "      <td>3.94</td>\n",
       "      <td>5.65</td>\n",
       "      <td>7.69</td>\n",
       "      <td>5.65</td>\n",
       "      <td>12.85</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>33</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>2700.00</td>\n",
       "      <td>5.70</td>\n",
       "      <td>6.37</td>\n",
       "      <td>3.73</td>\n",
       "      <td>6.82</td>\n",
       "      <td>7.58</td>\n",
       "      <td>6.82</td>\n",
       "      <td>13.93</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>33</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>1875.00</td>\n",
       "      <td>4.23</td>\n",
       "      <td>5.49</td>\n",
       "      <td>3.18</td>\n",
       "      <td>6.55</td>\n",
       "      <td>8.20</td>\n",
       "      <td>6.55</td>\n",
       "      <td>10.61</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>1600.00</td>\n",
       "      <td>2.55</td>\n",
       "      <td>4.17</td>\n",
       "      <td>2.27</td>\n",
       "      <td>3.04</td>\n",
       "      <td>5.02</td>\n",
       "      <td>3.04</td>\n",
       "      <td>10.68</td>\n",
       "      <td>Walking</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>33</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>1650.00</td>\n",
       "      <td>2.64</td>\n",
       "      <td>3.86</td>\n",
       "      <td>2.41</td>\n",
       "      <td>3.12</td>\n",
       "      <td>4.69</td>\n",
       "      <td>3.12</td>\n",
       "      <td>10.86</td>\n",
       "      <td>Walking</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>33</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>1320.00</td>\n",
       "      <td>2.29</td>\n",
       "      <td>3.78</td>\n",
       "      <td>2.28</td>\n",
       "      <td>3.09</td>\n",
       "      <td>4.96</td>\n",
       "      <td>3.09</td>\n",
       "      <td>9.67</td>\n",
       "      <td>Walking</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>33</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>...</td>\n",
       "      <td>1600.00</td>\n",
       "      <td>4.60</td>\n",
       "      <td>6.35</td>\n",
       "      <td>3.72</td>\n",
       "      <td>5.88</td>\n",
       "      <td>7.73</td>\n",
       "      <td>5.88</td>\n",
       "      <td>13.26</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>1700.00</td>\n",
       "      <td>6.06</td>\n",
       "      <td>6.60</td>\n",
       "      <td>3.33</td>\n",
       "      <td>7.15</td>\n",
       "      <td>7.80</td>\n",
       "      <td>7.15</td>\n",
       "      <td>14.48</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>33</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>1683.33</td>\n",
       "      <td>6.24</td>\n",
       "      <td>6.73</td>\n",
       "      <td>3.36</td>\n",
       "      <td>7.32</td>\n",
       "      <td>7.90</td>\n",
       "      <td>7.32</td>\n",
       "      <td>14.28</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>33</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>4000.00</td>\n",
       "      <td>6.65</td>\n",
       "      <td>7.05</td>\n",
       "      <td>4.12</td>\n",
       "      <td>7.66</td>\n",
       "      <td>8.49</td>\n",
       "      <td>7.66</td>\n",
       "      <td>14.63</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>17</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>1500.00</td>\n",
       "      <td>2.04</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.20</td>\n",
       "      <td>2.80</td>\n",
       "      <td>10.53</td>\n",
       "      <td>Walking</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>...</td>\n",
       "      <td>2516.67</td>\n",
       "      <td>2.98</td>\n",
       "      <td>2.82</td>\n",
       "      <td>1.74</td>\n",
       "      <td>3.51</td>\n",
       "      <td>3.35</td>\n",
       "      <td>3.51</td>\n",
       "      <td>10.55</td>\n",
       "      <td>Walking</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>17</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>...</td>\n",
       "      <td>725.00</td>\n",
       "      <td>2.96</td>\n",
       "      <td>2.94</td>\n",
       "      <td>1.82</td>\n",
       "      <td>3.58</td>\n",
       "      <td>3.52</td>\n",
       "      <td>3.58</td>\n",
       "      <td>10.61</td>\n",
       "      <td>Walking</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>51</td>\n",
       "      <td>17</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>857.14</td>\n",
       "      <td>2.84</td>\n",
       "      <td>2.83</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.44</td>\n",
       "      <td>3.53</td>\n",
       "      <td>10.56</td>\n",
       "      <td>Walking</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>428</td>\n",
       "      <td>27</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>275.00</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.13</td>\n",
       "      <td>0.21</td>\n",
       "      <td>3.13</td>\n",
       "      <td>9.91</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>429</td>\n",
       "      <td>27</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>1037.50</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.07</td>\n",
       "      <td>3.16</td>\n",
       "      <td>9.90</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>430</td>\n",
       "      <td>27</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>491.67</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3.16</td>\n",
       "      <td>9.90</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>431</td>\n",
       "      <td>27</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>351.79</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.21</td>\n",
       "      <td>0.08</td>\n",
       "      <td>3.21</td>\n",
       "      <td>9.90</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>432</td>\n",
       "      <td>27</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>1900.00</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3.23</td>\n",
       "      <td>9.91</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>433</td>\n",
       "      <td>27</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>1770.00</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.12</td>\n",
       "      <td>3.28</td>\n",
       "      <td>9.92</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>434</td>\n",
       "      <td>27</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>...</td>\n",
       "      <td>925.00</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.07</td>\n",
       "      <td>3.28</td>\n",
       "      <td>9.92</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>435</td>\n",
       "      <td>27</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>...</td>\n",
       "      <td>1162.50</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.06</td>\n",
       "      <td>3.29</td>\n",
       "      <td>9.92</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>436</td>\n",
       "      <td>27</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>825.00</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>3.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>3.21</td>\n",
       "      <td>9.89</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>437</td>\n",
       "      <td>27</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>...</td>\n",
       "      <td>543.75</td>\n",
       "      <td>3.21</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3.21</td>\n",
       "      <td>0.06</td>\n",
       "      <td>3.21</td>\n",
       "      <td>9.89</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>439</td>\n",
       "      <td>27</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>525.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.18</td>\n",
       "      <td>10.09</td>\n",
       "      <td>Standing</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>440</td>\n",
       "      <td>27</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>1750.00</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.05</td>\n",
       "      <td>10.06</td>\n",
       "      <td>Standing</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>441</td>\n",
       "      <td>27</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.89</td>\n",
       "      <td>10.04</td>\n",
       "      <td>Standing</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>442</td>\n",
       "      <td>27</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.85</td>\n",
       "      <td>10.03</td>\n",
       "      <td>Standing</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>443</td>\n",
       "      <td>27</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.50</td>\n",
       "      <td>10.01</td>\n",
       "      <td>Standing</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>444</td>\n",
       "      <td>27</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.62</td>\n",
       "      <td>10.03</td>\n",
       "      <td>Standing</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>445</td>\n",
       "      <td>27</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.63</td>\n",
       "      <td>10.02</td>\n",
       "      <td>Standing</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>446</td>\n",
       "      <td>27</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>2.39</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.74</td>\n",
       "      <td>5.90</td>\n",
       "      <td>0.74</td>\n",
       "      <td>4.06</td>\n",
       "      <td>Standing</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>600</td>\n",
       "      <td>36</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>421.88</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.83</td>\n",
       "      <td>Standing</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>601</td>\n",
       "      <td>36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>362.96</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.94</td>\n",
       "      <td>9.83</td>\n",
       "      <td>Standing</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows Ã— 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      unique_id  user_id    x0    x1    x2    x3    x4    x5    x6    x7  ...  \\\n",
       "0             1       33  0.04  0.09  0.14  0.12  0.11  0.10  0.08  0.13  ...   \n",
       "1             2       33  0.12  0.12  0.06  0.07  0.11  0.10  0.11  0.09  ...   \n",
       "2             3       33  0.14  0.09  0.11  0.09  0.09  0.11  0.12  0.08  ...   \n",
       "3             4       33  0.06  0.10  0.09  0.09  0.11  0.07  0.12  0.10  ...   \n",
       "4             5       33  0.12  0.11  0.10  0.08  0.10  0.14  0.10  0.11  ...   \n",
       "5             6       33  0.09  0.09  0.10  0.12  0.08  0.06  0.09  0.08  ...   \n",
       "6             7       33  0.12  0.12  0.12  0.13  0.15  0.11  0.08  0.09  ...   \n",
       "7             8       33  0.10  0.10  0.10  0.10  0.11  0.11  0.09  0.09  ...   \n",
       "8             9       33  0.08  0.07  0.08  0.08  0.05  0.08  0.14  0.10  ...   \n",
       "9            10       33  0.01  0.00  0.00  0.01  0.01  0.01  0.01  0.01  ...   \n",
       "10           11       33  0.03  0.08  0.07  0.07  0.09  0.13  0.14  0.16  ...   \n",
       "11           12       33  0.10  0.10  0.14  0.11  0.10  0.09  0.09  0.09  ...   \n",
       "12           13       33  0.17  0.11  0.08  0.12  0.11  0.11  0.06  0.07  ...   \n",
       "13           14       33  0.00  0.01  0.01  0.01  0.01  0.00  0.00  0.00  ...   \n",
       "14           15       33  0.03  0.11  0.07  0.12  0.08  0.09  0.11  0.12  ...   \n",
       "15           16       33  0.15  0.11  0.08  0.10  0.12  0.13  0.08  0.10  ...   \n",
       "16           17       33  0.09  0.06  0.11  0.06  0.07  0.06  0.06  0.06  ...   \n",
       "17           18       33  0.05  0.08  0.08  0.09  0.09  0.10  0.09  0.14  ...   \n",
       "18           19       33  0.12  0.11  0.09  0.11  0.12  0.11  0.10  0.10  ...   \n",
       "19           20       33  0.10  0.07  0.11  0.06  0.04  0.08  0.05  0.05  ...   \n",
       "20           21       33  0.01  0.06  0.09  0.08  0.07  0.10  0.11  0.18  ...   \n",
       "21           22       33  0.09  0.08  0.06  0.10  0.13  0.11  0.13  0.10  ...   \n",
       "22           23       33  0.18  0.14  0.12  0.10  0.07  0.07  0.03  0.01  ...   \n",
       "23           24       33  0.02  0.07  0.08  0.09  0.09  0.11  0.10  0.14  ...   \n",
       "24           25       33  0.15  0.12  0.10  0.07  0.13  0.09  0.10  0.10  ...   \n",
       "25           26       33  0.10  0.10  0.09  0.10  0.06  0.09  0.07  0.04  ...   \n",
       "32           33       33  0.08  0.10  0.11  0.10  0.16  0.10  0.10  0.12  ...   \n",
       "33           34       33  0.12  0.09  0.11  0.08  0.08  0.09  0.11  0.10  ...   \n",
       "34           35       33  0.07  0.10  0.06  0.10  0.05  0.09  0.07  0.06  ...   \n",
       "35           36       33  0.08  0.08  0.09  0.09  0.10  0.13  0.08  0.15  ...   \n",
       "36           37       33  0.11  0.09  0.13  0.10  0.10  0.07  0.09  0.10  ...   \n",
       "37           38       33  0.10  0.11  0.08  0.10  0.10  0.10  0.10  0.05  ...   \n",
       "38           39       33  0.06  0.08  0.12  0.15  0.20  0.10  0.08  0.11  ...   \n",
       "39           40       33  0.12  0.06  0.12  0.11  0.06  0.11  0.10  0.16  ...   \n",
       "40           41       33  0.13  0.10  0.08  0.08  0.09  0.11  0.07  0.12  ...   \n",
       "41           42       33  0.16  0.12  0.06  0.05  0.07  0.13  0.11  0.10  ...   \n",
       "47           48       17  0.15  0.24  0.10  0.08  0.11  0.09  0.08  0.03  ...   \n",
       "48           49       17  0.08  0.05  0.10  0.10  0.13  0.14  0.10  0.11  ...   \n",
       "49           50       17  0.10  0.09  0.11  0.08  0.10  0.14  0.08  0.11  ...   \n",
       "50           51       17  0.12  0.11  0.08  0.08  0.11  0.10  0.15  0.09  ...   \n",
       "1090        428       27  0.21  0.23  0.20  0.00  0.12  0.07  0.00  0.06  ...   \n",
       "1091        429       27  0.07  0.22  0.28  0.00  0.22  0.12  0.00  0.04  ...   \n",
       "1092        430       27  0.18  0.23  0.15  0.00  0.15  0.09  0.00  0.09  ...   \n",
       "1093        431       27  0.07  0.09  0.14  0.00  0.18  0.21  0.00  0.17  ...   \n",
       "1094        432       27  0.08  0.15  0.10  0.00  0.15  0.09  0.00  0.16  ...   \n",
       "1095        433       27  0.06  0.05  0.06  0.00  0.10  0.14  0.00  0.13  ...   \n",
       "1096        434       27  0.02  0.01  0.01  0.00  0.04  0.15  0.00  0.30  ...   \n",
       "1097        435       27  0.02  0.02  0.02  0.00  0.07  0.09  0.00  0.23  ...   \n",
       "1098        436       27  0.08  0.07  0.14  0.00  0.19  0.20  0.00  0.17  ...   \n",
       "1099        437       27  0.02  0.07  0.10  0.00  0.22  0.29  0.00  0.19  ...   \n",
       "1101        439       27  0.11  0.04  0.04  0.03  0.10  0.09  0.12  0.09  ...   \n",
       "1102        440       27  0.10  0.06  0.07  0.06  0.10  0.05  0.12  0.09  ...   \n",
       "1103        441       27  0.06  0.06  0.05  0.06  0.07  0.10  0.11  0.18  ...   \n",
       "1104        442       27  0.04  0.06  0.05  0.06  0.13  0.09  0.18  0.16  ...   \n",
       "1105        443       27  0.14  0.21  0.17  0.10  0.16  0.07  0.10  0.06  ...   \n",
       "1106        444       27  0.13  0.16  0.17  0.11  0.14  0.09  0.05  0.05  ...   \n",
       "1107        445       27  0.11  0.13  0.18  0.09  0.15  0.10  0.06  0.07  ...   \n",
       "1108        446       27  0.03  0.05  0.03  0.03  0.10  0.01  0.05  0.03  ...   \n",
       "1262        600       36  0.01  0.00  0.01  0.00  0.01  0.00  0.02  0.06  ...   \n",
       "1263        601       36  0.00  0.01  0.02  0.06  0.13  0.40  0.24  0.09  ...   \n",
       "\n",
       "        zpeak  xabsoldev  yabsoldev  zabsoldev  xstandardev  ystandarddev  \\\n",
       "0     1550.00       3.29       7.21       4.00         4.05          8.17   \n",
       "1     1233.33       4.23       6.88       4.05         5.43          8.19   \n",
       "2     1780.00       4.18       6.89       4.07         5.55          8.19   \n",
       "3     1380.00       2.26       4.13       2.49         2.87          4.95   \n",
       "4     1775.00       2.29       3.94       2.41         3.08          4.64   \n",
       "5     2675.00       2.00       3.39       1.94         3.13          4.95   \n",
       "6     1825.00       1.56       2.55       1.72         2.09          3.20   \n",
       "7     2983.33       1.84       2.69       1.41         2.22          3.35   \n",
       "8     1533.33       2.26       2.70       1.30         2.68          3.40   \n",
       "9        0.00       0.07       0.37       0.03         1.85          9.34   \n",
       "10     175.00       1.64       2.77       2.39         2.60          3.60   \n",
       "11    1460.00       1.62       3.03       1.49         1.98          3.74   \n",
       "12    2700.00       1.79       2.98       1.58         2.24          3.70   \n",
       "13       0.00       0.07       0.36       0.07         1.97          9.96   \n",
       "14    3900.00       1.59       2.71       1.50         2.05          3.40   \n",
       "15    3525.00       2.14       2.93       1.71         2.78          3.62   \n",
       "16    1833.33       1.44       2.63       0.93         2.75          4.86   \n",
       "17    1562.50       1.45       3.03       1.61         1.82          3.82   \n",
       "18    4075.00       1.69       3.14       1.81         2.11          3.84   \n",
       "19    2175.00       1.14       2.39       1.05         2.33          4.96   \n",
       "20    1640.00       1.21       2.77       1.39         1.55          3.36   \n",
       "21    3450.00       1.50       2.96       1.68         2.03          3.73   \n",
       "22    1916.67       2.00       2.51       1.25         3.42          4.28   \n",
       "23    2075.00       1.66       3.27       1.76         2.13          3.95   \n",
       "24    1660.00       1.86       3.28       2.03         2.41          4.09   \n",
       "25    1500.00       1.30       2.58       1.51         2.21          4.59   \n",
       "32    3075.00       4.57       6.50       3.94         5.65          7.69   \n",
       "33    2700.00       5.70       6.37       3.73         6.82          7.58   \n",
       "34    1875.00       4.23       5.49       3.18         6.55          8.20   \n",
       "35    1600.00       2.55       4.17       2.27         3.04          5.02   \n",
       "36    1650.00       2.64       3.86       2.41         3.12          4.69   \n",
       "37    1320.00       2.29       3.78       2.28         3.09          4.96   \n",
       "38    1600.00       4.60       6.35       3.72         5.88          7.73   \n",
       "39    1700.00       6.06       6.60       3.33         7.15          7.80   \n",
       "40    1683.33       6.24       6.73       3.36         7.32          7.90   \n",
       "41    4000.00       6.65       7.05       4.12         7.66          8.49   \n",
       "47    1500.00       2.04       2.40       1.60         2.80          3.20   \n",
       "48    2516.67       2.98       2.82       1.74         3.51          3.35   \n",
       "49     725.00       2.96       2.94       1.82         3.58          3.52   \n",
       "50     857.14       2.84       2.83       1.92         3.53          3.44   \n",
       "1090   275.00       3.12       0.09       0.10         3.13          0.21   \n",
       "1091  1037.50       3.15       0.04       0.05         3.16          0.07   \n",
       "1092   491.67       3.15       0.06       0.06         3.16          0.09   \n",
       "1093   351.79       3.20       0.05       0.04         3.21          0.08   \n",
       "1094  1900.00       3.22       0.06       0.10         3.23          0.09   \n",
       "1095  1770.00       3.27       0.06       0.07         3.28          0.12   \n",
       "1096   925.00       3.27       0.03       0.04         3.28          0.07   \n",
       "1097  1162.50       3.29       0.04       0.04         3.29          0.06   \n",
       "1098   825.00       3.20       0.04       0.05         3.21          0.07   \n",
       "1099   543.75       3.21       0.03       0.04         3.21          0.06   \n",
       "1101   525.00       0.94       0.21       0.36         1.18          0.33   \n",
       "1102  1750.00       0.88       0.20       0.31         1.05          0.27   \n",
       "1103     0.00       0.80       0.17       0.28         0.89          0.21   \n",
       "1104     0.00       0.76       0.15       0.22         0.85          0.19   \n",
       "1105     0.00       0.42       0.09       0.20         0.50          0.13   \n",
       "1106     0.00       0.51       0.10       0.22         0.62          0.14   \n",
       "1107     0.00       0.53       0.15       0.24         0.63          0.19   \n",
       "1108     0.00       0.26       2.39       0.30         0.74          5.90   \n",
       "1262   421.88       0.47       0.09       0.13         0.54          0.15   \n",
       "1263   362.96       0.92       0.06       0.08         0.94          0.09   \n",
       "\n",
       "      zstandarddev  resultant       class  ActivityEncoded  \n",
       "0             4.05      11.96     Jogging                1  \n",
       "1             5.43      12.05     Jogging                1  \n",
       "2             5.55      11.99     Jogging                1  \n",
       "3             2.87      10.69     Walking                5  \n",
       "4             3.08      10.80     Walking                5  \n",
       "5             3.13       8.63     Walking                5  \n",
       "6             2.09       9.87    Upstairs                4  \n",
       "7             2.22       9.91    Upstairs                4  \n",
       "8             2.68       9.78    Upstairs                4  \n",
       "9             1.85       0.39    Upstairs                4  \n",
       "10            2.60       9.52  Downstairs                0  \n",
       "11            1.98       9.62  Downstairs                0  \n",
       "12            2.24       9.89  Downstairs                0  \n",
       "13            1.97       0.40  Downstairs                0  \n",
       "14            2.05       9.69    Upstairs                4  \n",
       "15            2.78       9.75    Upstairs                4  \n",
       "16            2.75       6.42    Upstairs                4  \n",
       "17            1.82       9.84  Downstairs                0  \n",
       "18            2.11       9.90  Downstairs                0  \n",
       "19            2.33       5.78  Downstairs                0  \n",
       "20            1.55       9.67    Upstairs                4  \n",
       "21            2.03       9.84    Upstairs                4  \n",
       "22            3.42       7.23    Upstairs                4  \n",
       "23            2.13       9.79  Downstairs                0  \n",
       "24            2.41      10.05  Downstairs                0  \n",
       "25            2.21       7.37  Downstairs                0  \n",
       "32            5.65      12.85     Jogging                1  \n",
       "33            6.82      13.93     Jogging                1  \n",
       "34            6.55      10.61     Jogging                1  \n",
       "35            3.04      10.68     Walking                5  \n",
       "36            3.12      10.86     Walking                5  \n",
       "37            3.09       9.67     Walking                5  \n",
       "38            5.88      13.26     Jogging                1  \n",
       "39            7.15      14.48     Jogging                1  \n",
       "40            7.32      14.28     Jogging                1  \n",
       "41            7.66      14.63     Jogging                1  \n",
       "47            2.80      10.53     Walking                5  \n",
       "48            3.51      10.55     Walking                5  \n",
       "49            3.58      10.61     Walking                5  \n",
       "50            3.53      10.56     Walking                5  \n",
       "1090          3.13       9.91     Sitting                2  \n",
       "1091          3.16       9.90     Sitting                2  \n",
       "1092          3.16       9.90     Sitting                2  \n",
       "1093          3.21       9.90     Sitting                2  \n",
       "1094          3.23       9.91     Sitting                2  \n",
       "1095          3.28       9.92     Sitting                2  \n",
       "1096          3.28       9.92     Sitting                2  \n",
       "1097          3.29       9.92     Sitting                2  \n",
       "1098          3.21       9.89     Sitting                2  \n",
       "1099          3.21       9.89     Sitting                2  \n",
       "1101          1.18      10.09    Standing                3  \n",
       "1102          1.05      10.06    Standing                3  \n",
       "1103          0.89      10.04    Standing                3  \n",
       "1104          0.85      10.03    Standing                3  \n",
       "1105          0.50      10.01    Standing                3  \n",
       "1106          0.62      10.03    Standing                3  \n",
       "1107          0.63      10.02    Standing                3  \n",
       "1108          0.74       4.06    Standing                3  \n",
       "1262          0.54       9.83    Standing                3  \n",
       "1263          0.94       9.83    Standing                3  \n",
       "\n",
       "[60 rows x 47 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "# Define column name of the label vector\n",
    "LABEL = 'ActivityEncoded'\n",
    "# Transform the labels from String to Integer via LabelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Add a new column to the existing DataFrame with the encoded values\n",
    "data_reformat[LABEL] = le.fit_transform(data_reformat['class'].values.ravel())\n",
    "dummy=data_reformat.groupby(['class'])\n",
    "dummy.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unique_id            int64\n",
       "user_id              int64\n",
       "x0                 float64\n",
       "x1                 float64\n",
       "x2                 float64\n",
       "x3                 float64\n",
       "x4                 float64\n",
       "x5                 float64\n",
       "x6                 float64\n",
       "x7                 float64\n",
       "x8                 float64\n",
       "x9                 float64\n",
       "y0                 float64\n",
       "y1                 float64\n",
       "y2                 float64\n",
       "y3                 float64\n",
       "y4                 float64\n",
       "y5                 float64\n",
       "y6                 float64\n",
       "y7                 float64\n",
       "y8                 float64\n",
       "y9                 float64\n",
       "z0                 float64\n",
       "z1                 float64\n",
       "z2                 float64\n",
       "z3                 float64\n",
       "z4                 float64\n",
       "z5                 float64\n",
       "z6                 float64\n",
       "z7                 float64\n",
       "z8                 float64\n",
       "z9                 float64\n",
       "xavg                 int64\n",
       "yavg               float64\n",
       "zavg               float64\n",
       "xpeak              float64\n",
       "ypeak              float64\n",
       "zpeak              float64\n",
       "xabsoldev          float64\n",
       "yabsoldev          float64\n",
       "zabsoldev          float64\n",
       "xstandardev        float64\n",
       "ystandarddev       float64\n",
       "zstandarddev       float64\n",
       "resultant          float64\n",
       "class               object\n",
       "ActivityEncoded      int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reformat.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    5\n",
       "4    5\n",
       "5    5\n",
       "6    4\n",
       "7    4\n",
       "8    4\n",
       "9    4\n",
       "Name: ActivityEncoded, dtype: int32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y =data_reformat[\"ActivityEncoded\"]\n",
    "y.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x0              float64\n",
       "x1              float64\n",
       "x2              float64\n",
       "x3              float64\n",
       "x4              float64\n",
       "x5              float64\n",
       "x6              float64\n",
       "x7              float64\n",
       "x8              float64\n",
       "x9              float64\n",
       "y0              float64\n",
       "y1              float64\n",
       "y2              float64\n",
       "y3              float64\n",
       "y4              float64\n",
       "y5              float64\n",
       "y6              float64\n",
       "y7              float64\n",
       "y8              float64\n",
       "y9              float64\n",
       "z0              float64\n",
       "z1              float64\n",
       "z2              float64\n",
       "z3              float64\n",
       "z4              float64\n",
       "z5              float64\n",
       "z6              float64\n",
       "z7              float64\n",
       "z8              float64\n",
       "z9              float64\n",
       "xavg              int64\n",
       "yavg            float64\n",
       "zavg            float64\n",
       "xpeak           float64\n",
       "ypeak           float64\n",
       "zpeak           float64\n",
       "xabsoldev       float64\n",
       "yabsoldev       float64\n",
       "zabsoldev       float64\n",
       "xstandardev     float64\n",
       "ystandarddev    float64\n",
       "zstandarddev    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data_reformat[['x0','x1','x2','x3','x4','x5','x6','x7','x8','x9','y0','y1','y2','y3','y4','y5','y6','y7','y8','y9','z0','z1','z2','z3','z4','z5','z6','z7','z8','z9','xavg','yavg','zavg','xpeak','ypeak','zpeak','xabsoldev','yabsoldev','zabsoldev','xstandardev','ystandarddev','zstandarddev']]\n",
    "#feature_names = x.columns\n",
    "x.head()\n",
    "x.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to create training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5034    1\n",
       "154     5\n",
       "58      5\n",
       "540     1\n",
       "3536    0\n",
       "       ..\n",
       "3603    1\n",
       "4722    1\n",
       "3340    4\n",
       "3064    4\n",
       "3398    5\n",
       "Name: ActivityEncoded, Length: 4063, dtype: int32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScater model and fit it to the training data\n",
    "X_scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training and testing data using the X_scaler\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encoding\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)\n",
    "y_test_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create a normal neural network with 2 inputs, 6 hidden nodes, and 2 outputs\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "#59.78%\n",
    "#model.add(Dense(units=6, activation='relu', input_dim=30))\n",
    "#model.add(Dense(units=6, activation='softmax'))\n",
    "#82.66%, with noise - (0.05- 83.76%, 83.1%, 0.2 - 84.2%, 81.9%),with dropout (0.5)-85 to 87%, with dropout (0.1,0.1) - 85.2%, (0.2,0.1) - 86-87%\n",
    "model.add(Dense(units=60, activation='relu', input_dim=42))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(units=6, activation='softmax'))\n",
    "#keras.layers.Dropout(0.05, noise_shape=None, seed=5)\n",
    "\n",
    "#82.7%\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.layers.normalization import BatchNormalization\n",
    "#model.add(Dense(units=60, use_bias=False, activation='relu', input_dim=42))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dense(units=100, activation='relu'))\n",
    "##model.add(BatchNormalization())\n",
    "#model.add(Dense(units=6, activation='softmax'))\n",
    "##model.add(BatchNormalization())\n",
    "#keras.layers.Dropout(0.05, noise_shape=None, seed=5)\n",
    "\n",
    "#63.83 -- old experiment; tried again, not worth the effort\n",
    "#model.add(Dense(units=120, activation='relu', input_dim=30))\n",
    "#model.add(Dense(units=120, activation='relu'))\n",
    "#model.add(Dense(units=120, activation='relu'))\n",
    "#model.add(Dense(units=120, activation='relu'))\n",
    "#model.add(Dense(units=120, activation='relu'))\n",
    "#model.add(Dense(units=120, activation='relu'))\n",
    "#model.add(Dense(units=120, activation='relu'))\n",
    "#model.add(Dense(units=6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 60)                2580      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               6100      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 9,286\n",
      "Trainable params: 9,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#from keras.optimizers import SGD\n",
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#model.compile(loss='categorical_crossentropy',\n",
    "#              optimizer=sgd,\n",
    "#              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      " - 3s - loss: 1.1713 - accuracy: 0.6188\n",
      "Epoch 2/300\n",
      " - 0s - loss: 0.7874 - accuracy: 0.7194\n",
      "Epoch 3/300\n",
      " - 0s - loss: 0.6936 - accuracy: 0.7421\n",
      "Epoch 4/300\n",
      " - 0s - loss: 0.6284 - accuracy: 0.7637\n",
      "Epoch 5/300\n",
      " - 0s - loss: 0.5924 - accuracy: 0.7768\n",
      "Epoch 6/300\n",
      " - 0s - loss: 0.5647 - accuracy: 0.7854\n",
      "Epoch 7/300\n",
      " - 0s - loss: 0.5413 - accuracy: 0.7923\n",
      "Epoch 8/300\n",
      " - 0s - loss: 0.5210 - accuracy: 0.8009\n",
      "Epoch 9/300\n",
      " - 0s - loss: 0.5115 - accuracy: 0.8024\n",
      "Epoch 10/300\n",
      " - 0s - loss: 0.4921 - accuracy: 0.8080\n",
      "Epoch 11/300\n",
      " - 0s - loss: 0.4765 - accuracy: 0.8189\n",
      "Epoch 12/300\n",
      " - 0s - loss: 0.4724 - accuracy: 0.8184\n",
      "Epoch 13/300\n",
      " - 0s - loss: 0.4519 - accuracy: 0.8235\n",
      "Epoch 14/300\n",
      " - 0s - loss: 0.4534 - accuracy: 0.8221\n",
      "Epoch 15/300\n",
      " - 0s - loss: 0.4463 - accuracy: 0.8272\n",
      "Epoch 16/300\n",
      " - 0s - loss: 0.4310 - accuracy: 0.8312\n",
      "Epoch 17/300\n",
      " - 0s - loss: 0.4224 - accuracy: 0.8334\n",
      "Epoch 18/300\n",
      " - 0s - loss: 0.4172 - accuracy: 0.8346\n",
      "Epoch 19/300\n",
      " - 0s - loss: 0.4118 - accuracy: 0.8405\n",
      "Epoch 20/300\n",
      " - 0s - loss: 0.3996 - accuracy: 0.8435\n",
      "Epoch 21/300\n",
      " - 0s - loss: 0.3936 - accuracy: 0.8489\n",
      "Epoch 22/300\n",
      " - 0s - loss: 0.3902 - accuracy: 0.8484\n",
      "Epoch 23/300\n",
      " - 0s - loss: 0.3860 - accuracy: 0.8504\n",
      "Epoch 24/300\n",
      " - 0s - loss: 0.3742 - accuracy: 0.8516\n",
      "Epoch 25/300\n",
      " - 0s - loss: 0.3688 - accuracy: 0.8553\n",
      "Epoch 26/300\n",
      " - 0s - loss: 0.3552 - accuracy: 0.8570\n",
      "Epoch 27/300\n",
      " - 0s - loss: 0.3521 - accuracy: 0.8649\n",
      "Epoch 28/300\n",
      " - 0s - loss: 0.3477 - accuracy: 0.8656\n",
      "Epoch 29/300\n",
      " - 0s - loss: 0.3421 - accuracy: 0.8686\n",
      "Epoch 30/300\n",
      " - 0s - loss: 0.3444 - accuracy: 0.8651\n",
      "Epoch 31/300\n",
      " - 0s - loss: 0.3383 - accuracy: 0.8644\n",
      "Epoch 32/300\n",
      " - 0s - loss: 0.3349 - accuracy: 0.8681\n",
      "Epoch 33/300\n",
      " - 0s - loss: 0.3172 - accuracy: 0.8737\n",
      "Epoch 34/300\n",
      " - 0s - loss: 0.3334 - accuracy: 0.8747\n",
      "Epoch 35/300\n",
      " - 0s - loss: 0.3232 - accuracy: 0.8718\n",
      "Epoch 36/300\n",
      " - 0s - loss: 0.3152 - accuracy: 0.8760\n",
      "Epoch 37/300\n",
      " - 0s - loss: 0.3100 - accuracy: 0.8769\n",
      "Epoch 38/300\n",
      " - 0s - loss: 0.3044 - accuracy: 0.8824\n",
      "Epoch 39/300\n",
      " - 0s - loss: 0.3051 - accuracy: 0.8772\n",
      "Epoch 40/300\n",
      " - 0s - loss: 0.2941 - accuracy: 0.8851\n",
      "Epoch 41/300\n",
      " - 0s - loss: 0.2988 - accuracy: 0.8875\n",
      "Epoch 42/300\n",
      " - 0s - loss: 0.2907 - accuracy: 0.8890\n",
      "Epoch 43/300\n",
      " - 0s - loss: 0.2882 - accuracy: 0.8851\n",
      "Epoch 44/300\n",
      " - 0s - loss: 0.2880 - accuracy: 0.8905\n",
      "Epoch 45/300\n",
      " - 0s - loss: 0.2826 - accuracy: 0.8944\n",
      "Epoch 46/300\n",
      " - 0s - loss: 0.2770 - accuracy: 0.8954\n",
      "Epoch 47/300\n",
      " - 0s - loss: 0.2790 - accuracy: 0.8883\n",
      "Epoch 48/300\n",
      " - 0s - loss: 0.2763 - accuracy: 0.8993\n",
      "Epoch 49/300\n",
      " - 0s - loss: 0.2735 - accuracy: 0.9038\n",
      "Epoch 50/300\n",
      " - 0s - loss: 0.2706 - accuracy: 0.8907\n",
      "Epoch 51/300\n",
      " - 0s - loss: 0.2643 - accuracy: 0.9013\n",
      "Epoch 52/300\n",
      " - 0s - loss: 0.2622 - accuracy: 0.9020\n",
      "Epoch 53/300\n",
      " - 0s - loss: 0.2580 - accuracy: 0.9008\n",
      "Epoch 54/300\n",
      " - 0s - loss: 0.2628 - accuracy: 0.8964\n",
      "Epoch 55/300\n",
      " - 0s - loss: 0.2467 - accuracy: 0.9048\n",
      "Epoch 56/300\n",
      " - 0s - loss: 0.2538 - accuracy: 0.9016\n",
      "Epoch 57/300\n",
      " - 0s - loss: 0.2570 - accuracy: 0.8991\n",
      "Epoch 58/300\n",
      " - 0s - loss: 0.2617 - accuracy: 0.8932\n",
      "Epoch 59/300\n",
      " - 0s - loss: 0.2481 - accuracy: 0.9052\n",
      "Epoch 60/300\n",
      " - 0s - loss: 0.2391 - accuracy: 0.9119\n",
      "Epoch 61/300\n",
      " - 0s - loss: 0.2440 - accuracy: 0.9077\n",
      "Epoch 62/300\n",
      " - 0s - loss: 0.2401 - accuracy: 0.9070\n",
      "Epoch 63/300\n",
      " - 0s - loss: 0.2359 - accuracy: 0.9077\n",
      "Epoch 64/300\n",
      " - 0s - loss: 0.2390 - accuracy: 0.9102\n",
      "Epoch 65/300\n",
      " - 0s - loss: 0.2290 - accuracy: 0.9146\n",
      "Epoch 66/300\n",
      " - 0s - loss: 0.2339 - accuracy: 0.9084\n",
      "Epoch 67/300\n",
      " - 0s - loss: 0.2341 - accuracy: 0.9121\n",
      "Epoch 68/300\n",
      " - 0s - loss: 0.2201 - accuracy: 0.9148\n",
      "Epoch 69/300\n",
      " - 0s - loss: 0.2354 - accuracy: 0.9131\n",
      "Epoch 70/300\n",
      " - 0s - loss: 0.2204 - accuracy: 0.9175\n",
      "Epoch 71/300\n",
      " - 0s - loss: 0.2182 - accuracy: 0.9156\n",
      "Epoch 72/300\n",
      " - 0s - loss: 0.2145 - accuracy: 0.9183\n",
      "Epoch 73/300\n",
      " - 0s - loss: 0.2277 - accuracy: 0.9163\n",
      "Epoch 74/300\n",
      " - 0s - loss: 0.2217 - accuracy: 0.9183\n",
      "Epoch 75/300\n",
      " - 0s - loss: 0.2174 - accuracy: 0.9183\n",
      "Epoch 76/300\n",
      " - 0s - loss: 0.2215 - accuracy: 0.9173\n",
      "Epoch 77/300\n",
      " - 0s - loss: 0.2123 - accuracy: 0.9190\n",
      "Epoch 78/300\n",
      " - 0s - loss: 0.2225 - accuracy: 0.9153\n",
      "Epoch 79/300\n",
      " - 0s - loss: 0.2165 - accuracy: 0.9158\n",
      "Epoch 80/300\n",
      " - 0s - loss: 0.1993 - accuracy: 0.9230\n",
      "Epoch 81/300\n",
      " - 0s - loss: 0.2052 - accuracy: 0.9198\n",
      "Epoch 82/300\n",
      " - 0s - loss: 0.2062 - accuracy: 0.9235\n",
      "Epoch 83/300\n",
      " - 0s - loss: 0.2150 - accuracy: 0.9161\n",
      "Epoch 84/300\n",
      " - 0s - loss: 0.1937 - accuracy: 0.9284\n",
      "Epoch 85/300\n",
      " - 0s - loss: 0.1955 - accuracy: 0.9252\n",
      "Epoch 86/300\n",
      " - 0s - loss: 0.2067 - accuracy: 0.9207\n",
      "Epoch 87/300\n",
      " - 0s - loss: 0.1868 - accuracy: 0.9254\n",
      "Epoch 88/300\n",
      " - 0s - loss: 0.1943 - accuracy: 0.9237\n",
      "Epoch 89/300\n",
      " - 0s - loss: 0.1961 - accuracy: 0.9286\n",
      "Epoch 90/300\n",
      " - 0s - loss: 0.1961 - accuracy: 0.9237\n",
      "Epoch 91/300\n",
      " - 0s - loss: 0.1925 - accuracy: 0.9244\n",
      "Epoch 92/300\n",
      " - 0s - loss: 0.1825 - accuracy: 0.9303\n",
      "Epoch 93/300\n",
      " - 0s - loss: 0.1948 - accuracy: 0.9207\n",
      "Epoch 94/300\n",
      " - 0s - loss: 0.1930 - accuracy: 0.9276\n",
      "Epoch 95/300\n",
      " - 0s - loss: 0.1890 - accuracy: 0.9299\n",
      "Epoch 96/300\n",
      " - 0s - loss: 0.1804 - accuracy: 0.9323\n",
      "Epoch 97/300\n",
      " - 0s - loss: 0.1781 - accuracy: 0.9345\n",
      "Epoch 98/300\n",
      " - 0s - loss: 0.1741 - accuracy: 0.9338\n",
      "Epoch 99/300\n",
      " - 0s - loss: 0.1779 - accuracy: 0.9291\n",
      "Epoch 100/300\n",
      " - 0s - loss: 0.1724 - accuracy: 0.9328\n",
      "Epoch 101/300\n",
      " - 0s - loss: 0.1814 - accuracy: 0.9323\n",
      "Epoch 102/300\n",
      " - 0s - loss: 0.1833 - accuracy: 0.9313\n",
      "Epoch 103/300\n",
      " - 0s - loss: 0.1821 - accuracy: 0.9306\n",
      "Epoch 104/300\n",
      " - 0s - loss: 0.1753 - accuracy: 0.9331\n",
      "Epoch 105/300\n",
      " - 0s - loss: 0.1780 - accuracy: 0.9281\n",
      "Epoch 106/300\n",
      " - 0s - loss: 0.1669 - accuracy: 0.9380\n",
      "Epoch 107/300\n",
      " - 0s - loss: 0.1756 - accuracy: 0.9360\n",
      "Epoch 108/300\n",
      " - 0s - loss: 0.1762 - accuracy: 0.9338\n",
      "Epoch 109/300\n",
      " - 0s - loss: 0.1777 - accuracy: 0.9308\n",
      "Epoch 110/300\n",
      " - 0s - loss: 0.1708 - accuracy: 0.9353\n",
      "Epoch 111/300\n",
      " - 0s - loss: 0.1811 - accuracy: 0.9313\n",
      "Epoch 112/300\n",
      " - 0s - loss: 0.1719 - accuracy: 0.9348\n",
      "Epoch 113/300\n",
      " - 0s - loss: 0.1708 - accuracy: 0.9355\n",
      "Epoch 114/300\n",
      " - 0s - loss: 0.1665 - accuracy: 0.9397\n",
      "Epoch 115/300\n",
      " - 0s - loss: 0.1787 - accuracy: 0.9365\n",
      "Epoch 116/300\n",
      " - 0s - loss: 0.1658 - accuracy: 0.9382\n",
      "Epoch 117/300\n",
      " - 0s - loss: 0.1679 - accuracy: 0.9436\n",
      "Epoch 118/300\n",
      " - 0s - loss: 0.1617 - accuracy: 0.9375\n",
      "Epoch 119/300\n",
      " - 0s - loss: 0.1638 - accuracy: 0.9380\n",
      "Epoch 120/300\n",
      " - 0s - loss: 0.1594 - accuracy: 0.9380\n",
      "Epoch 121/300\n",
      " - 0s - loss: 0.1680 - accuracy: 0.9367\n",
      "Epoch 122/300\n",
      " - 0s - loss: 0.1575 - accuracy: 0.9407\n",
      "Epoch 123/300\n",
      " - 0s - loss: 0.1641 - accuracy: 0.9372\n",
      "Epoch 124/300\n",
      " - 0s - loss: 0.1622 - accuracy: 0.9370\n",
      "Epoch 125/300\n",
      " - 0s - loss: 0.1635 - accuracy: 0.9414\n",
      "Epoch 126/300\n",
      " - 0s - loss: 0.1457 - accuracy: 0.9429\n",
      "Epoch 127/300\n",
      " - 0s - loss: 0.1602 - accuracy: 0.9382\n",
      "Epoch 128/300\n",
      " - 0s - loss: 0.1544 - accuracy: 0.9427\n",
      "Epoch 129/300\n",
      " - 0s - loss: 0.1588 - accuracy: 0.9417\n",
      "Epoch 130/300\n",
      " - 0s - loss: 0.1597 - accuracy: 0.9424\n",
      "Epoch 131/300\n",
      " - 0s - loss: 0.1587 - accuracy: 0.9439\n",
      "Epoch 132/300\n",
      " - 0s - loss: 0.1554 - accuracy: 0.9404\n",
      "Epoch 133/300\n",
      " - 0s - loss: 0.1467 - accuracy: 0.9473\n",
      "Epoch 134/300\n",
      " - 0s - loss: 0.1520 - accuracy: 0.9409\n",
      "Epoch 135/300\n",
      " - 0s - loss: 0.1571 - accuracy: 0.9387\n",
      "Epoch 136/300\n",
      " - 0s - loss: 0.1433 - accuracy: 0.9488\n",
      "Epoch 137/300\n",
      " - 0s - loss: 0.1569 - accuracy: 0.9454\n",
      "Epoch 138/300\n",
      " - 0s - loss: 0.1400 - accuracy: 0.9510\n",
      "Epoch 139/300\n",
      " - 0s - loss: 0.1448 - accuracy: 0.9473\n",
      "Epoch 140/300\n",
      " - 0s - loss: 0.1522 - accuracy: 0.9436\n",
      "Epoch 141/300\n",
      " - 0s - loss: 0.1422 - accuracy: 0.9478\n",
      "Epoch 142/300\n",
      " - 0s - loss: 0.1461 - accuracy: 0.9441\n",
      "Epoch 143/300\n",
      " - 0s - loss: 0.1466 - accuracy: 0.9436\n",
      "Epoch 144/300\n",
      " - 0s - loss: 0.1522 - accuracy: 0.9429\n",
      "Epoch 145/300\n",
      " - 0s - loss: 0.1434 - accuracy: 0.9463\n",
      "Epoch 146/300\n",
      " - 0s - loss: 0.1509 - accuracy: 0.9434\n",
      "Epoch 147/300\n",
      " - 0s - loss: 0.1418 - accuracy: 0.9466\n",
      "Epoch 148/300\n",
      " - 0s - loss: 0.1374 - accuracy: 0.9488\n",
      "Epoch 149/300\n",
      " - 0s - loss: 0.1481 - accuracy: 0.9471\n",
      "Epoch 150/300\n",
      " - 0s - loss: 0.1436 - accuracy: 0.9461\n",
      "Epoch 151/300\n",
      " - 0s - loss: 0.1480 - accuracy: 0.9414\n",
      "Epoch 152/300\n",
      " - 0s - loss: 0.1403 - accuracy: 0.9486\n",
      "Epoch 153/300\n",
      " - 0s - loss: 0.1379 - accuracy: 0.9495\n",
      "Epoch 154/300\n",
      " - 0s - loss: 0.1440 - accuracy: 0.9441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/300\n",
      " - 0s - loss: 0.1386 - accuracy: 0.9503\n",
      "Epoch 156/300\n",
      " - 0s - loss: 0.1315 - accuracy: 0.9525\n",
      "Epoch 157/300\n",
      " - 0s - loss: 0.1425 - accuracy: 0.9461\n",
      "Epoch 158/300\n",
      " - 0s - loss: 0.1402 - accuracy: 0.9481\n",
      "Epoch 159/300\n",
      " - 0s - loss: 0.1288 - accuracy: 0.9473\n",
      "Epoch 160/300\n",
      " - 0s - loss: 0.1394 - accuracy: 0.9468\n",
      "Epoch 161/300\n",
      " - 0s - loss: 0.1368 - accuracy: 0.9493\n",
      "Epoch 162/300\n",
      " - 0s - loss: 0.1343 - accuracy: 0.9491\n",
      "Epoch 163/300\n",
      " - 0s - loss: 0.1346 - accuracy: 0.9532\n",
      "Epoch 164/300\n",
      " - 0s - loss: 0.1323 - accuracy: 0.9520\n",
      "Epoch 165/300\n",
      " - 0s - loss: 0.1240 - accuracy: 0.9545\n",
      "Epoch 166/300\n",
      " - 0s - loss: 0.1341 - accuracy: 0.9535\n",
      "Epoch 167/300\n",
      " - 0s - loss: 0.1353 - accuracy: 0.9532\n",
      "Epoch 168/300\n",
      " - 0s - loss: 0.1183 - accuracy: 0.9582\n",
      "Epoch 169/300\n",
      " - 0s - loss: 0.1388 - accuracy: 0.9510\n",
      "Epoch 170/300\n",
      " - 0s - loss: 0.1285 - accuracy: 0.9542\n",
      "Epoch 171/300\n",
      " - 0s - loss: 0.1263 - accuracy: 0.9572\n",
      "Epoch 172/300\n",
      " - 0s - loss: 0.1236 - accuracy: 0.9552\n",
      "Epoch 173/300\n",
      " - 0s - loss: 0.1209 - accuracy: 0.9569\n",
      "Epoch 174/300\n",
      " - 0s - loss: 0.1402 - accuracy: 0.9481\n",
      "Epoch 175/300\n",
      " - 0s - loss: 0.1389 - accuracy: 0.9483\n",
      "Epoch 176/300\n",
      " - 0s - loss: 0.1294 - accuracy: 0.9508\n",
      "Epoch 177/300\n",
      " - 0s - loss: 0.1292 - accuracy: 0.9532\n",
      "Epoch 178/300\n",
      " - 0s - loss: 0.1179 - accuracy: 0.9537\n",
      "Epoch 179/300\n",
      " - 0s - loss: 0.1183 - accuracy: 0.9547\n",
      "Epoch 180/300\n",
      " - 0s - loss: 0.1310 - accuracy: 0.9545\n",
      "Epoch 181/300\n",
      " - 0s - loss: 0.1199 - accuracy: 0.9537\n",
      "Epoch 182/300\n",
      " - 0s - loss: 0.1204 - accuracy: 0.9582\n",
      "Epoch 183/300\n",
      " - 0s - loss: 0.1199 - accuracy: 0.9547\n",
      "Epoch 184/300\n",
      " - 0s - loss: 0.1190 - accuracy: 0.9606\n",
      "Epoch 185/300\n",
      " - 0s - loss: 0.1289 - accuracy: 0.9537\n",
      "Epoch 186/300\n",
      " - 0s - loss: 0.1289 - accuracy: 0.9535\n",
      "Epoch 187/300\n",
      " - 0s - loss: 0.1125 - accuracy: 0.9599\n",
      "Epoch 188/300\n",
      " - 0s - loss: 0.1302 - accuracy: 0.9520\n",
      "Epoch 189/300\n",
      " - 0s - loss: 0.1290 - accuracy: 0.9540\n",
      "Epoch 190/300\n",
      " - 0s - loss: 0.1194 - accuracy: 0.9577\n",
      "Epoch 191/300\n",
      " - 0s - loss: 0.1137 - accuracy: 0.9579\n",
      "Epoch 192/300\n",
      " - 0s - loss: 0.1182 - accuracy: 0.9582\n",
      "Epoch 193/300\n",
      " - 0s - loss: 0.1212 - accuracy: 0.9564\n",
      "Epoch 194/300\n",
      " - 0s - loss: 0.1209 - accuracy: 0.9574\n",
      "Epoch 195/300\n",
      " - 0s - loss: 0.1177 - accuracy: 0.9557\n",
      "Epoch 196/300\n",
      " - 0s - loss: 0.1382 - accuracy: 0.9486\n",
      "Epoch 197/300\n",
      " - 0s - loss: 0.1270 - accuracy: 0.9552\n",
      "Epoch 198/300\n",
      " - 0s - loss: 0.1265 - accuracy: 0.9555\n",
      "Epoch 199/300\n",
      " - 0s - loss: 0.1121 - accuracy: 0.9589\n",
      "Epoch 200/300\n",
      " - 0s - loss: 0.1248 - accuracy: 0.9542\n",
      "Epoch 201/300\n",
      " - 0s - loss: 0.1084 - accuracy: 0.9574\n",
      "Epoch 202/300\n",
      " - 0s - loss: 0.1272 - accuracy: 0.9564\n",
      "Epoch 203/300\n",
      " - 0s - loss: 0.1095 - accuracy: 0.9631\n",
      "Epoch 204/300\n",
      " - 0s - loss: 0.1167 - accuracy: 0.9559\n",
      "Epoch 205/300\n",
      " - 0s - loss: 0.1193 - accuracy: 0.9559\n",
      "Epoch 206/300\n",
      " - 0s - loss: 0.1147 - accuracy: 0.9587\n",
      "Epoch 207/300\n",
      " - 0s - loss: 0.1175 - accuracy: 0.9574\n",
      "Epoch 208/300\n",
      " - 0s - loss: 0.1101 - accuracy: 0.9609\n",
      "Epoch 209/300\n",
      " - 0s - loss: 0.1149 - accuracy: 0.9596\n",
      "Epoch 210/300\n",
      " - 0s - loss: 0.1112 - accuracy: 0.9577\n",
      "Epoch 211/300\n",
      " - 0s - loss: 0.1116 - accuracy: 0.9572\n",
      "Epoch 212/300\n",
      " - 0s - loss: 0.1135 - accuracy: 0.9614\n",
      "Epoch 213/300\n",
      " - 0s - loss: 0.1273 - accuracy: 0.9527\n",
      "Epoch 214/300\n",
      " - 0s - loss: 0.1085 - accuracy: 0.9609\n",
      "Epoch 215/300\n",
      " - 0s - loss: 0.1138 - accuracy: 0.9596\n",
      "Epoch 216/300\n",
      " - 0s - loss: 0.1257 - accuracy: 0.9572\n",
      "Epoch 217/300\n",
      " - 0s - loss: 0.1077 - accuracy: 0.9599\n",
      "Epoch 218/300\n",
      " - 0s - loss: 0.1107 - accuracy: 0.9582\n",
      "Epoch 219/300\n",
      " - 0s - loss: 0.1100 - accuracy: 0.9626\n",
      "Epoch 220/300\n",
      " - 0s - loss: 0.1166 - accuracy: 0.9584\n",
      "Epoch 221/300\n",
      " - 0s - loss: 0.1274 - accuracy: 0.9520\n",
      "Epoch 222/300\n",
      " - 0s - loss: 0.1219 - accuracy: 0.9582\n",
      "Epoch 223/300\n",
      " - 0s - loss: 0.1197 - accuracy: 0.9557\n",
      "Epoch 224/300\n",
      " - 0s - loss: 0.1007 - accuracy: 0.9641\n",
      "Epoch 225/300\n",
      " - 0s - loss: 0.1074 - accuracy: 0.9614\n",
      "Epoch 226/300\n",
      " - 0s - loss: 0.1154 - accuracy: 0.9559\n",
      "Epoch 227/300\n",
      " - 0s - loss: 0.1053 - accuracy: 0.9636\n",
      "Epoch 228/300\n",
      " - 0s - loss: 0.1143 - accuracy: 0.9606\n",
      "Epoch 229/300\n",
      " - 0s - loss: 0.1142 - accuracy: 0.9604\n",
      "Epoch 230/300\n",
      " - 0s - loss: 0.1056 - accuracy: 0.9611\n",
      "Epoch 231/300\n",
      " - 0s - loss: 0.1143 - accuracy: 0.9557\n",
      "Epoch 232/300\n",
      " - 0s - loss: 0.1049 - accuracy: 0.9611\n",
      "Epoch 233/300\n",
      " - 0s - loss: 0.1000 - accuracy: 0.9638\n",
      "Epoch 234/300\n",
      " - 0s - loss: 0.1113 - accuracy: 0.9559\n",
      "Epoch 235/300\n",
      " - 0s - loss: 0.1203 - accuracy: 0.9606\n",
      "Epoch 236/300\n",
      " - 0s - loss: 0.1107 - accuracy: 0.9589\n",
      "Epoch 237/300\n",
      " - 0s - loss: 0.1106 - accuracy: 0.9609\n",
      "Epoch 238/300\n",
      " - 0s - loss: 0.0961 - accuracy: 0.9633\n",
      "Epoch 239/300\n",
      " - 0s - loss: 0.1090 - accuracy: 0.9579\n",
      "Epoch 240/300\n",
      " - 0s - loss: 0.1035 - accuracy: 0.9641\n",
      "Epoch 241/300\n",
      " - 0s - loss: 0.1091 - accuracy: 0.9614\n",
      "Epoch 242/300\n",
      " - 0s - loss: 0.1024 - accuracy: 0.9655\n",
      "Epoch 243/300\n",
      " - 0s - loss: 0.1086 - accuracy: 0.9628\n",
      "Epoch 244/300\n",
      " - 0s - loss: 0.1044 - accuracy: 0.9631\n",
      "Epoch 245/300\n",
      " - 0s - loss: 0.1056 - accuracy: 0.9633\n",
      "Epoch 246/300\n",
      " - 0s - loss: 0.1043 - accuracy: 0.9643\n",
      "Epoch 247/300\n",
      " - 0s - loss: 0.1014 - accuracy: 0.9660\n",
      "Epoch 248/300\n",
      " - 0s - loss: 0.1082 - accuracy: 0.9616\n",
      "Epoch 249/300\n",
      " - 0s - loss: 0.0993 - accuracy: 0.9643\n",
      "Epoch 250/300\n",
      " - 0s - loss: 0.1067 - accuracy: 0.9574\n",
      "Epoch 251/300\n",
      " - 0s - loss: 0.1012 - accuracy: 0.9623\n",
      "Epoch 252/300\n",
      " - 0s - loss: 0.1103 - accuracy: 0.9604\n",
      "Epoch 253/300\n",
      " - 0s - loss: 0.1102 - accuracy: 0.9572\n",
      "Epoch 254/300\n",
      " - 0s - loss: 0.1015 - accuracy: 0.9648\n",
      "Epoch 255/300\n",
      " - 0s - loss: 0.1091 - accuracy: 0.9591\n",
      "Epoch 256/300\n",
      " - 0s - loss: 0.1013 - accuracy: 0.9621\n",
      "Epoch 257/300\n",
      " - 0s - loss: 0.1147 - accuracy: 0.9582\n",
      "Epoch 258/300\n",
      " - 0s - loss: 0.0992 - accuracy: 0.9668\n",
      "Epoch 259/300\n",
      " - 0s - loss: 0.1114 - accuracy: 0.9604\n",
      "Epoch 260/300\n",
      " - 0s - loss: 0.1011 - accuracy: 0.9633\n",
      "Epoch 261/300\n",
      " - 0s - loss: 0.1078 - accuracy: 0.9601\n",
      "Epoch 262/300\n",
      " - 0s - loss: 0.0993 - accuracy: 0.9655\n",
      "Epoch 263/300\n",
      " - 0s - loss: 0.1008 - accuracy: 0.9621\n",
      "Epoch 264/300\n",
      " - 0s - loss: 0.1092 - accuracy: 0.9648\n",
      "Epoch 265/300\n",
      " - 0s - loss: 0.1043 - accuracy: 0.9655\n",
      "Epoch 266/300\n",
      " - 0s - loss: 0.0988 - accuracy: 0.9621\n",
      "Epoch 267/300\n",
      " - 0s - loss: 0.0957 - accuracy: 0.9641\n",
      "Epoch 268/300\n",
      " - 0s - loss: 0.1026 - accuracy: 0.9628\n",
      "Epoch 269/300\n",
      " - 0s - loss: 0.1033 - accuracy: 0.9658\n",
      "Epoch 270/300\n",
      " - 0s - loss: 0.1082 - accuracy: 0.9621\n",
      "Epoch 271/300\n",
      " - 0s - loss: 0.1120 - accuracy: 0.9611\n",
      "Epoch 272/300\n",
      " - 0s - loss: 0.1043 - accuracy: 0.9638\n",
      "Epoch 273/300\n",
      " - 0s - loss: 0.1082 - accuracy: 0.9636\n",
      "Epoch 274/300\n",
      " - 0s - loss: 0.1113 - accuracy: 0.9599\n",
      "Epoch 275/300\n",
      " - 0s - loss: 0.1015 - accuracy: 0.9631\n",
      "Epoch 276/300\n",
      " - 0s - loss: 0.1079 - accuracy: 0.9601\n",
      "Epoch 277/300\n",
      " - 0s - loss: 0.0976 - accuracy: 0.9623\n",
      "Epoch 278/300\n",
      " - 0s - loss: 0.1035 - accuracy: 0.9614\n",
      "Epoch 279/300\n",
      " - 0s - loss: 0.1010 - accuracy: 0.9665\n",
      "Epoch 280/300\n",
      " - 0s - loss: 0.0936 - accuracy: 0.9651\n",
      "Epoch 281/300\n",
      " - 0s - loss: 0.1038 - accuracy: 0.9660\n",
      "Epoch 282/300\n",
      " - 0s - loss: 0.1093 - accuracy: 0.9648\n",
      "Epoch 283/300\n",
      " - 0s - loss: 0.1008 - accuracy: 0.9648\n",
      "Epoch 284/300\n",
      " - 0s - loss: 0.0988 - accuracy: 0.9653\n",
      "Epoch 285/300\n",
      " - 0s - loss: 0.0992 - accuracy: 0.9638\n",
      "Epoch 286/300\n",
      " - 0s - loss: 0.1027 - accuracy: 0.9646\n",
      "Epoch 287/300\n",
      " - 0s - loss: 0.0954 - accuracy: 0.9658\n",
      "Epoch 288/300\n",
      " - 0s - loss: 0.1080 - accuracy: 0.9628\n",
      "Epoch 289/300\n",
      " - 0s - loss: 0.1043 - accuracy: 0.9606\n",
      "Epoch 290/300\n",
      " - 0s - loss: 0.0947 - accuracy: 0.9700\n",
      "Epoch 291/300\n",
      " - 0s - loss: 0.0935 - accuracy: 0.9665\n",
      "Epoch 292/300\n",
      " - 0s - loss: 0.1007 - accuracy: 0.9655\n",
      "Epoch 293/300\n",
      " - 0s - loss: 0.1038 - accuracy: 0.9631\n",
      "Epoch 294/300\n",
      " - 0s - loss: 0.1020 - accuracy: 0.9646\n",
      "Epoch 295/300\n",
      " - 0s - loss: 0.1006 - accuracy: 0.9653\n",
      "Epoch 296/300\n",
      " - 0s - loss: 0.0940 - accuracy: 0.9633\n",
      "Epoch 297/300\n",
      " - 0s - loss: 0.1023 - accuracy: 0.9651\n",
      "Epoch 298/300\n",
      " - 0s - loss: 0.0928 - accuracy: 0.9668\n",
      "Epoch 299/300\n",
      " - 0s - loss: 0.1031 - accuracy: 0.9648\n",
      "Epoch 300/300\n",
      " - 0s - loss: 0.0953 - accuracy: 0.9675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1583bbb46d8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "model.fit(X_train_scaled, y_train_categorical, epochs=300, class_weight=class_weights, verbose=2)\n",
    "\n",
    "#model.fit(\n",
    "#    X_train_scaled,\n",
    "#    y_train_categorical,\n",
    "#    epochs=200,\n",
    "#    shuffle=True,\n",
    "#    verbose=2\n",
    "#)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Neural Network - Loss: 0.4938596262263196, Accuracy: 0.8649446368217468\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "#other code to use label encoder\n",
    "\n",
    "#label_encoder = LabelEncoder()\n",
    "#integer_encoded = label_encoder.fit_transform(data_reformat['class'].values)\n",
    "#print(integer_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Neural Network - Loss: 0.026464736286010718, Accuracy: 0.995323657989502\n"
     ]
    }
   ],
   "source": [
    "model_loss_training, model_accuracy_training = model.evaluate(\n",
    "    X_train_scaled, y_train_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss_training}, Accuracy: {model_accuracy_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_19_input to have 3 dimensions, but got array with shape (1355, 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-173-7ad92aaa8e0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0my_pred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;31m# Take the class with the highest probability from the test predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mmax_y_pred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[1;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1441\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1442\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    133\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    136\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected lstm_19_input to have 3 dimensions, but got array with shape (1355, 42)"
     ]
    }
   ],
   "source": [
    "def show_confusion_matrix(validations, predictions):\n",
    "\n",
    "    matrix = metrics.confusion_matrix(validations, predictions)\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    plt.tight_layout()\n",
    "    sns.heatmap(matrix,\n",
    "                cmap='coolwarm',\n",
    "                linecolor='white',\n",
    "                linewidths=1,\n",
    "                xticklabels= [1,2,3,4,5,6],\n",
    "                yticklabels= [1,2,3,4,5,6],\n",
    "                annot=True,\n",
    "                square=True,\n",
    "                fmt='d')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "y_pred_test = model.predict(X_test_scaled)\n",
    "# Take the class with the highest probability from the test predictions\n",
    "max_y_pred_test = np.argmax(y_pred_test, axis=1)\n",
    "max_y_test = np.argmax(y_test_categorical, axis=1)\n",
    "\n",
    "show_confusion_matrix(max_y_test, max_y_pred_test)\n",
    "\n",
    "print(classification_report(max_y_test, max_y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab just one data point to test with\n",
    "test = np.expand_dims(X_test_scaled[100], axis=0)\n",
    "result = np.expand_dims(y_test_categorical[100], axis=0)\n",
    "test.shape\n",
    "test\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_2_input to have 3 dimensions, but got array with shape (1, 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-180-c2bb14c68125>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Make a prediction. The result should be 5 - STANDING\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Predicted class: {model.predict_classes(test)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36mpredict_classes\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \"\"\"\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mproba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mproba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[1;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1441\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1442\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    133\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    136\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected lstm_2_input to have 3 dimensions, but got array with shape (1, 42)"
     ]
    }
   ],
   "source": [
    "# Make a prediction. The result should be 5 - STANDING\n",
    "print(f\"Predicted class: {model.predict_classes(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout , BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "#adding more layers to the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=60, activation='relu', input_dim=42))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Dense(196, activation='relu'))\n",
    "model.add(Dense(196, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(units=6, activation='softmax'))\n",
    "#keras.layers.Dropout(0.05, noise_shape=None, seed=5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 60)                2580      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               6100      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                6464      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 196)               12740     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 196)               38612     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                6304      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 72,998\n",
      "Trainable params: 72,998\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4063 samples\n",
      "Epoch 1/100\n",
      "4063/4063 - 3s - loss: 0.9758 - accuracy: 0.6567\n",
      "Epoch 2/100\n",
      "4063/4063 - 0s - loss: 0.6059 - accuracy: 0.7573\n",
      "Epoch 3/100\n",
      "4063/4063 - 0s - loss: 0.5199 - accuracy: 0.7923\n",
      "Epoch 4/100\n",
      "4063/4063 - 0s - loss: 0.4592 - accuracy: 0.8110\n",
      "Epoch 5/100\n",
      "4063/4063 - 0s - loss: 0.4148 - accuracy: 0.8317\n",
      "Epoch 6/100\n",
      "4063/4063 - 0s - loss: 0.4017 - accuracy: 0.8353\n",
      "Epoch 7/100\n",
      "4063/4063 - 0s - loss: 0.3664 - accuracy: 0.8536\n",
      "Epoch 8/100\n",
      "4063/4063 - 0s - loss: 0.3264 - accuracy: 0.8646\n",
      "Epoch 9/100\n",
      "4063/4063 - 0s - loss: 0.2966 - accuracy: 0.8814\n",
      "Epoch 10/100\n",
      "4063/4063 - 0s - loss: 0.2577 - accuracy: 0.8961\n",
      "Epoch 11/100\n",
      "4063/4063 - 0s - loss: 0.2564 - accuracy: 0.8932\n",
      "Epoch 12/100\n",
      "4063/4063 - 0s - loss: 0.2201 - accuracy: 0.9084\n",
      "Epoch 13/100\n",
      "4063/4063 - 0s - loss: 0.2064 - accuracy: 0.9195\n",
      "Epoch 14/100\n",
      "4063/4063 - 0s - loss: 0.2023 - accuracy: 0.9203\n",
      "Epoch 15/100\n",
      "4063/4063 - 0s - loss: 0.1988 - accuracy: 0.9225\n",
      "Epoch 16/100\n",
      "4063/4063 - 0s - loss: 0.1536 - accuracy: 0.9429\n",
      "Epoch 17/100\n",
      "4063/4063 - 0s - loss: 0.1302 - accuracy: 0.9463\n",
      "Epoch 18/100\n",
      "4063/4063 - 0s - loss: 0.1254 - accuracy: 0.9510\n",
      "Epoch 19/100\n",
      "4063/4063 - 0s - loss: 0.1431 - accuracy: 0.9466\n",
      "Epoch 20/100\n",
      "4063/4063 - 0s - loss: 0.1275 - accuracy: 0.9510\n",
      "Epoch 21/100\n",
      "4063/4063 - 0s - loss: 0.1039 - accuracy: 0.9619\n",
      "Epoch 22/100\n",
      "4063/4063 - 0s - loss: 0.0862 - accuracy: 0.9668\n",
      "Epoch 23/100\n",
      "4063/4063 - 0s - loss: 0.0788 - accuracy: 0.9724\n",
      "Epoch 24/100\n",
      "4063/4063 - 0s - loss: 0.0685 - accuracy: 0.9746\n",
      "Epoch 25/100\n",
      "4063/4063 - 0s - loss: 0.0854 - accuracy: 0.9700\n",
      "Epoch 26/100\n",
      "4063/4063 - 0s - loss: 0.0931 - accuracy: 0.9678\n",
      "Epoch 27/100\n",
      "4063/4063 - 0s - loss: 0.0858 - accuracy: 0.9727\n",
      "Epoch 28/100\n",
      "4063/4063 - 0s - loss: 0.0698 - accuracy: 0.9744\n",
      "Epoch 29/100\n",
      "4063/4063 - 0s - loss: 0.0616 - accuracy: 0.9783\n",
      "Epoch 30/100\n",
      "4063/4063 - 0s - loss: 0.0773 - accuracy: 0.9737\n",
      "Epoch 31/100\n",
      "4063/4063 - 0s - loss: 0.0875 - accuracy: 0.9702\n",
      "Epoch 32/100\n",
      "4063/4063 - 0s - loss: 0.0417 - accuracy: 0.9862\n",
      "Epoch 33/100\n",
      "4063/4063 - 0s - loss: 0.0358 - accuracy: 0.9857\n",
      "Epoch 34/100\n",
      "4063/4063 - 0s - loss: 0.0420 - accuracy: 0.9872\n",
      "Epoch 35/100\n",
      "4063/4063 - 0s - loss: 0.0568 - accuracy: 0.9783\n",
      "Epoch 36/100\n",
      "4063/4063 - 0s - loss: 0.0660 - accuracy: 0.9766\n",
      "Epoch 37/100\n",
      "4063/4063 - 0s - loss: 0.0227 - accuracy: 0.9943\n",
      "Epoch 38/100\n",
      "4063/4063 - 0s - loss: 0.0261 - accuracy: 0.9914\n",
      "Epoch 39/100\n",
      "4063/4063 - 0s - loss: 0.0504 - accuracy: 0.9803\n",
      "Epoch 40/100\n",
      "4063/4063 - 0s - loss: 0.0923 - accuracy: 0.9719\n",
      "Epoch 41/100\n",
      "4063/4063 - 0s - loss: 0.0453 - accuracy: 0.9833\n",
      "Epoch 42/100\n",
      "4063/4063 - 0s - loss: 0.0513 - accuracy: 0.9820\n",
      "Epoch 43/100\n",
      "4063/4063 - 0s - loss: 0.0302 - accuracy: 0.9902\n",
      "Epoch 44/100\n",
      "4063/4063 - 0s - loss: 0.0371 - accuracy: 0.9889\n",
      "Epoch 45/100\n",
      "4063/4063 - 0s - loss: 0.0261 - accuracy: 0.9906\n",
      "Epoch 46/100\n",
      "4063/4063 - 0s - loss: 0.0254 - accuracy: 0.9921\n",
      "Epoch 47/100\n",
      "4063/4063 - 0s - loss: 0.0233 - accuracy: 0.9914\n",
      "Epoch 48/100\n",
      "4063/4063 - 0s - loss: 0.0314 - accuracy: 0.9897\n",
      "Epoch 49/100\n",
      "4063/4063 - 0s - loss: 0.0759 - accuracy: 0.9764\n",
      "Epoch 50/100\n",
      "4063/4063 - 0s - loss: 0.0475 - accuracy: 0.9857\n",
      "Epoch 51/100\n",
      "4063/4063 - 0s - loss: 0.0373 - accuracy: 0.9889\n",
      "Epoch 52/100\n",
      "4063/4063 - 0s - loss: 0.0190 - accuracy: 0.9938\n",
      "Epoch 53/100\n",
      "4063/4063 - 0s - loss: 0.0332 - accuracy: 0.9887\n",
      "Epoch 54/100\n",
      "4063/4063 - 0s - loss: 0.0440 - accuracy: 0.9855\n",
      "Epoch 55/100\n",
      "4063/4063 - 0s - loss: 0.0317 - accuracy: 0.9894\n",
      "Epoch 56/100\n",
      "4063/4063 - 0s - loss: 0.0317 - accuracy: 0.9879\n",
      "Epoch 57/100\n",
      "4063/4063 - 0s - loss: 0.0266 - accuracy: 0.9926\n",
      "Epoch 58/100\n",
      "4063/4063 - 0s - loss: 0.0322 - accuracy: 0.9902\n",
      "Epoch 59/100\n",
      "4063/4063 - 0s - loss: 0.0314 - accuracy: 0.9892\n",
      "Epoch 60/100\n",
      "4063/4063 - 0s - loss: 0.0389 - accuracy: 0.9882\n",
      "Epoch 61/100\n",
      "4063/4063 - 0s - loss: 0.0434 - accuracy: 0.9867\n",
      "Epoch 62/100\n",
      "4063/4063 - 0s - loss: 0.0445 - accuracy: 0.9845\n",
      "Epoch 63/100\n",
      "4063/4063 - 0s - loss: 0.0285 - accuracy: 0.9894\n",
      "Epoch 64/100\n",
      "4063/4063 - 0s - loss: 0.0233 - accuracy: 0.9919\n",
      "Epoch 65/100\n",
      "4063/4063 - 0s - loss: 0.0253 - accuracy: 0.9894\n",
      "Epoch 66/100\n",
      "4063/4063 - 0s - loss: 0.0360 - accuracy: 0.9874\n",
      "Epoch 67/100\n",
      "4063/4063 - 0s - loss: 0.0218 - accuracy: 0.9926\n",
      "Epoch 68/100\n",
      "4063/4063 - 0s - loss: 0.0358 - accuracy: 0.9892\n",
      "Epoch 69/100\n",
      "4063/4063 - 0s - loss: 0.0423 - accuracy: 0.9870\n",
      "Epoch 70/100\n",
      "4063/4063 - 0s - loss: 0.0254 - accuracy: 0.9921\n",
      "Epoch 71/100\n",
      "4063/4063 - 0s - loss: 0.0110 - accuracy: 0.9966\n",
      "Epoch 72/100\n",
      "4063/4063 - 0s - loss: 0.0110 - accuracy: 0.9956\n",
      "Epoch 73/100\n",
      "4063/4063 - 0s - loss: 0.0082 - accuracy: 0.9970\n",
      "Epoch 74/100\n",
      "4063/4063 - 0s - loss: 0.0138 - accuracy: 0.9956\n",
      "Epoch 75/100\n",
      "4063/4063 - 0s - loss: 0.0192 - accuracy: 0.9941\n",
      "Epoch 76/100\n",
      "4063/4063 - 0s - loss: 0.0227 - accuracy: 0.9936\n",
      "Epoch 77/100\n",
      "4063/4063 - 0s - loss: 0.0472 - accuracy: 0.9877\n",
      "Epoch 78/100\n",
      "4063/4063 - 0s - loss: 0.0334 - accuracy: 0.9897\n",
      "Epoch 79/100\n",
      "4063/4063 - 0s - loss: 0.0474 - accuracy: 0.9847\n",
      "Epoch 80/100\n",
      "4063/4063 - 0s - loss: 0.0440 - accuracy: 0.9882\n",
      "Epoch 81/100\n",
      "4063/4063 - 0s - loss: 0.0203 - accuracy: 0.9929\n",
      "Epoch 82/100\n",
      "4063/4063 - 0s - loss: 0.0266 - accuracy: 0.9894\n",
      "Epoch 83/100\n",
      "4063/4063 - 0s - loss: 0.0410 - accuracy: 0.9855\n",
      "Epoch 84/100\n",
      "4063/4063 - 0s - loss: 0.0279 - accuracy: 0.9919\n",
      "Epoch 85/100\n",
      "4063/4063 - 0s - loss: 0.0255 - accuracy: 0.9919\n",
      "Epoch 86/100\n",
      "4063/4063 - 0s - loss: 0.0233 - accuracy: 0.9926\n",
      "Epoch 87/100\n",
      "4063/4063 - 0s - loss: 0.0159 - accuracy: 0.9941\n",
      "Epoch 88/100\n",
      "4063/4063 - 0s - loss: 0.0122 - accuracy: 0.9963\n",
      "Epoch 89/100\n",
      "4063/4063 - 1s - loss: 0.0224 - accuracy: 0.9919\n",
      "Epoch 90/100\n",
      "4063/4063 - 0s - loss: 0.0203 - accuracy: 0.9926\n",
      "Epoch 91/100\n",
      "4063/4063 - 0s - loss: 0.0153 - accuracy: 0.9951\n",
      "Epoch 92/100\n",
      "4063/4063 - 0s - loss: 0.0392 - accuracy: 0.9860\n",
      "Epoch 93/100\n",
      "4063/4063 - 0s - loss: 0.0478 - accuracy: 0.9847\n",
      "Epoch 94/100\n",
      "4063/4063 - 0s - loss: 0.0449 - accuracy: 0.9862\n",
      "Epoch 95/100\n",
      "4063/4063 - 0s - loss: 0.0216 - accuracy: 0.9926\n",
      "Epoch 96/100\n",
      "4063/4063 - 0s - loss: 0.0074 - accuracy: 0.9978\n",
      "Epoch 97/100\n",
      "4063/4063 - 0s - loss: 0.0090 - accuracy: 0.9968\n",
      "Epoch 98/100\n",
      "4063/4063 - 0s - loss: 0.0234 - accuracy: 0.9936\n",
      "Epoch 99/100\n",
      "4063/4063 - 0s - loss: 0.0463 - accuracy: 0.9877\n",
      "Epoch 100/100\n",
      "4063/4063 - 0s - loss: 0.0516 - accuracy: 0.9833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1584bd8cd68>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=100,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1355/1 - 1s - loss: 1.2519 - accuracy: 0.8428\n",
      "Normal Neural Network - Loss: 0.8624963840435352, Accuracy: 0.8428044319152832\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.layers import Dense,LSTM,Dropout\n",
    "\n",
    "#np.random.seed(7)\n",
    "#model = Sequential()\n",
    "#model.add(LSTM(24, input_dim =42,return_sequences=True))\n",
    "#model.add(LSTM(12))\n",
    "#model.add(Dense(6, activation='sigmoid'))\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('WISDM_ar_latest/WISDM_ar_v1.1/WISDM_ar_v1.1.csv')\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import LSTM, Dense\n",
    "# import numpy as np\n",
    "\n",
    "# data_dim = 6\n",
    "# timesteps = 100\n",
    "# num_classes = 10\n",
    "# batch_size = 32\n",
    "\n",
    "# # Expected input batch shape: (batch_size, timesteps, data_dim)\n",
    "# # Note that we have to provide the full batch_input_shape since the network is stateful.\n",
    "# # the sample of index i in batch k is the follow-up for the sample i in batch k-1.\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(32, return_sequences=True, stateful=True,\n",
    "#                batch_input_shape=(batch_size, timesteps, data_dim)))\n",
    "# model.add(LSTM(32, return_sequences=True, stateful=True))\n",
    "# model.add(LSTM(32, stateful=True))\n",
    "# model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Generate dummy training data\n",
    "# x_train = np.random.random((batch_size * 10, timesteps, data_dim))\n",
    "# y_train = np.random.random((batch_size * 10, num_classes))\n",
    "\n",
    "# # Generate dummy validation data\n",
    "# x_val = np.random.random((batch_size * 3, timesteps, data_dim))\n",
    "# y_val = np.random.random((batch_size * 3, num_classes))\n",
    "\n",
    "# model.fit(x_train, y_train,\n",
    "#           batch_size=batch_size, epochs=5, shuffle=False,\n",
    "#           validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>activity</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49105962326000</td>\n",
       "      <td>-0.694638</td>\n",
       "      <td>12.680544</td>\n",
       "      <td>0.503953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106062271000</td>\n",
       "      <td>5.012288</td>\n",
       "      <td>11.264028</td>\n",
       "      <td>0.953424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106112167000</td>\n",
       "      <td>4.903325</td>\n",
       "      <td>10.882658</td>\n",
       "      <td>-0.081722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106222305000</td>\n",
       "      <td>-0.612916</td>\n",
       "      <td>18.496431</td>\n",
       "      <td>3.023717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106332290000</td>\n",
       "      <td>-1.184970</td>\n",
       "      <td>12.108489</td>\n",
       "      <td>7.205164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098204</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623331483000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>-1.570000</td>\n",
       "      <td>1.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098205</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623371431000</td>\n",
       "      <td>9.040000</td>\n",
       "      <td>-1.460000</td>\n",
       "      <td>1.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098206</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623411592000</td>\n",
       "      <td>9.080000</td>\n",
       "      <td>-1.380000</td>\n",
       "      <td>1.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098207</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623491487000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>-1.460000</td>\n",
       "      <td>1.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098208</th>\n",
       "      <td>19</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>131623531465000</td>\n",
       "      <td>8.880000</td>\n",
       "      <td>-1.330000</td>\n",
       "      <td>1.610000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1098209 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id activity        timestamp     acc_x      acc_y     acc_z\n",
       "0             33  Jogging   49105962326000 -0.694638  12.680544  0.503953\n",
       "1             33  Jogging   49106062271000  5.012288  11.264028  0.953424\n",
       "2             33  Jogging   49106112167000  4.903325  10.882658 -0.081722\n",
       "3             33  Jogging   49106222305000 -0.612916  18.496431  3.023717\n",
       "4             33  Jogging   49106332290000 -1.184970  12.108489  7.205164\n",
       "...          ...      ...              ...       ...        ...       ...\n",
       "1098204       19  Sitting  131623331483000  9.000000  -1.570000  1.690000\n",
       "1098205       19  Sitting  131623371431000  9.040000  -1.460000  1.730000\n",
       "1098206       19  Sitting  131623411592000  9.080000  -1.380000  1.690000\n",
       "1098207       19  Sitting  131623491487000  9.000000  -1.460000  1.730000\n",
       "1098208       19  Sitting  131623531465000  8.880000  -1.330000  1.610000\n",
       "\n",
       "[1098209 rows x 6 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('WISDM_ar_latest/WISDM_ar_v1.1/WISDM_ar_v1.1.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>activity</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>ActivityEncoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49105962326000</td>\n",
       "      <td>-0.694638</td>\n",
       "      <td>12.680544</td>\n",
       "      <td>0.503953</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106062271000</td>\n",
       "      <td>5.012288</td>\n",
       "      <td>11.264028</td>\n",
       "      <td>0.953424</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106112167000</td>\n",
       "      <td>4.903325</td>\n",
       "      <td>10.882658</td>\n",
       "      <td>-0.081722</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106222305000</td>\n",
       "      <td>-0.612916</td>\n",
       "      <td>18.496431</td>\n",
       "      <td>3.023717</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106332290000</td>\n",
       "      <td>-1.184970</td>\n",
       "      <td>12.108489</td>\n",
       "      <td>7.205164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106442306000</td>\n",
       "      <td>1.375655</td>\n",
       "      <td>-2.492524</td>\n",
       "      <td>-6.510526</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106542312000</td>\n",
       "      <td>-0.612916</td>\n",
       "      <td>10.569390</td>\n",
       "      <td>5.706926</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106652389000</td>\n",
       "      <td>-0.503953</td>\n",
       "      <td>13.947236</td>\n",
       "      <td>7.055340</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106762313000</td>\n",
       "      <td>-8.430995</td>\n",
       "      <td>11.413852</td>\n",
       "      <td>5.134871</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>49106872299000</td>\n",
       "      <td>0.953424</td>\n",
       "      <td>1.375655</td>\n",
       "      <td>1.648062</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>33</td>\n",
       "      <td>Walking</td>\n",
       "      <td>49394992294000</td>\n",
       "      <td>0.844462</td>\n",
       "      <td>8.008764</td>\n",
       "      <td>2.792171</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>33</td>\n",
       "      <td>Walking</td>\n",
       "      <td>49395102310000</td>\n",
       "      <td>1.116869</td>\n",
       "      <td>8.621680</td>\n",
       "      <td>3.786457</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>33</td>\n",
       "      <td>Walking</td>\n",
       "      <td>49395202316000</td>\n",
       "      <td>-0.503953</td>\n",
       "      <td>16.657684</td>\n",
       "      <td>1.307553</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>33</td>\n",
       "      <td>Walking</td>\n",
       "      <td>49395302292000</td>\n",
       "      <td>4.794363</td>\n",
       "      <td>10.760075</td>\n",
       "      <td>-1.184970</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>33</td>\n",
       "      <td>Walking</td>\n",
       "      <td>49395412338000</td>\n",
       "      <td>-0.040861</td>\n",
       "      <td>9.234595</td>\n",
       "      <td>-0.694638</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>33</td>\n",
       "      <td>Walking</td>\n",
       "      <td>49395522293000</td>\n",
       "      <td>2.492524</td>\n",
       "      <td>8.730643</td>\n",
       "      <td>-1.457377</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>33</td>\n",
       "      <td>Walking</td>\n",
       "      <td>49395632309000</td>\n",
       "      <td>0.531194</td>\n",
       "      <td>9.888372</td>\n",
       "      <td>-1.225831</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>33</td>\n",
       "      <td>Walking</td>\n",
       "      <td>49395742294000</td>\n",
       "      <td>1.757025</td>\n",
       "      <td>11.032481</td>\n",
       "      <td>-0.653777</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>33</td>\n",
       "      <td>Walking</td>\n",
       "      <td>49395852340000</td>\n",
       "      <td>2.982856</td>\n",
       "      <td>12.149350</td>\n",
       "      <td>-1.307553</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>33</td>\n",
       "      <td>Walking</td>\n",
       "      <td>49395962295000</td>\n",
       "      <td>-0.803601</td>\n",
       "      <td>12.721405</td>\n",
       "      <td>-1.266692</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>33</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>49560572311000</td>\n",
       "      <td>10.119919</td>\n",
       "      <td>4.331271</td>\n",
       "      <td>-3.786457</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>33</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>49560682449000</td>\n",
       "      <td>4.862464</td>\n",
       "      <td>3.909040</td>\n",
       "      <td>-2.792171</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>33</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>49560782303000</td>\n",
       "      <td>9.466142</td>\n",
       "      <td>13.402422</td>\n",
       "      <td>-3.827318</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>33</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>49560842209000</td>\n",
       "      <td>5.747787</td>\n",
       "      <td>7.627395</td>\n",
       "      <td>-3.146300</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>33</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>49560942184000</td>\n",
       "      <td>2.301839</td>\n",
       "      <td>8.921328</td>\n",
       "      <td>-3.023717</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>33</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>49561002456000</td>\n",
       "      <td>7.818079</td>\n",
       "      <td>7.055340</td>\n",
       "      <td>-4.331271</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>33</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>49561062118000</td>\n",
       "      <td>2.833032</td>\n",
       "      <td>8.539958</td>\n",
       "      <td>-3.336985</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>33</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>49561162613000</td>\n",
       "      <td>1.797886</td>\n",
       "      <td>10.950760</td>\n",
       "      <td>-3.105439</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>33</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>49561262283000</td>\n",
       "      <td>0.994285</td>\n",
       "      <td>8.771504</td>\n",
       "      <td>-3.568531</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>33</td>\n",
       "      <td>Upstairs</td>\n",
       "      <td>49561332321000</td>\n",
       "      <td>1.225831</td>\n",
       "      <td>9.466142</td>\n",
       "      <td>-3.527670</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>33</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>49646322311000</td>\n",
       "      <td>-0.040861</td>\n",
       "      <td>4.985047</td>\n",
       "      <td>6.510526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>33</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>49646422317000</td>\n",
       "      <td>-0.463092</td>\n",
       "      <td>4.372132</td>\n",
       "      <td>7.436710</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>33</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>49646522323000</td>\n",
       "      <td>-0.299648</td>\n",
       "      <td>4.603678</td>\n",
       "      <td>6.510526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>33</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>49646572281000</td>\n",
       "      <td>-0.272407</td>\n",
       "      <td>4.481094</td>\n",
       "      <td>6.360703</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>33</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>49646672317000</td>\n",
       "      <td>-1.525479</td>\n",
       "      <td>5.175732</td>\n",
       "      <td>7.164303</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>33</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>49646782303000</td>\n",
       "      <td>1.266692</td>\n",
       "      <td>6.782933</td>\n",
       "      <td>8.853226</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>33</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>49646882461000</td>\n",
       "      <td>-1.116869</td>\n",
       "      <td>1.457377</td>\n",
       "      <td>7.082581</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>33</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>49646982315000</td>\n",
       "      <td>-0.381370</td>\n",
       "      <td>0.762740</td>\n",
       "      <td>7.545672</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>33</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>49647082260000</td>\n",
       "      <td>-0.653777</td>\n",
       "      <td>2.369940</td>\n",
       "      <td>7.273266</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>33</td>\n",
       "      <td>Downstairs</td>\n",
       "      <td>49647192306000</td>\n",
       "      <td>0.272407</td>\n",
       "      <td>1.648062</td>\n",
       "      <td>8.117727</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221335</th>\n",
       "      <td>27</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>12363992261000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>9.660000</td>\n",
       "      <td>1.035146</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221336</th>\n",
       "      <td>27</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>12364042279000</td>\n",
       "      <td>2.680000</td>\n",
       "      <td>9.530000</td>\n",
       "      <td>0.503953</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221337</th>\n",
       "      <td>27</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>12364092267000</td>\n",
       "      <td>3.490000</td>\n",
       "      <td>8.890000</td>\n",
       "      <td>0.762740</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221338</th>\n",
       "      <td>27</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>12364142316000</td>\n",
       "      <td>3.640000</td>\n",
       "      <td>9.380000</td>\n",
       "      <td>0.926184</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221339</th>\n",
       "      <td>27</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>12364192273000</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>9.340000</td>\n",
       "      <td>1.035146</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221340</th>\n",
       "      <td>27</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>12364242292000</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>9.430000</td>\n",
       "      <td>0.885323</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221341</th>\n",
       "      <td>27</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>12364292249000</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>9.530000</td>\n",
       "      <td>0.844462</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221342</th>\n",
       "      <td>27</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>12364342267000</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>9.720000</td>\n",
       "      <td>0.844462</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221343</th>\n",
       "      <td>27</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>12364392285000</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>9.380000</td>\n",
       "      <td>0.926184</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221344</th>\n",
       "      <td>27</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>12364442273000</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>9.430000</td>\n",
       "      <td>0.762740</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223434</th>\n",
       "      <td>27</td>\n",
       "      <td>Standing</td>\n",
       "      <td>12535892255000</td>\n",
       "      <td>-1.880000</td>\n",
       "      <td>9.850000</td>\n",
       "      <td>-0.231546</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223435</th>\n",
       "      <td>27</td>\n",
       "      <td>Standing</td>\n",
       "      <td>12535942273000</td>\n",
       "      <td>-0.190000</td>\n",
       "      <td>9.920000</td>\n",
       "      <td>-0.572055</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223436</th>\n",
       "      <td>27</td>\n",
       "      <td>Standing</td>\n",
       "      <td>12535992231000</td>\n",
       "      <td>-0.610000</td>\n",
       "      <td>10.270000</td>\n",
       "      <td>-0.885323</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223437</th>\n",
       "      <td>27</td>\n",
       "      <td>Standing</td>\n",
       "      <td>12536042310000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>10.570000</td>\n",
       "      <td>-1.757025</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223438</th>\n",
       "      <td>27</td>\n",
       "      <td>Standing</td>\n",
       "      <td>12536092206000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>9.470000</td>\n",
       "      <td>-1.116869</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223439</th>\n",
       "      <td>27</td>\n",
       "      <td>Standing</td>\n",
       "      <td>12536142285000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>9.470000</td>\n",
       "      <td>-1.947710</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223440</th>\n",
       "      <td>27</td>\n",
       "      <td>Standing</td>\n",
       "      <td>12536192243000</td>\n",
       "      <td>-1.040000</td>\n",
       "      <td>10.650000</td>\n",
       "      <td>-1.525479</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223441</th>\n",
       "      <td>27</td>\n",
       "      <td>Standing</td>\n",
       "      <td>12536242261000</td>\n",
       "      <td>-1.920000</td>\n",
       "      <td>9.510000</td>\n",
       "      <td>-0.572055</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223442</th>\n",
       "      <td>27</td>\n",
       "      <td>Standing</td>\n",
       "      <td>12536292249000</td>\n",
       "      <td>-1.310000</td>\n",
       "      <td>9.850000</td>\n",
       "      <td>-0.531194</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223443</th>\n",
       "      <td>27</td>\n",
       "      <td>Standing</td>\n",
       "      <td>12536342298000</td>\n",
       "      <td>-0.080000</td>\n",
       "      <td>9.920000</td>\n",
       "      <td>-1.757025</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id    activity       timestamp      acc_x      acc_y     acc_z  \\\n",
       "0            33     Jogging  49105962326000  -0.694638  12.680544  0.503953   \n",
       "1            33     Jogging  49106062271000   5.012288  11.264028  0.953424   \n",
       "2            33     Jogging  49106112167000   4.903325  10.882658 -0.081722   \n",
       "3            33     Jogging  49106222305000  -0.612916  18.496431  3.023717   \n",
       "4            33     Jogging  49106332290000  -1.184970  12.108489  7.205164   \n",
       "5            33     Jogging  49106442306000   1.375655  -2.492524 -6.510526   \n",
       "6            33     Jogging  49106542312000  -0.612916  10.569390  5.706926   \n",
       "7            33     Jogging  49106652389000  -0.503953  13.947236  7.055340   \n",
       "8            33     Jogging  49106762313000  -8.430995  11.413852  5.134871   \n",
       "9            33     Jogging  49106872299000   0.953424   1.375655  1.648062   \n",
       "597          33     Walking  49394992294000   0.844462   8.008764  2.792171   \n",
       "598          33     Walking  49395102310000   1.116869   8.621680  3.786457   \n",
       "599          33     Walking  49395202316000  -0.503953  16.657684  1.307553   \n",
       "600          33     Walking  49395302292000   4.794363  10.760075 -1.184970   \n",
       "601          33     Walking  49395412338000  -0.040861   9.234595 -0.694638   \n",
       "602          33     Walking  49395522293000   2.492524   8.730643 -1.457377   \n",
       "603          33     Walking  49395632309000   0.531194   9.888372 -1.225831   \n",
       "604          33     Walking  49395742294000   1.757025  11.032481 -0.653777   \n",
       "605          33     Walking  49395852340000   2.982856  12.149350 -1.307553   \n",
       "606          33     Walking  49395962295000  -0.803601  12.721405 -1.266692   \n",
       "1161         33    Upstairs  49560572311000  10.119919   4.331271 -3.786457   \n",
       "1162         33    Upstairs  49560682449000   4.862464   3.909040 -2.792171   \n",
       "1163         33    Upstairs  49560782303000   9.466142  13.402422 -3.827318   \n",
       "1164         33    Upstairs  49560842209000   5.747787   7.627395 -3.146300   \n",
       "1165         33    Upstairs  49560942184000   2.301839   8.921328 -3.023717   \n",
       "1166         33    Upstairs  49561002456000   7.818079   7.055340 -4.331271   \n",
       "1167         33    Upstairs  49561062118000   2.833032   8.539958 -3.336985   \n",
       "1168         33    Upstairs  49561162613000   1.797886  10.950760 -3.105439   \n",
       "1169         33    Upstairs  49561262283000   0.994285   8.771504 -3.568531   \n",
       "1170         33    Upstairs  49561332321000   1.225831   9.466142 -3.527670   \n",
       "1769         33  Downstairs  49646322311000  -0.040861   4.985047  6.510526   \n",
       "1770         33  Downstairs  49646422317000  -0.463092   4.372132  7.436710   \n",
       "1771         33  Downstairs  49646522323000  -0.299648   4.603678  6.510526   \n",
       "1772         33  Downstairs  49646572281000  -0.272407   4.481094  6.360703   \n",
       "1773         33  Downstairs  49646672317000  -1.525479   5.175732  7.164303   \n",
       "1774         33  Downstairs  49646782303000   1.266692   6.782933  8.853226   \n",
       "1775         33  Downstairs  49646882461000  -1.116869   1.457377  7.082581   \n",
       "1776         33  Downstairs  49646982315000  -0.381370   0.762740  7.545672   \n",
       "1777         33  Downstairs  49647082260000  -0.653777   2.369940  7.273266   \n",
       "1778         33  Downstairs  49647192306000   0.272407   1.648062  8.117727   \n",
       "221335       27     Sitting  12363992261000   2.600000   9.660000  1.035146   \n",
       "221336       27     Sitting  12364042279000   2.680000   9.530000  0.503953   \n",
       "221337       27     Sitting  12364092267000   3.490000   8.890000  0.762740   \n",
       "221338       27     Sitting  12364142316000   3.640000   9.380000  0.926184   \n",
       "221339       27     Sitting  12364192273000   2.910000   9.340000  1.035146   \n",
       "221340       27     Sitting  12364242292000   3.150000   9.430000  0.885323   \n",
       "221341       27     Sitting  12364292249000   3.150000   9.530000  0.844462   \n",
       "221342       27     Sitting  12364342267000   2.910000   9.720000  0.844462   \n",
       "221343       27     Sitting  12364392285000   3.210000   9.380000  0.926184   \n",
       "221344       27     Sitting  12364442273000   3.150000   9.430000  0.762740   \n",
       "223434       27    Standing  12535892255000  -1.880000   9.850000 -0.231546   \n",
       "223435       27    Standing  12535942273000  -0.190000   9.920000 -0.572055   \n",
       "223436       27    Standing  12535992231000  -0.610000  10.270000 -0.885323   \n",
       "223437       27    Standing  12536042310000   0.760000  10.570000 -1.757025   \n",
       "223438       27    Standing  12536092206000   0.420000   9.470000 -1.116869   \n",
       "223439       27    Standing  12536142285000   0.380000   9.470000 -1.947710   \n",
       "223440       27    Standing  12536192243000  -1.040000  10.650000 -1.525479   \n",
       "223441       27    Standing  12536242261000  -1.920000   9.510000 -0.572055   \n",
       "223442       27    Standing  12536292249000  -1.310000   9.850000 -0.531194   \n",
       "223443       27    Standing  12536342298000  -0.080000   9.920000 -1.757025   \n",
       "\n",
       "        ActivityEncoded  \n",
       "0                     1  \n",
       "1                     1  \n",
       "2                     1  \n",
       "3                     1  \n",
       "4                     1  \n",
       "5                     1  \n",
       "6                     1  \n",
       "7                     1  \n",
       "8                     1  \n",
       "9                     1  \n",
       "597                   5  \n",
       "598                   5  \n",
       "599                   5  \n",
       "600                   5  \n",
       "601                   5  \n",
       "602                   5  \n",
       "603                   5  \n",
       "604                   5  \n",
       "605                   5  \n",
       "606                   5  \n",
       "1161                  4  \n",
       "1162                  4  \n",
       "1163                  4  \n",
       "1164                  4  \n",
       "1165                  4  \n",
       "1166                  4  \n",
       "1167                  4  \n",
       "1168                  4  \n",
       "1169                  4  \n",
       "1170                  4  \n",
       "1769                  0  \n",
       "1770                  0  \n",
       "1771                  0  \n",
       "1772                  0  \n",
       "1773                  0  \n",
       "1774                  0  \n",
       "1775                  0  \n",
       "1776                  0  \n",
       "1777                  0  \n",
       "1778                  0  \n",
       "221335                2  \n",
       "221336                2  \n",
       "221337                2  \n",
       "221338                2  \n",
       "221339                2  \n",
       "221340                2  \n",
       "221341                2  \n",
       "221342                2  \n",
       "221343                2  \n",
       "221344                2  \n",
       "223434                3  \n",
       "223435                3  \n",
       "223436                3  \n",
       "223437                3  \n",
       "223438                3  \n",
       "223439                3  \n",
       "223440                3  \n",
       "223441                3  \n",
       "223442                3  \n",
       "223443                3  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "# Define column name of the label vector\n",
    "LABEL = 'ActivityEncoded'\n",
    "# Transform the labels from String to Integer via LabelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Add a new column to the existing DataFrame with the encoded values\n",
    "data[LABEL] = le.fit_transform(data['activity'].values.ravel())\n",
    "# dummy=data_reformat.groupby(['class'])\n",
    "# dummy.head(10)\n",
    "\n",
    "data\n",
    "\n",
    "dummy2=data.groupby(['activity'])\n",
    "dummy2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ActivityEncoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8489</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8490</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8491</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8492</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8493</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098204</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098205</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098206</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098207</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098208</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>834785 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ActivityEncoded\n",
       "8489                   5\n",
       "8490                   5\n",
       "8491                   5\n",
       "8492                   5\n",
       "8493                   5\n",
       "...                  ...\n",
       "1098204                2\n",
       "1098205                2\n",
       "1098206                2\n",
       "1098207                2\n",
       "1098208                2\n",
       "\n",
       "[834785 rows x 1 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = data[data['user_id'] <=28]\n",
    "data_test =  data[data['user_id'] > 28]\n",
    "x_train = data_train[['acc_x','acc_y','acc_z']]\n",
    "x_test = data_test[['acc_x','acc_y','acc_z']]\n",
    "y_train = data_train[['ActivityEncoded']]\n",
    "y_test = data_test[['ActivityEncoded']]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_train_filter = data_train[['user_id','acc_x','acc_y','acc_z','ActivityEncoded']]\n",
    "#data_train_filter = data_train[['acc_x','acc_y','acc_z','ActivityEncoded']]\n",
    "#data_train_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_train_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.51016316, 0.74190794, 0.48893191],\n",
       "       [0.49570273, 0.75152636, 0.48789509],\n",
       "       [0.4898497 , 0.74293849, 0.48478463],\n",
       "       ...,\n",
       "       [0.7252275 , 0.45977301, 0.54529307],\n",
       "       [0.72320526, 0.45775536, 0.54630804],\n",
       "       [0.72017189, 0.46103405, 0.54326313]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "x_training_scaled = scaler.fit_transform(x_train)\n",
    "x_training_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_set = []\n",
    "labels = []\n",
    "x_training_scaled[0:200]\n",
    "x_training_scaled[0:200,0]\n",
    "x_training_scaled[2,1]\n",
    "#x_train[\"acc_x\"].values[0:200]\n",
    "#y_train[\"ActivityEncoded\"].values[0]\n",
    "for i in range(0, 834600, 200):\n",
    "    features_set.append([x_training_scaled[i:i+200, 0],x_training_scaled[i:i+200, 1],x_training_scaled[i:i+200, 2]])\n",
    "    #for j in range(0, 200, 1):\n",
    "    #    features_set.append(x_training_scaled[(i+j), 0])\n",
    "    #for j in range(0, 200, 1):\n",
    "    #    features_set.append(x_training_scaled[(i+j), 1])\n",
    "    #for j in range(0, 200, 1):\n",
    "    #    features_set.append(x_training_scaled[(i+j), 2])\n",
    "    max_labels_perwindow = stats.mode(y_train[\"ActivityEncoded\"][i: i+200])[0][0]\n",
    "    labels.append(max_labels_perwindow)\n",
    "\n",
    "len(labels)\n",
    "len(features_set)\n",
    "labels = to_categorical(labels)\n",
    "#len(labels)\n",
    "#labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4173, 3, 200)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_set, labels = np.array(features_set), np.array(labels)\n",
    "features_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4173, 200, 3)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # features_set = np.reshape(features_set, (features_set.shape[0], features_set.shape[1], 1))\n",
    "#features_set.shape[0]\n",
    "#features_set.shape[1]\n",
    "#features_set = np.reshape(features_set, (4173,600,1))\n",
    "features_set = np.reshape(features_set, (4173,200,3))\n",
    "#features_set = np.reshape(features_set, (features_set.shape[0], 240))\n",
    "features_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.51016316, 0.49570273, 0.4898497 ],\n",
       "        [0.49948999, 0.49948999, 0.51119605],\n",
       "        [0.4984571 , 0.49294836, 0.49363695],\n",
       "        ...,\n",
       "        [0.52176451, 0.43916459, 0.49480721],\n",
       "        [0.47718129, 0.41013365, 0.51312435],\n",
       "        [0.48962312, 0.59088579, 0.64134433]],\n",
       "\n",
       "       [[0.48227519, 0.56731629, 0.58659686],\n",
       "        [0.6255023 , 0.64788154, 0.49363695],\n",
       "        [0.58384249, 0.52186922, 0.58384249],\n",
       "        ...,\n",
       "        [0.49860889, 0.46750431, 0.50033692],\n",
       "        [0.47337962, 0.40045667, 0.48478463],\n",
       "        [0.50033692, 0.67452253, 0.50517541]],\n",
       "\n",
       "       [[0.56249615, 0.50258865, 0.58005523],\n",
       "        [0.65752183, 0.56043037, 0.56731629],\n",
       "        [0.57041495, 0.51016316, 0.62068215],\n",
       "        ...,\n",
       "        [0.45298884, 0.50621223, 0.47130598],\n",
       "        [0.58708412, 0.65033009, 0.4021847 ],\n",
       "        [0.43259806, 0.46266582, 0.47337962]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.72320526, 0.72320526, 0.72219414],\n",
       "        [0.72219414, 0.72219414, 0.72118301],\n",
       "        [0.72118301, 0.71941355, 0.72118301],\n",
       "        ...,\n",
       "        [0.56939863, 0.56559249, 0.56559249],\n",
       "        [0.56254758, 0.55975641, 0.56178635],\n",
       "        [0.55975641, 0.56254758, 0.56077138]],\n",
       "\n",
       "       [[0.72320526, 0.72118301, 0.72219414],\n",
       "        [0.72219414, 0.72219414, 0.72118301],\n",
       "        [0.72118301, 0.72219414, 0.72017189],\n",
       "        ...,\n",
       "        [0.55112916, 0.55671149, 0.55671149],\n",
       "        [0.55671149, 0.55671149, 0.55975641],\n",
       "        [0.55975641, 0.55772647, 0.55772647]],\n",
       "\n",
       "       [[0.71056623, 0.73078868, 0.72901921],\n",
       "        [0.73028311, 0.72800809, 0.71840243],\n",
       "        [0.7173913 , 0.72219414, 0.72421638],\n",
       "        ...,\n",
       "        [0.55290535, 0.56356255, 0.55975641],\n",
       "        [0.54630804, 0.55595027, 0.56254758],\n",
       "        [0.56178635, 0.55595027, 0.57142857]]])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.add(LSTM(units=50, return_sequences=True, input_shape=(features_set.shape[1], 1)))\n",
    "#model.add(LSTM(units=50, return_sequences=True, input_shape=(600, 1)))\n",
    "# model.add(LSTM(units=36, return_sequences=True, input_shape=(200, 3)))\n",
    "model.add(LSTM(units=36,input_shape=(200, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.add(LSTM(units=50, return_sequences=True))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dropout(0.1))\n",
    "# model.add(LSTM(units=18))\n",
    "#model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_23 (LSTM)               (None, 36)                5760      \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 6)                 222       \n",
      "=================================================================\n",
      "Total params: 5,982\n",
      "Trainable params: 5,982\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(6, activation='sigmoid'))\n",
    "model.summary()\n",
    "# model.add(Dense(units = 6))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['accuracy'])\n",
    "model.compile(optimizer = 'rmsprop',loss = 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 26s - loss: 1.4856 - accuracy: 0.3896\n",
      "Epoch 2/30\n",
      " - 25s - loss: 1.4269 - accuracy: 0.3988\n",
      "Epoch 3/30\n",
      " - 27s - loss: 1.3789 - accuracy: 0.4256\n",
      "Epoch 4/30\n",
      " - 25s - loss: 1.3478 - accuracy: 0.4294\n",
      "Epoch 5/30\n",
      " - 26s - loss: 1.3440 - accuracy: 0.4603\n",
      "Epoch 6/30\n",
      " - 26s - loss: 1.3407 - accuracy: 0.4800\n",
      "Epoch 7/30\n",
      " - 25s - loss: 1.2892 - accuracy: 0.5143\n",
      "Epoch 8/30\n",
      " - 25s - loss: 1.2962 - accuracy: 0.5008\n",
      "Epoch 9/30\n",
      " - 25s - loss: 1.2457 - accuracy: 0.5363\n",
      "Epoch 10/30\n",
      " - 25s - loss: 1.4163 - accuracy: 0.4436\n",
      "Epoch 11/30\n",
      " - 25s - loss: 1.3672 - accuracy: 0.4584\n",
      "Epoch 12/30\n",
      " - 24s - loss: 1.2097 - accuracy: 0.5624\n",
      "Epoch 13/30\n",
      " - 25s - loss: 1.2844 - accuracy: 0.5011\n",
      "Epoch 14/30\n",
      " - 25s - loss: 1.2675 - accuracy: 0.5361\n",
      "Epoch 15/30\n",
      " - 25s - loss: 1.2956 - accuracy: 0.5066\n",
      "Epoch 16/30\n",
      " - 24s - loss: 1.1754 - accuracy: 0.5852\n",
      "Epoch 17/30\n",
      " - 24s - loss: 1.2628 - accuracy: 0.5253\n",
      "Epoch 18/30\n",
      " - 24s - loss: 1.1942 - accuracy: 0.5629\n",
      "Epoch 19/30\n",
      " - 24s - loss: 1.1408 - accuracy: 0.5984\n",
      "Epoch 20/30\n",
      " - 25s - loss: 1.1349 - accuracy: 0.6089\n",
      "Epoch 21/30\n",
      " - 24s - loss: 1.2879 - accuracy: 0.4934\n",
      "Epoch 22/30\n",
      " - 25s - loss: 1.2093 - accuracy: 0.5579\n",
      "Epoch 23/30\n",
      " - 24s - loss: 1.1064 - accuracy: 0.6216\n",
      "Epoch 24/30\n",
      " - 24s - loss: 1.0829 - accuracy: 0.6382\n",
      "Epoch 25/30\n",
      " - 24s - loss: 1.0664 - accuracy: 0.6446\n",
      "Epoch 26/30\n",
      " - 24s - loss: 1.0540 - accuracy: 0.6477\n",
      "Epoch 27/30\n",
      " - 25s - loss: 1.0484 - accuracy: 0.6504\n",
      "Epoch 28/30\n",
      " - 24s - loss: 1.0295 - accuracy: 0.6602\n",
      "Epoch 29/30\n",
      " - 24s - loss: 0.9973 - accuracy: 0.6731\n",
      "Epoch 30/30\n",
      " - 24s - loss: 0.9964 - accuracy: 0.6705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14e09ca2780>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(features_set, labels, epochs = 30, batch_size = 16, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_24 (LSTM)               (None, 200, 36)           5760      \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 200, 36)           0         \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 18)                3960      \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 6)                 114       \n",
      "=================================================================\n",
      "Total params: 9,834\n",
      "Trainable params: 9,834\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initiliazing the sequential model\n",
    "model = Sequential()\n",
    "# Configuring the parameters\n",
    "model.add(LSTM(36,return_sequences=True,input_shape=(200, 3)))\n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(LSTM(units=18))\n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(0.1))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'rmsprop',loss = 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 48s - loss: 1.4918 - accuracy: 0.3892\n",
      "Epoch 2/30\n",
      " - 49s - loss: 1.4224 - accuracy: 0.4012\n",
      "Epoch 3/30\n",
      " - 47s - loss: 1.4006 - accuracy: 0.4162\n",
      "Epoch 4/30\n",
      " - 47s - loss: 1.3699 - accuracy: 0.4289\n",
      "Epoch 5/30\n",
      " - 47s - loss: 1.3734 - accuracy: 0.4234\n",
      "Epoch 6/30\n",
      " - 46s - loss: 1.3490 - accuracy: 0.4292\n",
      "Epoch 7/30\n",
      " - 46s - loss: 1.3371 - accuracy: 0.4294\n",
      "Epoch 8/30\n",
      " - 46s - loss: 1.3245 - accuracy: 0.4359\n",
      "Epoch 9/30\n",
      " - 46s - loss: 1.3004 - accuracy: 0.4484\n",
      "Epoch 10/30\n",
      " - 49s - loss: 1.2883 - accuracy: 0.4970\n",
      "Epoch 11/30\n",
      " - 52s - loss: 1.2567 - accuracy: 0.5325\n",
      "Epoch 12/30\n",
      " - 54s - loss: 1.2055 - accuracy: 0.5588\n",
      "Epoch 13/30\n",
      " - 53s - loss: 1.1820 - accuracy: 0.5799\n",
      "Epoch 14/30\n",
      " - 52s - loss: 1.1513 - accuracy: 0.5977\n",
      "Epoch 15/30\n",
      " - 52s - loss: 1.1101 - accuracy: 0.6175\n",
      "Epoch 16/30\n",
      " - 53s - loss: 1.0849 - accuracy: 0.6341\n",
      "Epoch 17/30\n",
      " - 53s - loss: 1.0817 - accuracy: 0.6322\n",
      "Epoch 18/30\n",
      " - 53s - loss: 1.0291 - accuracy: 0.6530\n",
      "Epoch 19/30\n",
      " - 52s - loss: 1.0275 - accuracy: 0.6585\n",
      "Epoch 20/30\n",
      " - 53s - loss: 0.9940 - accuracy: 0.6698\n",
      "Epoch 21/30\n",
      " - 53s - loss: 0.9752 - accuracy: 0.6667\n",
      "Epoch 22/30\n",
      " - 52s - loss: 0.9415 - accuracy: 0.6885\n",
      "Epoch 23/30\n",
      " - 53s - loss: 0.9214 - accuracy: 0.6911\n",
      "Epoch 24/30\n",
      " - 53s - loss: 0.8935 - accuracy: 0.6973\n",
      "Epoch 25/30\n",
      " - 52s - loss: 0.8759 - accuracy: 0.7055\n",
      "Epoch 26/30\n",
      " - 52s - loss: 0.8609 - accuracy: 0.7096\n",
      "Epoch 27/30\n",
      " - 52s - loss: 0.8637 - accuracy: 0.7067\n",
      "Epoch 28/30\n",
      " - 52s - loss: 0.8194 - accuracy: 0.7242\n",
      "Epoch 29/30\n",
      " - 51s - loss: 0.8033 - accuracy: 0.7309\n",
      "Epoch 30/30\n",
      " - 46s - loss: 0.8018 - accuracy: 0.7309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14e07c4c518>"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(features_set, labels, epochs = 30, batch_size = 16, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.694638</td>\n",
       "      <td>12.680544</td>\n",
       "      <td>0.503953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.012288</td>\n",
       "      <td>11.264028</td>\n",
       "      <td>0.953424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.903325</td>\n",
       "      <td>10.882658</td>\n",
       "      <td>-0.081722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.612916</td>\n",
       "      <td>18.496431</td>\n",
       "      <td>3.023717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.184970</td>\n",
       "      <td>12.108489</td>\n",
       "      <td>7.205164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832897</th>\n",
       "      <td>8.200000</td>\n",
       "      <td>4.520000</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832898</th>\n",
       "      <td>8.960000</td>\n",
       "      <td>4.020000</td>\n",
       "      <td>1.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832899</th>\n",
       "      <td>7.970000</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>0.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832900</th>\n",
       "      <td>8.010000</td>\n",
       "      <td>4.790000</td>\n",
       "      <td>0.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832901</th>\n",
       "      <td>8.350000</td>\n",
       "      <td>4.440000</td>\n",
       "      <td>1.080000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>263424 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           acc_x      acc_y     acc_z\n",
       "0      -0.694638  12.680544  0.503953\n",
       "1       5.012288  11.264028  0.953424\n",
       "2       4.903325  10.882658 -0.081722\n",
       "3      -0.612916  18.496431  3.023717\n",
       "4      -1.184970  12.108489  7.205164\n",
       "...          ...        ...       ...\n",
       "832897  8.200000   4.520000  0.150000\n",
       "832898  8.960000   4.020000  1.460000\n",
       "832899  7.970000   4.750000  0.610000\n",
       "832900  8.010000   4.790000  0.690000\n",
       "832901  8.350000   4.440000  1.080000\n",
       "\n",
       "[263424 rows x 3 columns]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ActivityEncoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832897</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832898</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832899</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832900</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832901</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>263424 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ActivityEncoded\n",
       "0                     1\n",
       "1                     1\n",
       "2                     1\n",
       "3                     1\n",
       "4                     1\n",
       "...                 ...\n",
       "832897                3\n",
       "832898                3\n",
       "832899                3\n",
       "832900                3\n",
       "832901                3\n",
       "\n",
       "[263424 rows x 1 columns]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263424"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "x_testing_scaled = scaler.fit_transform(x_test)\n",
    "x_testing_scaled\n",
    "len(x_testing_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1317"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_set_2 = []\n",
    "labels_2 = []\n",
    "#x_testing_scaled[0:200]\n",
    "#x_testing_scaled[0:200,0]\n",
    "#x_testing_scaled[2,1]\n",
    "#x_train[\"acc_x\"].values[0:200]\n",
    "#y_train[\"ActivityEncoded\"].values[0]\n",
    "for i in range(0, 263400, 200):\n",
    "    features_set_2.append([x_testing_scaled[i:i+200, 0],x_testing_scaled[i:i+200, 1],x_testing_scaled[i:i+200, 2]])\n",
    "    #for j in range(0, 200, 1):\n",
    "    #    features_set.append(x_training_scaled[(i+j), 0])\n",
    "    #for j in range(0, 200, 1):\n",
    "    #    features_set.append(x_training_scaled[(i+j), 1])\n",
    "    #for j in range(0, 200, 1):\n",
    "    #    features_set.append(x_training_scaled[(i+j), 2])\n",
    "    max_labels_perwindow = stats.mode(y_test[\"ActivityEncoded\"][i: i+200])[0][0]\n",
    "    labels_2.append(max_labels_perwindow)\n",
    "\n",
    "len(features_set_2)\n",
    "labels_2 = to_categorical(labels_2)\n",
    "len(labels_2)\n",
    "len(features_set_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1317, 3, 200)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_set_2, labels_2 = np.array(features_set_2), np.array(labels_2)\n",
    "#test_features_2 = np.array(features_set_2)\n",
    "# test_features = np.reshape(test_features, (test_features.shape[0], test_features.shape[1], 1))\n",
    "# Making Predictions\n",
    "\n",
    "features_set_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1317, 200, 3)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_set_2= np.reshape(features_set_2,(1317,200,3))\n",
    "features_set_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Neural Network - Loss: 1.2356204040682306, Accuracy: 0.5998481512069702\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = model.evaluate(\n",
    "    features_set_2, labels_2, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1008 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.25      0.22       132\n",
      "           1       0.74      0.79      0.77       397\n",
      "           2       0.66      0.92      0.77        89\n",
      "           3       0.33      0.09      0.14        76\n",
      "           4       0.10      0.01      0.01       145\n",
      "           5       0.62      0.74      0.67       478\n",
      "\n",
      "    accuracy                           0.60      1317\n",
      "   macro avg       0.44      0.47      0.43      1317\n",
      "weighted avg       0.54      0.60      0.56      1317\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.3914654e-01, 5.1790744e-02, 8.4314644e-03, 7.0330679e-02,\n",
       "        4.9258369e-01, 1.8669820e-01],\n",
       "       [1.2378928e-01, 2.2894025e-02, 4.7239661e-03, 2.1086335e-03,\n",
       "        1.0316917e-01, 4.6477556e-02],\n",
       "       [5.8560771e-01, 4.5514107e-02, 2.6317149e-02, 1.1424026e-01,\n",
       "        5.1806003e-01, 1.1996475e-01],\n",
       "       ...,\n",
       "       [9.5556766e-02, 1.8322796e-02, 4.8887730e-04, 2.0384789e-02,\n",
       "        1.1859828e-01, 7.5786674e-01],\n",
       "       [1.1843583e-01, 1.8299311e-02, 1.0354817e-03, 3.5903901e-02,\n",
       "        1.4763725e-01, 8.3099222e-01],\n",
       "       [7.7690713e-02, 4.2829856e-02, 1.3249982e-04, 8.8800006e-03,\n",
       "        1.0802234e-01, 6.7654061e-01]], dtype=float32)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pred_test = model.predict(features_set_2)\n",
    "labels_pred_test\n",
    "# Take the class with the highest probability from the test predictions\n",
    "max_labels_pred_test = np.argmax(labels_pred_test, axis=1)\n",
    "max_labels_test = np.argmax(labels_2, axis=1)\n",
    "\n",
    "show_confusion_matrix(max_labels_test, max_labels_pred_test)\n",
    "\n",
    "print(classification_report(max_labels_test, max_labels_pred_test))\n",
    "labels_pred_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
